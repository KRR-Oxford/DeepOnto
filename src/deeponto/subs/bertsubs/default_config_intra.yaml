# Copyright 2023 Jiaoyan Chen. All rights reserved.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# @paper(
#     "Contextual Semantic Embeddings for Ontology Subsumption Prediction (World Wide Web Journal)",
# )
model: bertsubs (intra-ontology)

onto_file: foodon.owl
label_property:
  - http://www.w3.org/2000/01/rdf-schema#label
  - http://www.geneontology.org/formats/oboInOwl#hasExactSynonym
use_one_label: true # true: select only one label
subsumption_type: named_class # named_class or restriction

# test_subsumption_file is optional. If it is set to None (or left as empty), the output is the fine-tuned model and its validation results;
#                                    otherwise, the output is the find-tuned model, its validation results and the testing results.
# test_type: "prediction" (predict scores for candidate subsumptions given in test_submission_file
#            or "evaluation" (calculate metrics with ground truths given in the test_subsumption_file)
test_subsumption_file: test_subsumption.csv
test_type: prediction

# validation_subsumption_file and train_subsumption_file are optional
# If train_subsumption_file is set to None (or left as empty)
#   All the named class (or restriction) subsumptions in the given ontology are used for training and validation
#   valid_ratio of the subsumptions are for validation, and the remaining are for training
train_subsumption_file:
valid_subsumption_file:
valid:
  valid_ratio: 0.05
  max_neg_size: 40

prompt:
  prompt_type: isolated # "isolated", "traversal", "path"
  prompt_hop: 1
  prompt_max_subsumptions: 4
  use_sub_special_token: false # true: <SUB>, false: <SEP> (only works for prompt_type of "path")
  context_dup: 4
  max_length: 128

# These hyperparameters are about the BERT model and its fine-tuning
fine_tune:
  do_fine_tune: true
  pretrained: bert-base-uncased
  tokenizer": bert-base-uncased
  output_dir: fine-tuned-bert
  warm_up_ratio: 0.0
  num_epochs: 5.0
  early_stop: false
  train_pos_dup: 2
  train_neg_dup: 2
  batch_size: 32

evaluation:
  batch_size: 32

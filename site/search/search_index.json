{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GET STARTED","text":"<p>   A package for ontology engineering with deep learning.  </p>"},{"location":"#news","title":"News","text":"<ul> <li>[2023 Jan] Rebuild \\(\\textsf{DeepOnto}\\) based on the OWLAPI and remove owlready2 from the essential dependencies.</li> </ul>"},{"location":"#about","title":"About","text":"<p>\\(\\textsf{DeepOnto}\\) aims to provide tools for implementing deep learning models, constructing resources, and conducting evaluation for various ontology engineering purposes.</p> <ul> <li>Documentation: https://krr-oxford.github.io/DeepOnto/.</li> <li>Github Repository: https://github.com/KRR-Oxford/DeepOnto. </li> <li>PyPI: https://pypi.org/project/deeponto/ (will be updated after major update). </li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#owlapi","title":"OWLAPI","text":"<p>\\(\\textsf{DeepOnto}\\) relies on OWLAPI version 4 (written in Java) for ontologies. </p> <p>We use what has been implemented in mOWL that uses JPype to bridge Python and Java Virtual Machine (JVM).</p>"},{"location":"#pytorch","title":"Pytorch","text":"<p>\\(\\textsf{DeepOnto}\\) relies on Pytorch for deep learning framework.</p> <p>Configure Pytorch installation with CUDA support using, for example:</p> <pre><code>pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n</code></pre> <p>Basic usage of Ontology does not rely on GPUs, but for efficient deep learning model training, please make sure <code>torch.cuda.is_available()</code> returns <code>True</code>.</p>"},{"location":"#install-from-pypi","title":"Install from PyPI","text":"<p>Other dependencies are specified in <code>setup.cfg</code> and <code>requirements.txt</code> which are supposed to be installed along with <code>deeponto</code>.</p> <pre><code>pip install deeponto\n</code></pre>"},{"location":"#use-git-repository","title":"Use Git Repository","text":"<p>One can git clone the repository without installing through PyPI and install the dependencies manually by:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#main-features","title":"Main Features","text":""},{"location":"#extending-the-owlapi","title":"Extending the OWLAPI","text":"<p>\\(\\textsf{DeepOnto}\\) extends the OWLAPI library for ontology processing and reasoning, and also for better integration with deep learning modules.  The base classes that extend the OWLAPI functionalities are <code>Ontology</code> and <code>OntologyReasoner</code>. Examples of how to use them can be found here.</p>"},{"location":"#bertmap","title":"BERTMap","text":"<p>BERTMap [1] is a BERT-based ontology matching (OM) system originally developed in repo but is now maintained in \\(\\textsf{DeepOnto}\\). See how to use BERTMap in this tutorial.</p>"},{"location":"#om-evaluation","title":"OM Evaluation","text":"<p>\\(\\textsf{DeepOnto}\\) provides an evaluation workaround for OM systems including global matching and local ranking as proposed in here [2]. See how to use this evaluation framework in this tutorial.</p>"},{"location":"#bio-ml","title":"Bio-ML","text":"<p>Bio-ML [2] is an OM resource and has been integrated into the Bio-ML track of the OAEI. See instructions of how to use Bio-ML.</p>"},{"location":"#license","title":"License","text":"<p>License</p> <p>Copyright 2021 Yuan He (KRR-Oxford). All rights reserved.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"#publications","title":"Publications","text":"<ul> <li>[1] Yuan He\u201a Jiaoyan Chen\u201a Denvar Antonyrajah and Ian Horrocks. BERTMap: A BERT\u2212Based Ontology Alignment System. In Proceedings of 36th AAAI Conference on Artificial Intelligence 2022 (AAAI-2022). /arxiv/ /aaai/  </li> <li>[2] Yuan He\u201a Jiaoyan Chen\u201a Hang Dong, Ernesto Jim\u00e9nez-Ruiz, Ali Hadian and Ian Horrocks. Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. The 21st International Semantic Web Conference (ISWC-2022, Best Resource Paper Candidate). /arxiv/ /iswc/  </li> <li>[3] Jiaoyan Chen, Yuan He, Yuxia Geng, Ernesto Jim\u00e9nez-Ruiz, Hang Dong and Ian Horrocks. Contextual Semantic Embeddings for Ontology Subsumption Prediction. 2022 (Under review). /arxiv/  </li> </ul> <p>Please report any bugs or queries by raising a GitHub issue or sending emails to the maintainer (Yuan He) through:</p> <p>first_name.last_name@cs.ox.ac.uk</p>"},{"location":"deeponto/align/evaluation/","title":"Evaluation","text":""},{"location":"deeponto/align/evaluation/#deeponto.align.evaluation.AlignmentEvaluator","title":"<code>AlignmentEvaluator</code>","text":"<p>Class that provides evaluation metrics for alignment.</p> Source code in <code>deeponto/align/evaluation.py</code> <pre><code>class AlignmentEvaluator:\n\"\"\"Class that provides evaluation metrics for alignment.\"\"\"\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def precision(prediction_mappings: List[EntityMapping], reference_mappings: Iterable[ReferenceMapping]) -&gt; float:\nr\"\"\"The percentage of correct predictions.\n\n        $$P = \\frac{|\\mathcal{M}_{pred} \\cap \\mathcal{M}_{ref}|}{|\\mathcal{M}_{pred}|}$$\n        \"\"\"\n        preds = [p.to_tuple() for p in prediction_mappings]\n        refs = [r.to_tuple() for r in reference_mappings]\n        return len(set(preds).intersection(set(refs))) / len(set(preds))\n\n    @staticmethod\n    def recall(prediction_mappings: List[EntityMapping], reference_mappings: Iterable[ReferenceMapping]) -&gt; float:\nr\"\"\"The percentage of correct retrievals.\n\n        $$R = \\frac{|\\mathcal{M}_{pred} \\cap \\mathcal{M}_{ref}|}{|\\mathcal{M}_{ref}|}$$\n        \"\"\"\n        preds = [p.to_tuple() for p in prediction_mappings]\n        refs = [r.to_tuple() for r in reference_mappings]\n        return len(set(preds).intersection(set(refs))) / len(set(refs))\n\n    @staticmethod\n    def f1(\n        prediction_mappings: List[EntityMapping],\n        reference_mappings: Iterable[ReferenceMapping],\n        null_reference_mappings: Optional[Iterable] = None,\n    ):\nr\"\"\"Compute the F1 score given the prediction and reference mappings.\n\n        $$F_1 = \\frac{2 P R}{P + R}$$\n\n        `null_reference_mappings` is an additional set whose elements\n        should be ignored in the calculation, i.e., neither positive nor negative.\n        \"\"\"\n        preds = [p.to_tuple() for p in prediction_mappings]\n        refs = [r.to_tuple() for r in reference_mappings]\n        null_refs = [n.to_tuple() for n in null_reference_mappings]\n        # elements in the {null_set} are removed from both {pred} and {ref} (ignored)\n        if null_refs:\n            preds = set(preds) - set(null_refs)\n            refs = set(refs) - set(null_refs)\n        P = len(set(preds).intersection(set(refs))) / len(set(preds))\n        R = len(set(preds).intersection(set(refs))) / len(set(refs))\n        F1 = 2 * P * R / (P + R)\n\n        return {\"P\": round(P, 3), \"R\": round(R, 3), \"F1\": round(F1, 3)}\n\n    ##################################################################################\n    ###                       [Eval Case 2]: Hits@K &amp; MRR                          ###\n    ##################################################################################\n\n    # TODO: check below algorithms after full deployment\n\n    @staticmethod\n    def hits_at_K(prediction_and_candidates: List[Tuple[EntityMapping, List[EntityMapping]]], K: int):\nr\"\"\"Compute $Hits@K$ for a list of `(prediction_mapping, candidate_mappings)` pair.\n\n        It is computed as the number of a `prediction_mapping` existed in the first $K$ ranked `candidate_mappings`,\n        divided by the total number of input pairs.\n\n        $$Hits@K = \\sum_i^N \\mathbb{I}_{rank_i \\leq k} / N$$\n        \"\"\"\n        n_hits = 0\n        for pred, cands in prediction_and_candidates:\n            ordered_candidates = [c.to_tuple() for c in EntityMapping.sort_entity_mappings_by_score(cands, k=K)]\n            if pred.to_tuple() in ordered_candidates:\n                n_hits += 1\n        return n_hits / len(prediction_and_candidates)\n\n    @staticmethod\n    def mean_reciprocal_rank(prediction_and_candidates: List[Tuple[EntityMapping, List[EntityMapping]]]):\nr\"\"\"Compute $MRR$ for a list of `(prediction_mapping, candidate_mappings)` pair.\n\n        $$MRR = \\sum_i^N rank_i^{-1} / N$$\n        \"\"\"\n        sum_inverted_ranks = 0\n        for pred, cands in prediction_and_candidates:\n            ordered_candidates = [c.to_tuple() for c in EntityMapping.sort_entity_mappings_by_score(cands)]\n            if pred.to_tuple() in ordered_candidates:\n                rank = ordered_candidates.index(pred) + 1\n            else:\n                rank = math.inf\n            sum_inverted_ranks += 1 / rank\n        return sum_inverted_ranks / len(prediction_and_candidates)\n</code></pre>"},{"location":"deeponto/align/evaluation/#deeponto.align.evaluation.AlignmentEvaluator.precision","title":"<code>precision(prediction_mappings, reference_mappings)</code>  <code>staticmethod</code>","text":"<p>The percentage of correct predictions.</p> \\[P = \\frac{|\\mathcal{M}_{pred} \\cap \\mathcal{M}_{ref}|}{|\\mathcal{M}_{pred}|}\\] Source code in <code>deeponto/align/evaluation.py</code> <pre><code>@staticmethod\ndef precision(prediction_mappings: List[EntityMapping], reference_mappings: Iterable[ReferenceMapping]) -&gt; float:\nr\"\"\"The percentage of correct predictions.\n\n    $$P = \\frac{|\\mathcal{M}_{pred} \\cap \\mathcal{M}_{ref}|}{|\\mathcal{M}_{pred}|}$$\n    \"\"\"\n    preds = [p.to_tuple() for p in prediction_mappings]\n    refs = [r.to_tuple() for r in reference_mappings]\n    return len(set(preds).intersection(set(refs))) / len(set(preds))\n</code></pre>"},{"location":"deeponto/align/evaluation/#deeponto.align.evaluation.AlignmentEvaluator.recall","title":"<code>recall(prediction_mappings, reference_mappings)</code>  <code>staticmethod</code>","text":"<p>The percentage of correct retrievals.</p> \\[R = \\frac{|\\mathcal{M}_{pred} \\cap \\mathcal{M}_{ref}|}{|\\mathcal{M}_{ref}|}\\] Source code in <code>deeponto/align/evaluation.py</code> <pre><code>@staticmethod\ndef recall(prediction_mappings: List[EntityMapping], reference_mappings: Iterable[ReferenceMapping]) -&gt; float:\nr\"\"\"The percentage of correct retrievals.\n\n    $$R = \\frac{|\\mathcal{M}_{pred} \\cap \\mathcal{M}_{ref}|}{|\\mathcal{M}_{ref}|}$$\n    \"\"\"\n    preds = [p.to_tuple() for p in prediction_mappings]\n    refs = [r.to_tuple() for r in reference_mappings]\n    return len(set(preds).intersection(set(refs))) / len(set(refs))\n</code></pre>"},{"location":"deeponto/align/evaluation/#deeponto.align.evaluation.AlignmentEvaluator.f1","title":"<code>f1(prediction_mappings, reference_mappings, null_reference_mappings=None)</code>  <code>staticmethod</code>","text":"<p>Compute the F1 score given the prediction and reference mappings.</p> \\[F_1 = \\frac{2 P R}{P + R}\\] <p><code>null_reference_mappings</code> is an additional set whose elements should be ignored in the calculation, i.e., neither positive nor negative.</p> Source code in <code>deeponto/align/evaluation.py</code> <pre><code>@staticmethod\ndef f1(\n    prediction_mappings: List[EntityMapping],\n    reference_mappings: Iterable[ReferenceMapping],\n    null_reference_mappings: Optional[Iterable] = None,\n):\nr\"\"\"Compute the F1 score given the prediction and reference mappings.\n\n    $$F_1 = \\frac{2 P R}{P + R}$$\n\n    `null_reference_mappings` is an additional set whose elements\n    should be ignored in the calculation, i.e., neither positive nor negative.\n    \"\"\"\n    preds = [p.to_tuple() for p in prediction_mappings]\n    refs = [r.to_tuple() for r in reference_mappings]\n    null_refs = [n.to_tuple() for n in null_reference_mappings]\n    # elements in the {null_set} are removed from both {pred} and {ref} (ignored)\n    if null_refs:\n        preds = set(preds) - set(null_refs)\n        refs = set(refs) - set(null_refs)\n    P = len(set(preds).intersection(set(refs))) / len(set(preds))\n    R = len(set(preds).intersection(set(refs))) / len(set(refs))\n    F1 = 2 * P * R / (P + R)\n\n    return {\"P\": round(P, 3), \"R\": round(R, 3), \"F1\": round(F1, 3)}\n</code></pre>"},{"location":"deeponto/align/evaluation/#deeponto.align.evaluation.AlignmentEvaluator.hits_at_K","title":"<code>hits_at_K(prediction_and_candidates, K)</code>  <code>staticmethod</code>","text":"<p>Compute \\(Hits@K\\) for a list of <code>(prediction_mapping, candidate_mappings)</code> pair.</p> <p>It is computed as the number of a <code>prediction_mapping</code> existed in the first \\(K\\) ranked <code>candidate_mappings</code>, divided by the total number of input pairs.</p> \\[Hits@K = \\sum_i^N \\mathbb{I}_{rank_i \\leq k} / N\\] Source code in <code>deeponto/align/evaluation.py</code> <pre><code>@staticmethod\ndef hits_at_K(prediction_and_candidates: List[Tuple[EntityMapping, List[EntityMapping]]], K: int):\nr\"\"\"Compute $Hits@K$ for a list of `(prediction_mapping, candidate_mappings)` pair.\n\n    It is computed as the number of a `prediction_mapping` existed in the first $K$ ranked `candidate_mappings`,\n    divided by the total number of input pairs.\n\n    $$Hits@K = \\sum_i^N \\mathbb{I}_{rank_i \\leq k} / N$$\n    \"\"\"\n    n_hits = 0\n    for pred, cands in prediction_and_candidates:\n        ordered_candidates = [c.to_tuple() for c in EntityMapping.sort_entity_mappings_by_score(cands, k=K)]\n        if pred.to_tuple() in ordered_candidates:\n            n_hits += 1\n    return n_hits / len(prediction_and_candidates)\n</code></pre>"},{"location":"deeponto/align/evaluation/#deeponto.align.evaluation.AlignmentEvaluator.mean_reciprocal_rank","title":"<code>mean_reciprocal_rank(prediction_and_candidates)</code>  <code>staticmethod</code>","text":"<p>Compute \\(MRR\\) for a list of <code>(prediction_mapping, candidate_mappings)</code> pair.</p> \\[MRR = \\sum_i^N rank_i^{-1} / N\\] Source code in <code>deeponto/align/evaluation.py</code> <pre><code>@staticmethod\ndef mean_reciprocal_rank(prediction_and_candidates: List[Tuple[EntityMapping, List[EntityMapping]]]):\nr\"\"\"Compute $MRR$ for a list of `(prediction_mapping, candidate_mappings)` pair.\n\n    $$MRR = \\sum_i^N rank_i^{-1} / N$$\n    \"\"\"\n    sum_inverted_ranks = 0\n    for pred, cands in prediction_and_candidates:\n        ordered_candidates = [c.to_tuple() for c in EntityMapping.sort_entity_mappings_by_score(cands)]\n        if pred.to_tuple() in ordered_candidates:\n            rank = ordered_candidates.index(pred) + 1\n        else:\n            rank = math.inf\n        sum_inverted_ranks += 1 / rank\n    return sum_inverted_ranks / len(prediction_and_candidates)\n</code></pre>"},{"location":"deeponto/align/mapping/","title":"Mapping","text":""},{"location":"deeponto/align/mapping/#deeponto.align.mapping.EntityMapping","title":"<code>EntityMapping</code>","text":"<p>A datastructure for entity mapping.</p> <p>Such entities should be named and have an IRI.</p> <p>Attributes:</p> Name Type Description <code>src_entity_iri</code> <code>str</code> <p>The IRI of the source entity, usually its IRI if available.</p> <code>tgt_entity_iri</code> <code>str</code> <p>The IRI of the target entity, usually its IRI if available.</p> <code>relation</code> <code>str</code> <p>A symbol that represents what semantic relation this mapping stands for. Defaults to <code>&lt;?rel&gt;</code> which means unspecified. Suggested inputs are <code>\"&lt;EquivalentTo&gt;\"</code> and <code>\"&lt;SubsumedBy&gt;\"</code>.</p> <code>score</code> <code>float</code> <p>The score that indicates the confidence of this mapping. Defaults to <code>0.0</code>.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>class EntityMapping:\nr\"\"\"A datastructure for entity mapping.\n\n    Such entities should be named and have an IRI.\n\n    Attributes:\n        src_entity_iri (str): The IRI of the source entity, usually its IRI if available.\n        tgt_entity_iri (str): The IRI of the target entity, usually its IRI if available.\n        relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n            Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n        score (float, optional): The score that indicates the confidence of this mapping. Defaults to `0.0`.\n    \"\"\"\n\n    def __init__(self, src_entity_iri: str, tgt_entity_iri: str, relation: str = DEFAULT_REL, score: float = 0.0):\n\"\"\"Intialise an entity mapping.\n\n        Args:\n            src_entity_iri (str): The IRI of the source entity, usually its IRI if available.\n            tgt_entity_iri (str): The IRI of the target entity, usually its IRI if available.\n            relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n                Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n            score (float, optional): The score that indicates the confidence of this mapping. Defaults to `0.0`.\n        \"\"\"\n        self.head = src_entity_iri\n        self.tail = tgt_entity_iri\n        self.relation = relation\n        self.score = score\n\n    @classmethod\n    def from_owl_objects(\n        cls, src_entity: OWLObject, tgt_entity: OWLObject, relation: str = DEFAULT_REL, score: float = 0.0\n    ):\n\"\"\"Create an entity mapping from two `OWLObject` entities which have an IRI.\n\n        Args:\n            src_entity (OWLObject): The source entity in `OWLObject`.\n            tgt_entity (OWLObject): The target entity in `OWLObject`.\n            relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n                Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n            score (float, optional): The score that indicates the confidence of this mapping. Defaults to `0.0`.\n        Returns:\n            (EntityMapping): The entity mapping created from the source and target entities.\n        \"\"\"\n        return cls(str(src_entity.getIRI()), str(tgt_entity.getIRI()), relation, score)\n\n    def to_tuple(self, with_score: bool = False):\n\"\"\"Transform an entity mapping (`self`) to a tuple representation\n\n        Note that `relation` is discarded and `score` is optionally preserved).\n        \"\"\"\n        if with_score:\n            return (self.head, self.tail, self.score)\n        else:\n            return (self.head, self.tail)\n\n    @staticmethod\n    def as_tuples(entity_mappings: List[EntityMapping], with_score: bool = False):\n\"\"\"Transform a list of entity mappings to their tuple representations.\n\n        Note that `relation` is discarded and `score` is optionally preserved).\n        \"\"\"\n        return [m.to_tuple(with_score=with_score) for m in entity_mappings]\n\n    @staticmethod\n    def sort_entity_mappings_by_score(entity_mappings: List[EntityMapping], k: Optional[int] = None):\nr\"\"\"Sort the entity mappings in a list by their scores in descending order.\n\n        Args:\n            entity_mappings (List[EntityMapping]): A list entity mappings to sort.\n            k (int, optional): The number of top $k$ scored entities preserved if specified. Defaults to `None` which\n                means to return **all** entity mappings.\n\n        Returns:\n            (List[EntityMapping]): A list of sorted entity mappings.\n        \"\"\"\n        return list(sorted(entity_mappings, key=lambda x: x.score, reverse=True))[:k]\n\n    @staticmethod\n    def read_table_mappings(\n        table_of_mappings_file: str,\n        threshold: Optional[float] = 0.0,\n        relation: str = DEFAULT_REL,\n        is_reference: bool = False,\n    ) -&gt; List[EntityMapping]:\nr\"\"\"Read entity mappings from `.csv` or `.tsv` files.\n\n        !!! note\n\n            The columns of the mapping table must have the headings: `\"SrcEntity\"`, `\"TgtEntity\"`, and `\"Score\"`.\n\n        Args:\n            table_of_mappings_file (str): The path to the table (`.csv` or `.tsv`) of mappings.\n            threshold (Optional[float], optional): Mappings with scores less than `threshold` will not be loaded. Defaults to 0.0.\n            relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n                Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n            is_reference (bool): Whether the loaded mappings are reference mappigns; if so, `threshold` is disabled and mapping scores\n                are all set to $1.0$. Defaults to `False`.\n\n        Returns:\n            (List[EntityMapping]): A list of entity mappings loaded from the table file.\n        \"\"\"\n        df = FileUtils.read_table(table_of_mappings_file)\n        entity_mappings = []\n        for _, dp in df.iterrows():\n            if is_reference:\n                entity_mappings.append(ReferenceMapping(dp[\"SrcEntity\"], dp[\"TgtEntity\"], relation))\n            else:\n                if dp[\"Score\"] &gt;= threshold:\n                    entity_mappings.append(EntityMapping(dp[\"SrcEntity\"], dp[\"TgtEntity\"], relation, dp[\"Score\"]))\n        return entity_mappings\n\n    def __repr__(self):\n        return f\"EntityMapping({self.head} {self.relation} {self.tail}, {round(self.score, 6)})\"\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.EntityMapping.__init__","title":"<code>__init__(src_entity_iri, tgt_entity_iri, relation=DEFAULT_REL, score=0.0)</code>","text":"<p>Intialise an entity mapping.</p> <p>Parameters:</p> Name Type Description Default <code>src_entity_iri</code> <code>str</code> <p>The IRI of the source entity, usually its IRI if available.</p> required <code>tgt_entity_iri</code> <code>str</code> <p>The IRI of the target entity, usually its IRI if available.</p> required <code>relation</code> <code>str</code> <p>A symbol that represents what semantic relation this mapping stands for. Defaults to <code>&lt;?rel&gt;</code> which means unspecified. Suggested inputs are <code>\"&lt;EquivalentTo&gt;\"</code> and <code>\"&lt;SubsumedBy&gt;\"</code>.</p> <code>DEFAULT_REL</code> <code>score</code> <code>float</code> <p>The score that indicates the confidence of this mapping. Defaults to <code>0.0</code>.</p> <code>0.0</code> Source code in <code>deeponto/align/mapping.py</code> <pre><code>def __init__(self, src_entity_iri: str, tgt_entity_iri: str, relation: str = DEFAULT_REL, score: float = 0.0):\n\"\"\"Intialise an entity mapping.\n\n    Args:\n        src_entity_iri (str): The IRI of the source entity, usually its IRI if available.\n        tgt_entity_iri (str): The IRI of the target entity, usually its IRI if available.\n        relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n            Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n        score (float, optional): The score that indicates the confidence of this mapping. Defaults to `0.0`.\n    \"\"\"\n    self.head = src_entity_iri\n    self.tail = tgt_entity_iri\n    self.relation = relation\n    self.score = score\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.EntityMapping.from_owl_objects","title":"<code>from_owl_objects(src_entity, tgt_entity, relation=DEFAULT_REL, score=0.0)</code>  <code>classmethod</code>","text":"<p>Create an entity mapping from two <code>OWLObject</code> entities which have an IRI.</p> <p>Parameters:</p> Name Type Description Default <code>src_entity</code> <code>OWLObject</code> <p>The source entity in <code>OWLObject</code>.</p> required <code>tgt_entity</code> <code>OWLObject</code> <p>The target entity in <code>OWLObject</code>.</p> required <code>relation</code> <code>str</code> <p>A symbol that represents what semantic relation this mapping stands for. Defaults to <code>&lt;?rel&gt;</code> which means unspecified. Suggested inputs are <code>\"&lt;EquivalentTo&gt;\"</code> and <code>\"&lt;SubsumedBy&gt;\"</code>.</p> <code>DEFAULT_REL</code> <code>score</code> <code>float</code> <p>The score that indicates the confidence of this mapping. Defaults to <code>0.0</code>.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>EntityMapping</code> <p>The entity mapping created from the source and target entities.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>@classmethod\ndef from_owl_objects(\n    cls, src_entity: OWLObject, tgt_entity: OWLObject, relation: str = DEFAULT_REL, score: float = 0.0\n):\n\"\"\"Create an entity mapping from two `OWLObject` entities which have an IRI.\n\n    Args:\n        src_entity (OWLObject): The source entity in `OWLObject`.\n        tgt_entity (OWLObject): The target entity in `OWLObject`.\n        relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n            Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n        score (float, optional): The score that indicates the confidence of this mapping. Defaults to `0.0`.\n    Returns:\n        (EntityMapping): The entity mapping created from the source and target entities.\n    \"\"\"\n    return cls(str(src_entity.getIRI()), str(tgt_entity.getIRI()), relation, score)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.EntityMapping.to_tuple","title":"<code>to_tuple(with_score=False)</code>","text":"<p>Transform an entity mapping (<code>self</code>) to a tuple representation</p> <p>Note that <code>relation</code> is discarded and <code>score</code> is optionally preserved).</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>def to_tuple(self, with_score: bool = False):\n\"\"\"Transform an entity mapping (`self`) to a tuple representation\n\n    Note that `relation` is discarded and `score` is optionally preserved).\n    \"\"\"\n    if with_score:\n        return (self.head, self.tail, self.score)\n    else:\n        return (self.head, self.tail)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.EntityMapping.as_tuples","title":"<code>as_tuples(entity_mappings, with_score=False)</code>  <code>staticmethod</code>","text":"<p>Transform a list of entity mappings to their tuple representations.</p> <p>Note that <code>relation</code> is discarded and <code>score</code> is optionally preserved).</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>@staticmethod\ndef as_tuples(entity_mappings: List[EntityMapping], with_score: bool = False):\n\"\"\"Transform a list of entity mappings to their tuple representations.\n\n    Note that `relation` is discarded and `score` is optionally preserved).\n    \"\"\"\n    return [m.to_tuple(with_score=with_score) for m in entity_mappings]\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.EntityMapping.sort_entity_mappings_by_score","title":"<code>sort_entity_mappings_by_score(entity_mappings, k=None)</code>  <code>staticmethod</code>","text":"<p>Sort the entity mappings in a list by their scores in descending order.</p> <p>Parameters:</p> Name Type Description Default <code>entity_mappings</code> <code>List[EntityMapping]</code> <p>A list entity mappings to sort.</p> required <code>k</code> <code>int</code> <p>The number of top \\(k\\) scored entities preserved if specified. Defaults to <code>None</code> which means to return all entity mappings.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[EntityMapping]</code> <p>A list of sorted entity mappings.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>@staticmethod\ndef sort_entity_mappings_by_score(entity_mappings: List[EntityMapping], k: Optional[int] = None):\nr\"\"\"Sort the entity mappings in a list by their scores in descending order.\n\n    Args:\n        entity_mappings (List[EntityMapping]): A list entity mappings to sort.\n        k (int, optional): The number of top $k$ scored entities preserved if specified. Defaults to `None` which\n            means to return **all** entity mappings.\n\n    Returns:\n        (List[EntityMapping]): A list of sorted entity mappings.\n    \"\"\"\n    return list(sorted(entity_mappings, key=lambda x: x.score, reverse=True))[:k]\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.EntityMapping.read_table_mappings","title":"<code>read_table_mappings(table_of_mappings_file, threshold=0.0, relation=DEFAULT_REL, is_reference=False)</code>  <code>staticmethod</code>","text":"<p>Read entity mappings from <code>.csv</code> or <code>.tsv</code> files.</p> <p>Note</p> <p>The columns of the mapping table must have the headings: <code>\"SrcEntity\"</code>, <code>\"TgtEntity\"</code>, and <code>\"Score\"</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table_of_mappings_file</code> <code>str</code> <p>The path to the table (<code>.csv</code> or <code>.tsv</code>) of mappings.</p> required <code>threshold</code> <code>Optional[float]</code> <p>Mappings with scores less than <code>threshold</code> will not be loaded. Defaults to 0.0.</p> <code>0.0</code> <code>relation</code> <code>str</code> <p>A symbol that represents what semantic relation this mapping stands for. Defaults to <code>&lt;?rel&gt;</code> which means unspecified. Suggested inputs are <code>\"&lt;EquivalentTo&gt;\"</code> and <code>\"&lt;SubsumedBy&gt;\"</code>.</p> <code>DEFAULT_REL</code> <code>is_reference</code> <code>bool</code> <p>Whether the loaded mappings are reference mappigns; if so, <code>threshold</code> is disabled and mapping scores are all set to \\(1.0\\). Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[EntityMapping]</code> <p>A list of entity mappings loaded from the table file.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>@staticmethod\ndef read_table_mappings(\n    table_of_mappings_file: str,\n    threshold: Optional[float] = 0.0,\n    relation: str = DEFAULT_REL,\n    is_reference: bool = False,\n) -&gt; List[EntityMapping]:\nr\"\"\"Read entity mappings from `.csv` or `.tsv` files.\n\n    !!! note\n\n        The columns of the mapping table must have the headings: `\"SrcEntity\"`, `\"TgtEntity\"`, and `\"Score\"`.\n\n    Args:\n        table_of_mappings_file (str): The path to the table (`.csv` or `.tsv`) of mappings.\n        threshold (Optional[float], optional): Mappings with scores less than `threshold` will not be loaded. Defaults to 0.0.\n        relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n            Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n        is_reference (bool): Whether the loaded mappings are reference mappigns; if so, `threshold` is disabled and mapping scores\n            are all set to $1.0$. Defaults to `False`.\n\n    Returns:\n        (List[EntityMapping]): A list of entity mappings loaded from the table file.\n    \"\"\"\n    df = FileUtils.read_table(table_of_mappings_file)\n    entity_mappings = []\n    for _, dp in df.iterrows():\n        if is_reference:\n            entity_mappings.append(ReferenceMapping(dp[\"SrcEntity\"], dp[\"TgtEntity\"], relation))\n        else:\n            if dp[\"Score\"] &gt;= threshold:\n                entity_mappings.append(EntityMapping(dp[\"SrcEntity\"], dp[\"TgtEntity\"], relation, dp[\"Score\"]))\n    return entity_mappings\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.ReferenceMapping","title":"<code>ReferenceMapping</code>","text":"<p>         Bases: <code>EntityMapping</code></p> <p>A datastructure for entity mapping that acts as a reference mapping.</p> <p>A reference mapppings is a ground truth entity mapping (with \\(score = 1.0\\)) and can have several entity mappings as candidates. These candidate mappings should have the same <code>head</code> (i.e., source entity) as the reference mapping.</p> <p>Attributes:</p> Name Type Description <code>src_entity_iri</code> <code>str</code> <p>The IRI of the source entity, usually its IRI if available.</p> <code>tgt_entity_iri</code> <code>str</code> <p>The IRI of the target entity, usually its IRI if available.</p> <code>relation</code> <code>str</code> <p>A symbol that represents what semantic relation this mapping stands for. Defaults to <code>&lt;?rel&gt;</code> which means unspecified. Suggested inputs are <code>\"&lt;EquivalentTo&gt;\"</code> and <code>\"&lt;SubsumedBy&gt;\"</code>.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>class ReferenceMapping(EntityMapping):\nr\"\"\"A datastructure for entity mapping that acts as a reference mapping.\n\n    A reference mapppings is a ground truth entity mapping (with $score = 1.0$) and can\n    have several entity mappings as candidates. These candidate mappings should have the\n    same `head` (i.e., source entity) as the reference mapping.\n\n    Attributes:\n        src_entity_iri (str): The IRI of the source entity, usually its IRI if available.\n        tgt_entity_iri (str): The IRI of the target entity, usually its IRI if available.\n        relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n            Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n    \"\"\"\n\n    def __init__(\n        self,\n        src_entity_iri: str,\n        tgt_entity_iri: str,\n        relation: str = DEFAULT_REL,\n        candidate_mappings: Optional[List[EntityMapping]] = [],\n    ):\nr\"\"\"Intialise a reference mapping.\n\n        Args:\n            src_entity_iri (str): The IRI of the source entity, usually its IRI if available.\n            tgt_entity_iri (str): The IRI of the target entity, usually its IRI if available.\n            relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n                Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n            candidate_mappings (List[EntityMapping], optional): A list of entity mappings that are candidates for this reference mapping. Defaults to `[]`.\n        \"\"\"\n        super().__init__(src_entity_iri, tgt_entity_iri, relation, 1.0)\n        self.candidates = []\n        for candidate in candidate_mappings:\n            self.add_candidate(candidate)\n\n    def __repr__(self):\n        reference_mapping_str = f\"ReferenceMapping({self.head} {self.relation} {self.tail}, 1.0)\"\n        if self.candidates:\n            candidate_mapping_str = pprintpp.pformat(self.candidates)\n            reference_mapping_str += f\" with candidates:\\n{candidate_mapping_str}\"\n        return reference_mapping_str\n\n    def add_candidate(self, candidate_mapping: EntityMapping):\n\"\"\"Add a candidate mapping whose relation and head entity are the\n        same as the reference mapping's.\n        \"\"\"\n        if self.relation != candidate_mapping.relation:\n            raise ValueError(\n                f\"Expect relation of candidate mapping to be {self.relation} but got {candidate_mapping.relation}\"\n            )\n        if self.head != candidate_mapping.head:\n            raise ValueError(\"Candidate mapping does not have the same head entity as the anchor mapping.\")\n        self.candidates.append(candidate_mapping)\n\n    @staticmethod\n    def read_table_mappings(table_of_mappings_file: str, relation: str = DEFAULT_REL):\nr\"\"\"Read reference mappings from `.csv` or `.tsv` files.\n\n        !!! note\n\n            The columns of the mapping table must have the headings: `\"SrcEntity\"`, `\"TgtEntity\"`, and `\"Score\"`.\n\n        Args:\n            table_of_mappings_file (str): The path to the table (`.csv` or `.tsv`) of mappings.\n            relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n                Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n\n        Returns:\n            (List[ReferenceMapping]): A list of reference mappings loaded from the table file.\n        \"\"\"\n        return EntityMapping.read_table_mappings(table_of_mappings_file, relation=relation, is_reference=True)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.ReferenceMapping.__init__","title":"<code>__init__(src_entity_iri, tgt_entity_iri, relation=DEFAULT_REL, candidate_mappings=[])</code>","text":"<p>Intialise a reference mapping.</p> <p>Parameters:</p> Name Type Description Default <code>src_entity_iri</code> <code>str</code> <p>The IRI of the source entity, usually its IRI if available.</p> required <code>tgt_entity_iri</code> <code>str</code> <p>The IRI of the target entity, usually its IRI if available.</p> required <code>relation</code> <code>str</code> <p>A symbol that represents what semantic relation this mapping stands for. Defaults to <code>&lt;?rel&gt;</code> which means unspecified. Suggested inputs are <code>\"&lt;EquivalentTo&gt;\"</code> and <code>\"&lt;SubsumedBy&gt;\"</code>.</p> <code>DEFAULT_REL</code> <code>candidate_mappings</code> <code>List[EntityMapping]</code> <p>A list of entity mappings that are candidates for this reference mapping. Defaults to <code>[]</code>.</p> <code>[]</code> Source code in <code>deeponto/align/mapping.py</code> <pre><code>def __init__(\n    self,\n    src_entity_iri: str,\n    tgt_entity_iri: str,\n    relation: str = DEFAULT_REL,\n    candidate_mappings: Optional[List[EntityMapping]] = [],\n):\nr\"\"\"Intialise a reference mapping.\n\n    Args:\n        src_entity_iri (str): The IRI of the source entity, usually its IRI if available.\n        tgt_entity_iri (str): The IRI of the target entity, usually its IRI if available.\n        relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n            Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n        candidate_mappings (List[EntityMapping], optional): A list of entity mappings that are candidates for this reference mapping. Defaults to `[]`.\n    \"\"\"\n    super().__init__(src_entity_iri, tgt_entity_iri, relation, 1.0)\n    self.candidates = []\n    for candidate in candidate_mappings:\n        self.add_candidate(candidate)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.ReferenceMapping.add_candidate","title":"<code>add_candidate(candidate_mapping)</code>","text":"<p>Add a candidate mapping whose relation and head entity are the same as the reference mapping's.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>def add_candidate(self, candidate_mapping: EntityMapping):\n\"\"\"Add a candidate mapping whose relation and head entity are the\n    same as the reference mapping's.\n    \"\"\"\n    if self.relation != candidate_mapping.relation:\n        raise ValueError(\n            f\"Expect relation of candidate mapping to be {self.relation} but got {candidate_mapping.relation}\"\n        )\n    if self.head != candidate_mapping.head:\n        raise ValueError(\"Candidate mapping does not have the same head entity as the anchor mapping.\")\n    self.candidates.append(candidate_mapping)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.ReferenceMapping.read_table_mappings","title":"<code>read_table_mappings(table_of_mappings_file, relation=DEFAULT_REL)</code>  <code>staticmethod</code>","text":"<p>Read reference mappings from <code>.csv</code> or <code>.tsv</code> files.</p> <p>Note</p> <p>The columns of the mapping table must have the headings: <code>\"SrcEntity\"</code>, <code>\"TgtEntity\"</code>, and <code>\"Score\"</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table_of_mappings_file</code> <code>str</code> <p>The path to the table (<code>.csv</code> or <code>.tsv</code>) of mappings.</p> required <code>relation</code> <code>str</code> <p>A symbol that represents what semantic relation this mapping stands for. Defaults to <code>&lt;?rel&gt;</code> which means unspecified. Suggested inputs are <code>\"&lt;EquivalentTo&gt;\"</code> and <code>\"&lt;SubsumedBy&gt;\"</code>.</p> <code>DEFAULT_REL</code> <p>Returns:</p> Type Description <code>List[ReferenceMapping]</code> <p>A list of reference mappings loaded from the table file.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>@staticmethod\ndef read_table_mappings(table_of_mappings_file: str, relation: str = DEFAULT_REL):\nr\"\"\"Read reference mappings from `.csv` or `.tsv` files.\n\n    !!! note\n\n        The columns of the mapping table must have the headings: `\"SrcEntity\"`, `\"TgtEntity\"`, and `\"Score\"`.\n\n    Args:\n        table_of_mappings_file (str): The path to the table (`.csv` or `.tsv`) of mappings.\n        relation (str, optional): A symbol that represents what semantic relation this mapping stands for. Defaults to `&lt;?rel&gt;` which means unspecified.\n            Suggested inputs are `\"&lt;EquivalentTo&gt;\"` and `\"&lt;SubsumedBy&gt;\"`.\n\n    Returns:\n        (List[ReferenceMapping]): A list of reference mappings loaded from the table file.\n    \"\"\"\n    return EntityMapping.read_table_mappings(table_of_mappings_file, relation=relation, is_reference=True)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.SubsFromEquivMappingGenerator","title":"<code>SubsFromEquivMappingGenerator</code>","text":"<p>Generating subsumption mappings from gold standard equivalence mappings.</p> <p>paper</p> <p>The online subsumption mapping construction algorithm is proposed in the paper: Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022).</p> <p>This generator has an attribute <code>delete_used_equiv_tgt_class</code> for determining whether or not to sabotage the equivalence mappings used to create \\(\\geq 1\\) subsumption mappings. The reason is that, if the equivalence mapping is broken, then the OM tool is expected to predict subsumption mappings directly without relying on the equivalence mappings as an intermediate.</p> <p>Attributes:</p> Name Type Description <code>src_onto</code> <code>Ontology</code> <p>The source ontology.</p> <code>tgt_onto</code> <code>Ontology</code> <p>The target ontology.</p> <code>equiv_class_pairs</code> <code>List[Tuple[str, str]]</code> <p>A list of class pairs (in IRIs) that are equivalent according to the input equivalence mappings.</p> <code>subs_generation_ratio</code> <code>int</code> <p>The maximum number of subsumption mappings generated from each equivalence mapping. Defaults to <code>None</code> which means there is no limit on the number of subsumption mappings.</p> <code>delete_used_equiv_tgt_class</code> <code>bool</code> <p>Whether to mark the target side of an equivalence mapping used for creating at least one subsumption mappings as \"deleted\". Defaults to <code>True</code>.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>class SubsFromEquivMappingGenerator:\nr\"\"\"Generating subsumption mappings from gold standard equivalence mappings.\n\n    !!! credit \"paper\"\n\n        The online subsumption mapping construction algorithm is proposed in the paper:\n        [*Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022)*](https://link.springer.com/chapter/10.1007/978-3-031-19433-7_33).\n\n    This generator has an attribute `delete_used_equiv_tgt_class` for determining whether or not to sabotage the equivalence\n    mappings used to create $\\geq 1$ subsumption mappings. The reason is that, if the equivalence mapping is broken, then the\n    OM tool is expected to predict subsumption mappings directly without relying on the equivalence mappings as an intermediate.\n\n    Attributes:\n        src_onto (Ontology): The source ontology.\n        tgt_onto (Ontology): The target ontology.\n        equiv_class_pairs (List[Tuple[str, str]]): A list of class pairs (in IRIs) that are **equivalent** according to the input\n            equivalence mappings.\n        subs_generation_ratio (int, optional): The maximum number of subsumption mappings generated from each equivalence\n            mapping. Defaults to `None` which means there is no limit on the number of subsumption mappings.\n        delete_used_equiv_tgt_class (bool): Whether to mark the target side of an equivalence mapping **used** for creating\n            at least one subsumption mappings as \"deleted\". Defaults to `True`.\n    \"\"\"\n\n    def __init__(\n        self,\n        src_onto: Ontology,\n        tgt_onto: Ontology,\n        equiv_mappings: List[ReferenceMapping],\n        subs_generation_ratio: Optional[int] = None,\n        delete_used_equiv_tgt_class: bool = True,\n    ):\n        self.src_onto = src_onto\n        self.tgt_onto = tgt_onto\n        self.equiv_class_pairs = [m.to_tuple() for m in equiv_mappings]\n        self.subs_generation_ratio = subs_generation_ratio\n        self.delete_used_equiv_tgt_class = delete_used_equiv_tgt_class\n\n        subs_from_equivs, self.used_equiv_tgt_class_iris = self.online_construction()\n        # turn into triples with scores 1.0\n        self.subs_from_equivs = [(c, p, 1.0) for c, p in subs_from_equivs]\n\n    def online_construction(self):\nr\"\"\"An **online** algorithm for constructing subsumption mappings from gold standard equivalence mappings.\n\n        Let $t$ denote the boolean value that indicates if the target class involved in an equivalence mapping\n        will be deleted. If $t$ is true, then for each equivalent class pair $(c, c')$, do the following:\n\n        1. If $c'$ has been inolved in a subsumption mapping, skip this pair as otherwise $c'$ will need to be deleted.\n        2. For each parent class of $c'$, skip it if it has been marked deleted (i.e., involved in an equivalence mapping that has been used to create a subsumption mapping).\n        3. If any subsumption mapping has been created from $(c, c')$, mark $c'$ as deleted.\n\n        Steps 1 and 2 ensure that target classes that have been **involved in a subsumption mapping** have **no conflicts** with\n        target classes that have been **used to create a subsumption mapping**.\n\n        This algorithm is *online* because the construction and deletion depend on the order of the input equivalent class pairs.\n        \"\"\"\n        subs_class_pairs = []\n        in_subs = defaultdict(lambda: False)  # in a subsumption mapping\n        used_equivs = defaultdict(lambda: False)  # in a used equivalence mapping\n\n        for src_class_iri, tgt_class_iri in self.equiv_class_pairs:\n\n            cur_subs_pairs = []\n\n            # NOTE (1) an equiv pair is skipped if the target side is marked constructed\n            if self.delete_used_equiv_tgt_class and in_subs[tgt_class_iri]:\n                continue\n\n            # construct subsumption pairs by matching the source class and the target class's parents\n            tgt_class = self.tgt_onto.get_owl_object_from_iri(tgt_class_iri)\n            tgt_class_parent_iris = self.tgt_onto.reasoner.super_entities_of(tgt_class, direct=True)\n            for parent_iri in tgt_class_parent_iris:\n                # skip this parent if it is marked as \"used\"\n                if self.delete_used_equiv_tgt_class and used_equivs[parent_iri]:\n                    continue\n                cur_subs_pairs.append((src_class_iri, parent_iri))\n                # if successfully created, mark this parent as \"in\"\n                if self.delete_used_equiv_tgt_class:\n                    in_subs[parent_iri] = True\n\n            # mark the target class as \"used\" because it has been used for creating a subsumption mapping\n            if self.delete_used_equiv_tgt_class and cur_subs_pairs:\n                used_equivs[tgt_class_iri] = True\n\n            if self.subs_generation_ratio and len(cur_subs_pairs) &gt; self.subs_generation_ratio:\n                cur_subs_pairs = random.sample(cur_subs_pairs, self.subs_generation_ratio)\n            subs_class_pairs += cur_subs_pairs\n\n        used_equiv_tgt_class_iris = None\n        if self.delete_used_equiv_tgt_class:\n            used_equiv_tgt_class_iris = [iri for iri, used in used_equivs.items() if used is True]\n            print(\n                f\"{len(used_equiv_tgt_class_iris)}/{len(self.equiv_class_pairs)} are used for creating at least one subsumption mapping.\"\n            )\n\n        subs_class_pairs = DataUtils.uniqify(subs_class_pairs)\n        print(f\"{len(subs_class_pairs)} subsumption mappings are created in the end.\")\n\n        return subs_class_pairs, used_equiv_tgt_class_iris\n\n    def save_subs(self, save_path: str):\n\"\"\"Save the constructed subsumption mappings (in tuples) to a local `.tsv` file.\"\"\"\n        subs_df = pd.DataFrame(self.subs_from_equivs, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n        subs_df.to_csv(save_path, sep=\"\\t\", index=False)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.SubsFromEquivMappingGenerator.online_construction","title":"<code>online_construction()</code>","text":"<p>An online algorithm for constructing subsumption mappings from gold standard equivalence mappings.</p> <p>Let \\(t\\) denote the boolean value that indicates if the target class involved in an equivalence mapping will be deleted. If \\(t\\) is true, then for each equivalent class pair \\((c, c')\\), do the following:</p> <ol> <li>If \\(c'\\) has been inolved in a subsumption mapping, skip this pair as otherwise \\(c'\\) will need to be deleted.</li> <li>For each parent class of \\(c'\\), skip it if it has been marked deleted (i.e., involved in an equivalence mapping that has been used to create a subsumption mapping).</li> <li>If any subsumption mapping has been created from \\((c, c')\\), mark \\(c'\\) as deleted.</li> </ol> <p>Steps 1 and 2 ensure that target classes that have been involved in a subsumption mapping have no conflicts with target classes that have been used to create a subsumption mapping.</p> <p>This algorithm is online because the construction and deletion depend on the order of the input equivalent class pairs.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>def online_construction(self):\nr\"\"\"An **online** algorithm for constructing subsumption mappings from gold standard equivalence mappings.\n\n    Let $t$ denote the boolean value that indicates if the target class involved in an equivalence mapping\n    will be deleted. If $t$ is true, then for each equivalent class pair $(c, c')$, do the following:\n\n    1. If $c'$ has been inolved in a subsumption mapping, skip this pair as otherwise $c'$ will need to be deleted.\n    2. For each parent class of $c'$, skip it if it has been marked deleted (i.e., involved in an equivalence mapping that has been used to create a subsumption mapping).\n    3. If any subsumption mapping has been created from $(c, c')$, mark $c'$ as deleted.\n\n    Steps 1 and 2 ensure that target classes that have been **involved in a subsumption mapping** have **no conflicts** with\n    target classes that have been **used to create a subsumption mapping**.\n\n    This algorithm is *online* because the construction and deletion depend on the order of the input equivalent class pairs.\n    \"\"\"\n    subs_class_pairs = []\n    in_subs = defaultdict(lambda: False)  # in a subsumption mapping\n    used_equivs = defaultdict(lambda: False)  # in a used equivalence mapping\n\n    for src_class_iri, tgt_class_iri in self.equiv_class_pairs:\n\n        cur_subs_pairs = []\n\n        # NOTE (1) an equiv pair is skipped if the target side is marked constructed\n        if self.delete_used_equiv_tgt_class and in_subs[tgt_class_iri]:\n            continue\n\n        # construct subsumption pairs by matching the source class and the target class's parents\n        tgt_class = self.tgt_onto.get_owl_object_from_iri(tgt_class_iri)\n        tgt_class_parent_iris = self.tgt_onto.reasoner.super_entities_of(tgt_class, direct=True)\n        for parent_iri in tgt_class_parent_iris:\n            # skip this parent if it is marked as \"used\"\n            if self.delete_used_equiv_tgt_class and used_equivs[parent_iri]:\n                continue\n            cur_subs_pairs.append((src_class_iri, parent_iri))\n            # if successfully created, mark this parent as \"in\"\n            if self.delete_used_equiv_tgt_class:\n                in_subs[parent_iri] = True\n\n        # mark the target class as \"used\" because it has been used for creating a subsumption mapping\n        if self.delete_used_equiv_tgt_class and cur_subs_pairs:\n            used_equivs[tgt_class_iri] = True\n\n        if self.subs_generation_ratio and len(cur_subs_pairs) &gt; self.subs_generation_ratio:\n            cur_subs_pairs = random.sample(cur_subs_pairs, self.subs_generation_ratio)\n        subs_class_pairs += cur_subs_pairs\n\n    used_equiv_tgt_class_iris = None\n    if self.delete_used_equiv_tgt_class:\n        used_equiv_tgt_class_iris = [iri for iri, used in used_equivs.items() if used is True]\n        print(\n            f\"{len(used_equiv_tgt_class_iris)}/{len(self.equiv_class_pairs)} are used for creating at least one subsumption mapping.\"\n        )\n\n    subs_class_pairs = DataUtils.uniqify(subs_class_pairs)\n    print(f\"{len(subs_class_pairs)} subsumption mappings are created in the end.\")\n\n    return subs_class_pairs, used_equiv_tgt_class_iris\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.SubsFromEquivMappingGenerator.save_subs","title":"<code>save_subs(save_path)</code>","text":"<p>Save the constructed subsumption mappings (in tuples) to a local <code>.tsv</code> file.</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>def save_subs(self, save_path: str):\n\"\"\"Save the constructed subsumption mappings (in tuples) to a local `.tsv` file.\"\"\"\n    subs_df = pd.DataFrame(self.subs_from_equivs, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n    subs_df.to_csv(save_path, sep=\"\\t\", index=False)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.NegativeCandidateMappingGenerator","title":"<code>NegativeCandidateMappingGenerator</code>","text":"<p>Generating negative candidate mappings for each gold standard mapping.</p> <p>Note that the source side of the golden standard mapping is fixed, i.e., candidate mappings are generated according to the target side.</p> <p>paper</p> <p>The candidate mapping generation algorithm is proposed in the paper: Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022).</p> Source code in <code>deeponto/align/mapping.py</code> <pre><code>class NegativeCandidateMappingGenerator:\nr\"\"\"Generating **negative** candidate mappings for each gold standard mapping.\n\n    Note that the source side of the golden standard mapping is fixed, i.e., candidate mappings are generated\n    according to the target side.\n\n    !!! credit \"paper\"\n\n        The candidate mapping generation algorithm is proposed in the paper:\n        [*Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022)*](https://link.springer.com/chapter/10.1007/978-3-031-19433-7_33).\n    \"\"\"\n\n    def __init__(\n        self,\n        src_onto: Ontology,\n        tgt_onto: Ontology,\n        reference_class_mappings: List[ReferenceMapping],  # equivalence or subsumption\n        annotation_property_iris: List[str],  # for text-based candidates\n        tokenizer: Tokenizer,  # for text-based candidates\n        max_hops: int = 5,  # for graph-based candidates\n        for_subsumption: bool = False,  # if for subsumption, avoid adding ancestors as candidates\n    ):\n\n        self.src_onto = src_onto\n        self.tgt_onto = tgt_onto\n        self.reference_class_mappings = reference_class_mappings\n        self.reference_class_dict = defaultdict(list)  # to prevent wrongly adding negative candidates\n        for m in self.reference_class_mappings:\n            src_class_iri, tgt_class_iri = m.to_tuple()\n            self.reference_class_dict[src_class_iri].append(tgt_class_iri)\n\n        # for IDF sample\n        self.tgt_annotation_index, self.annotation_property_iris = self.tgt_onto.build_annotation_index(\n            annotation_property_iris\n        )\n        self.tokenizer = tokenizer\n        self.tgt_inverted_annotation_index = self.tgt_onto.build_inverted_annotation_index(\n            self.tgt_annotation_index, self.tokenizer\n        )\n\n        # for neighbour sample\n        self.max_hops = max_hops\n\n        # if for subsumption, avoid adding ancestors as candidates\n        self.for_subsumption = for_subsumption\n        # if for subsumption, add (src_class, tgt_class_ancestor) into the reference mappings\n        if self.for_subsumption:\n            for m in self.reference_class_mappings:\n                src_class_iri, tgt_class_iri = m.to_tuple()\n                tgt_class = self.tgt_onto.get_owl_object_from_iri(tgt_class_iri)\n                tgt_class_ancestors = self.tgt_onto.reasoner.super_entities_of(tgt_class)\n                for tgt_ancestor_iri in tgt_class_ancestors:\n                    self.reference_class_dict[src_class_iri].append(tgt_ancestor_iri)\n\n\n    def mixed_sample(self, reference_class_mapping: ReferenceMapping, **strategy2nums):\n\"\"\"A mixed sampling approach that combines several sampling strategies.\n\n        As introduced in the Bio-ML paper, this mixed approach guarantees that the number of samples for each\n        strategy is either the **maximum that can be sampled** or the required number.\n\n        Specifically, at each sampling iteration, the number of candidates is **first increased by the number of \n        previously sampled candidates**, as in the worst case, all the candidates sampled at this iteration\n        will be duplicated with the previous. \n\n        The random sampling is used as the amending strategy, i.e., if other sampling strategies cannot retrieve\n        the specified number of samples, then use random sampling to amend the number.\n\n        Args:\n            reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n            **strategy2nums (int): The keyword arguments that specify the expected number of candidates for each\n                sampling strategy.\n        \"\"\"\n\n        valid_tgt_candidate_iris = []\n        sample_stats = defaultdict(lambda: 0)\n        i = 0\n        total_num_candidates = 0\n        for strategy, num_canddiates in strategy2nums.items():\n            i += 1\n            if strategy in SAMPLING_OPTIONS:\n                sampler = getattr(self, f\"{strategy}_sample\")\n                # for ith iteration, the worst case is when all n_cands are duplicated\n                # or should be excluded from other reference targets so we generate\n                # NOTE:  total_num_candidates + num_candidates + len(excluded_tgt_class_iris)\n                # candidates first and prune the rest; another edge case is when sampled\n                # candidates are not sufficient and we use random sample to meet n_cands\n                cur_valid_tgt_candidate_iris = sampler(\n                    reference_class_mapping, total_num_candidates + num_canddiates\n                )\n                # remove the duplicated candidates (and excluded refs) and prune the tail\n                cur_valid_tgt_candidate_iris = list(\n                    set(cur_valid_tgt_candidate_iris) - set(valid_tgt_candidate_iris)\n                )[:num_canddiates]\n                sample_stats[strategy] += len(cur_valid_tgt_candidate_iris)\n                # use random samples for complementation if not enough\n                while len(cur_valid_tgt_candidate_iris) &lt; num_canddiates:\n                    amend_candidate_iris = self.random_sample(\n                        reference_class_mapping, num_canddiates - len(cur_valid_tgt_candidate_iris)\n                    )\n                    amend_candidate_iris = list(\n                        set(amend_candidate_iris)\n                        - set(valid_tgt_candidate_iris)\n                        - set(cur_valid_tgt_candidate_iris)\n                    )\n                    cur_valid_tgt_candidate_iris += amend_candidate_iris\n                assert len(cur_valid_tgt_candidate_iris) == num_canddiates\n                # record how many random samples to amend\n                if strategy != \"random\":\n                    sample_stats[\"random\"] += num_canddiates - sample_stats[strategy]\n                valid_tgt_candidate_iris += cur_valid_tgt_candidate_iris\n                total_num_candidates += num_canddiates\n            else:\n                raise ValueError(f\"Invalid sampling trategy: {strategy}.\")\n        assert len(valid_tgt_candidate_iris) == total_num_candidates\n        return valid_tgt_candidate_iris, sample_stats\n\n    def random_sample(self, reference_class_mapping: ReferenceMapping, num_candidates: int):\nr\"\"\"**Randomly** sample a set of target class candidates $c'_{cand}$ for a given reference mapping $(c, c')$.\n\n        The sampled candidate classes will be combined with the source reference class $c$ to get a set of\n        candidate mappings $\\{(c, c'_{cand})\\}$.\n\n        Args:\n            reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n            num_candidates (int): The expected number of candidate mappings to generate.\n        \"\"\"\n        ref_src_class_iri, ref_tgt_class_iri = reference_class_mapping.to_tuple()\n        all_tgt_class_iris = set(self.tgt_onto.owl_classes.keys())\n        valid_tgt_class_iris = all_tgt_class_iris - set(\n            self.reference_class_dict[ref_src_class_iri]\n        )  # exclude gold standards\n        assert not ref_tgt_class_iri in valid_tgt_class_iris\n        return random.sample(valid_tgt_class_iris, num_candidates)\n\n    def idf_sample(self, reference_class_mapping: ReferenceMapping, num_candidates: int):\nr\"\"\"Sample a set of target class candidates $c'_{cand}$ for a given reference mapping $(c, c')$ based on the $idf$ scores\n        w.r.t. the inverted annotation index (sub-word level).\n\n        Candidate classes with higher $idf$ scores will be considered first, and then combined with the source reference class $c$\n        to get a set of candidate mappings $\\{(c, c'_{cand})\\}$.\n\n        Args:\n            reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n            num_candidates (int): The expected number of candidate mappings to generate.\n        \"\"\"\n        ref_src_class_iri, ref_tgt_class_iri = reference_class_mapping.to_tuple()\n\n        tgt_candidates = self.tgt_inverted_annotation_index.idf_select(\n            self.tgt_annotation_index[ref_tgt_class_iri]\n        )  # select all non-trivial candidates first\n        valid_tgt_class_iris = []\n        for tgt_candidate_iri, _ in tgt_candidates:\n            # valid as long as it is not one of the reference target\n            if tgt_candidate_iri not in self.reference_class_dict[ref_src_class_iri]:\n                valid_tgt_class_iris.append(tgt_candidate_iri)\n            if len(valid_tgt_class_iris) == num_candidates:\n                break\n        assert not ref_tgt_class_iri in valid_tgt_class_iris\n        return valid_tgt_class_iris\n\n    def neighbour_sample(self, reference_class_mapping: ReferenceMapping, num_candidates: int):\nr\"\"\"Sample a set of target class candidates $c'_{cand}$ for a given reference mapping $(c, c')$ based on the **subsumption\n        hierarchy**.\n\n        Define one-hop as one edge derived from an **asserted** subsumption axiom, i.e., to the parent class or the child class.\n        Candidates classes with nearer hops will be considered first, and then combined with the source reference class $c$\n        to get a set of candidate mappings $\\{(c, c'_{cand})\\}$.\n\n        Args:\n            reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n            num_candidates (int): The expected number of candidate mappings to generate.\n        \"\"\"\n        ref_src_class_iri, ref_tgt_class_iri = reference_class_mapping.to_tuple()\n\n        valid_tgt_class_iris = set()\n        cur_hop = 1\n        frontier = [ref_tgt_class_iri]\n        # extract from the nearest neighbours until enough candidates or max hop\n        while len(valid_tgt_class_iris) &lt; num_candidates and cur_hop &lt;= self.max_hops:\n\n            neighbours_of_cur_hop = []\n            for tgt_class_iri in frontier:\n                tgt_class = self.tgt_onto.get_owl_object_from_iri(tgt_class_iri)\n                parents = self.tgt_onto.reasoner.super_entities_of(tgt_class, direct=True)\n                children = self.tgt_onto.reasoner.sub_entities_of(tgt_class, direct=True)\n                neighbours_of_cur_hop += parents + children  # used for further hop expansion\n\n            valid_neighbours_of_cur_hop = set(neighbours_of_cur_hop) - set(self.reference_class_dict[ref_src_class_iri])\n            # print(valid_neighbours_of_cur_hop)\n\n            # NOTE if by adding neighbours of current hop the require number will be met\n            # we randomly pick among them\n            if len(valid_neighbours_of_cur_hop) &gt; num_candidates - len(valid_tgt_class_iris):\n                valid_neighbours_of_cur_hop = random.sample(\n                    valid_neighbours_of_cur_hop, num_candidates - len(valid_tgt_class_iris)\n                )\n            valid_tgt_class_iris.update(valid_neighbours_of_cur_hop)\n\n            frontier = neighbours_of_cur_hop  # update the frontier with all possible neighbors\n            cur_hop += 1\n\n        assert not ref_tgt_class_iri in valid_tgt_class_iris\n        return list(valid_tgt_class_iris)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.NegativeCandidateMappingGenerator.mixed_sample","title":"<code>mixed_sample(reference_class_mapping, **strategy2nums)</code>","text":"<p>A mixed sampling approach that combines several sampling strategies.</p> <p>As introduced in the Bio-ML paper, this mixed approach guarantees that the number of samples for each strategy is either the maximum that can be sampled or the required number.</p> <p>Specifically, at each sampling iteration, the number of candidates is first increased by the number of  previously sampled candidates, as in the worst case, all the candidates sampled at this iteration will be duplicated with the previous. </p> <p>The random sampling is used as the amending strategy, i.e., if other sampling strategies cannot retrieve the specified number of samples, then use random sampling to amend the number.</p> <p>Parameters:</p> Name Type Description Default <code>reference_class_mapping</code> <code>ReferenceMapping</code> <p>The reference class mapping for generating the candidate mappings.</p> required <code>**strategy2nums</code> <code>int</code> <p>The keyword arguments that specify the expected number of candidates for each sampling strategy.</p> <code>{}</code> Source code in <code>deeponto/align/mapping.py</code> <pre><code>def mixed_sample(self, reference_class_mapping: ReferenceMapping, **strategy2nums):\n\"\"\"A mixed sampling approach that combines several sampling strategies.\n\n    As introduced in the Bio-ML paper, this mixed approach guarantees that the number of samples for each\n    strategy is either the **maximum that can be sampled** or the required number.\n\n    Specifically, at each sampling iteration, the number of candidates is **first increased by the number of \n    previously sampled candidates**, as in the worst case, all the candidates sampled at this iteration\n    will be duplicated with the previous. \n\n    The random sampling is used as the amending strategy, i.e., if other sampling strategies cannot retrieve\n    the specified number of samples, then use random sampling to amend the number.\n\n    Args:\n        reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n        **strategy2nums (int): The keyword arguments that specify the expected number of candidates for each\n            sampling strategy.\n    \"\"\"\n\n    valid_tgt_candidate_iris = []\n    sample_stats = defaultdict(lambda: 0)\n    i = 0\n    total_num_candidates = 0\n    for strategy, num_canddiates in strategy2nums.items():\n        i += 1\n        if strategy in SAMPLING_OPTIONS:\n            sampler = getattr(self, f\"{strategy}_sample\")\n            # for ith iteration, the worst case is when all n_cands are duplicated\n            # or should be excluded from other reference targets so we generate\n            # NOTE:  total_num_candidates + num_candidates + len(excluded_tgt_class_iris)\n            # candidates first and prune the rest; another edge case is when sampled\n            # candidates are not sufficient and we use random sample to meet n_cands\n            cur_valid_tgt_candidate_iris = sampler(\n                reference_class_mapping, total_num_candidates + num_canddiates\n            )\n            # remove the duplicated candidates (and excluded refs) and prune the tail\n            cur_valid_tgt_candidate_iris = list(\n                set(cur_valid_tgt_candidate_iris) - set(valid_tgt_candidate_iris)\n            )[:num_canddiates]\n            sample_stats[strategy] += len(cur_valid_tgt_candidate_iris)\n            # use random samples for complementation if not enough\n            while len(cur_valid_tgt_candidate_iris) &lt; num_canddiates:\n                amend_candidate_iris = self.random_sample(\n                    reference_class_mapping, num_canddiates - len(cur_valid_tgt_candidate_iris)\n                )\n                amend_candidate_iris = list(\n                    set(amend_candidate_iris)\n                    - set(valid_tgt_candidate_iris)\n                    - set(cur_valid_tgt_candidate_iris)\n                )\n                cur_valid_tgt_candidate_iris += amend_candidate_iris\n            assert len(cur_valid_tgt_candidate_iris) == num_canddiates\n            # record how many random samples to amend\n            if strategy != \"random\":\n                sample_stats[\"random\"] += num_canddiates - sample_stats[strategy]\n            valid_tgt_candidate_iris += cur_valid_tgt_candidate_iris\n            total_num_candidates += num_canddiates\n        else:\n            raise ValueError(f\"Invalid sampling trategy: {strategy}.\")\n    assert len(valid_tgt_candidate_iris) == total_num_candidates\n    return valid_tgt_candidate_iris, sample_stats\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.NegativeCandidateMappingGenerator.random_sample","title":"<code>random_sample(reference_class_mapping, num_candidates)</code>","text":"<p>Randomly sample a set of target class candidates \\(c'_{cand}\\) for a given reference mapping \\((c, c')\\).</p> <p>The sampled candidate classes will be combined with the source reference class \\(c\\) to get a set of candidate mappings \\(\\{(c, c'_{cand})\\}\\).</p> <p>Parameters:</p> Name Type Description Default <code>reference_class_mapping</code> <code>ReferenceMapping</code> <p>The reference class mapping for generating the candidate mappings.</p> required <code>num_candidates</code> <code>int</code> <p>The expected number of candidate mappings to generate.</p> required Source code in <code>deeponto/align/mapping.py</code> <pre><code>def random_sample(self, reference_class_mapping: ReferenceMapping, num_candidates: int):\nr\"\"\"**Randomly** sample a set of target class candidates $c'_{cand}$ for a given reference mapping $(c, c')$.\n\n    The sampled candidate classes will be combined with the source reference class $c$ to get a set of\n    candidate mappings $\\{(c, c'_{cand})\\}$.\n\n    Args:\n        reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n        num_candidates (int): The expected number of candidate mappings to generate.\n    \"\"\"\n    ref_src_class_iri, ref_tgt_class_iri = reference_class_mapping.to_tuple()\n    all_tgt_class_iris = set(self.tgt_onto.owl_classes.keys())\n    valid_tgt_class_iris = all_tgt_class_iris - set(\n        self.reference_class_dict[ref_src_class_iri]\n    )  # exclude gold standards\n    assert not ref_tgt_class_iri in valid_tgt_class_iris\n    return random.sample(valid_tgt_class_iris, num_candidates)\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.NegativeCandidateMappingGenerator.idf_sample","title":"<code>idf_sample(reference_class_mapping, num_candidates)</code>","text":"<p>Sample a set of target class candidates \\(c'_{cand}\\) for a given reference mapping \\((c, c')\\) based on the \\(idf\\) scores w.r.t. the inverted annotation index (sub-word level).</p> <p>Candidate classes with higher \\(idf\\) scores will be considered first, and then combined with the source reference class \\(c\\) to get a set of candidate mappings \\(\\{(c, c'_{cand})\\}\\).</p> <p>Parameters:</p> Name Type Description Default <code>reference_class_mapping</code> <code>ReferenceMapping</code> <p>The reference class mapping for generating the candidate mappings.</p> required <code>num_candidates</code> <code>int</code> <p>The expected number of candidate mappings to generate.</p> required Source code in <code>deeponto/align/mapping.py</code> <pre><code>def idf_sample(self, reference_class_mapping: ReferenceMapping, num_candidates: int):\nr\"\"\"Sample a set of target class candidates $c'_{cand}$ for a given reference mapping $(c, c')$ based on the $idf$ scores\n    w.r.t. the inverted annotation index (sub-word level).\n\n    Candidate classes with higher $idf$ scores will be considered first, and then combined with the source reference class $c$\n    to get a set of candidate mappings $\\{(c, c'_{cand})\\}$.\n\n    Args:\n        reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n        num_candidates (int): The expected number of candidate mappings to generate.\n    \"\"\"\n    ref_src_class_iri, ref_tgt_class_iri = reference_class_mapping.to_tuple()\n\n    tgt_candidates = self.tgt_inverted_annotation_index.idf_select(\n        self.tgt_annotation_index[ref_tgt_class_iri]\n    )  # select all non-trivial candidates first\n    valid_tgt_class_iris = []\n    for tgt_candidate_iri, _ in tgt_candidates:\n        # valid as long as it is not one of the reference target\n        if tgt_candidate_iri not in self.reference_class_dict[ref_src_class_iri]:\n            valid_tgt_class_iris.append(tgt_candidate_iri)\n        if len(valid_tgt_class_iris) == num_candidates:\n            break\n    assert not ref_tgt_class_iri in valid_tgt_class_iris\n    return valid_tgt_class_iris\n</code></pre>"},{"location":"deeponto/align/mapping/#deeponto.align.mapping.NegativeCandidateMappingGenerator.neighbour_sample","title":"<code>neighbour_sample(reference_class_mapping, num_candidates)</code>","text":"<p>Sample a set of target class candidates \\(c'_{cand}\\) for a given reference mapping \\((c, c')\\) based on the subsumption hierarchy.</p> <p>Define one-hop as one edge derived from an asserted subsumption axiom, i.e., to the parent class or the child class. Candidates classes with nearer hops will be considered first, and then combined with the source reference class \\(c\\) to get a set of candidate mappings \\(\\{(c, c'_{cand})\\}\\).</p> <p>Parameters:</p> Name Type Description Default <code>reference_class_mapping</code> <code>ReferenceMapping</code> <p>The reference class mapping for generating the candidate mappings.</p> required <code>num_candidates</code> <code>int</code> <p>The expected number of candidate mappings to generate.</p> required Source code in <code>deeponto/align/mapping.py</code> <pre><code>def neighbour_sample(self, reference_class_mapping: ReferenceMapping, num_candidates: int):\nr\"\"\"Sample a set of target class candidates $c'_{cand}$ for a given reference mapping $(c, c')$ based on the **subsumption\n    hierarchy**.\n\n    Define one-hop as one edge derived from an **asserted** subsumption axiom, i.e., to the parent class or the child class.\n    Candidates classes with nearer hops will be considered first, and then combined with the source reference class $c$\n    to get a set of candidate mappings $\\{(c, c'_{cand})\\}$.\n\n    Args:\n        reference_class_mapping (ReferenceMapping): The reference class mapping for generating the candidate mappings.\n        num_candidates (int): The expected number of candidate mappings to generate.\n    \"\"\"\n    ref_src_class_iri, ref_tgt_class_iri = reference_class_mapping.to_tuple()\n\n    valid_tgt_class_iris = set()\n    cur_hop = 1\n    frontier = [ref_tgt_class_iri]\n    # extract from the nearest neighbours until enough candidates or max hop\n    while len(valid_tgt_class_iris) &lt; num_candidates and cur_hop &lt;= self.max_hops:\n\n        neighbours_of_cur_hop = []\n        for tgt_class_iri in frontier:\n            tgt_class = self.tgt_onto.get_owl_object_from_iri(tgt_class_iri)\n            parents = self.tgt_onto.reasoner.super_entities_of(tgt_class, direct=True)\n            children = self.tgt_onto.reasoner.sub_entities_of(tgt_class, direct=True)\n            neighbours_of_cur_hop += parents + children  # used for further hop expansion\n\n        valid_neighbours_of_cur_hop = set(neighbours_of_cur_hop) - set(self.reference_class_dict[ref_src_class_iri])\n        # print(valid_neighbours_of_cur_hop)\n\n        # NOTE if by adding neighbours of current hop the require number will be met\n        # we randomly pick among them\n        if len(valid_neighbours_of_cur_hop) &gt; num_candidates - len(valid_tgt_class_iris):\n            valid_neighbours_of_cur_hop = random.sample(\n                valid_neighbours_of_cur_hop, num_candidates - len(valid_tgt_class_iris)\n            )\n        valid_tgt_class_iris.update(valid_neighbours_of_cur_hop)\n\n        frontier = neighbours_of_cur_hop  # update the frontier with all possible neighbors\n        cur_hop += 1\n\n    assert not ref_tgt_class_iri in valid_tgt_class_iris\n    return list(valid_tgt_class_iris)\n</code></pre>"},{"location":"deeponto/align/bertmap/","title":"BERTMap","text":""},{"location":"deeponto/align/bertmap/#overview","title":"Overview","text":"<p>Paper</p> <p>\\(\\textsf{BERTMap}\\) is proposed in the paper: BERTMap: A BERT-based Ontology Alignment System (AAAI-2022).</p> <p>\\(\\textsf{BERTMap}\\) is a BERT-based ontology matching (OM) system consisting of following components:</p> <ul> <li>Text semantics corpora construction from input ontologies, and optionally from input mappings and other auxiliary ontologies.</li> <li>BERT synonym classifier training on synonym and non-synonym samples in text semantics corpora.</li> <li>Sub-word Inverted Index construction from the tokenised class annotations for candidate selection in mapping prediction.</li> <li>Mapping Predictor which integrates a simple edit distance-based string matching module and the fine-tuned BERT synonym classifier for mapping scoring. For each source ontology class, narrow down target class candidates using the sub-word inverted index, apply string matching for \"easy\" mappings and then apply BERT matching.</li> <li>Mapping Refiner which consists of the mapping extension and mapping repair modules. Mapping extension is an iterative process based on the locality principle. Mapping repair utilises the LogMap's debugger. </li> </ul> <p>\\(\\textsf{BERTMapLt}\\) is a light-weight version of \\(\\textsf{BERTMap}\\) without the BERT module and mapping refiner.</p> <p>See the tutorial for \\(\\textsf{BERTMap}\\) here.</p>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline","title":"<code>BERTMapPipeline</code>","text":"<p>Class for the whole ontology alignment pipeline of \\(\\textsf{BERTMap}\\) and \\(\\textsf{BERTMapLt}\\) models.</p> <p>Note</p> <p>Parameters related to BERT training are <code>None</code> by default. They will be constructed for \\(\\textsf{BERTMap}\\) and stay as <code>None</code> for \\(\\textsf{BERTMapLt}\\).</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>CfgNode</code> <p>The configuration for BERTMap or BERTMapLt.</p> <code>name</code> <code>str</code> <p>The name of the model, either <code>bertmap</code> or <code>bertmaplt</code>.</p> <code>output_path</code> <code>str</code> <p>The path to the output directory.</p> <code>src_onto</code> <code>Ontology</code> <p>The source ontology to be matched.</p> <code>tgt_onto</code> <code>Ontology</code> <p>The target ontology to be matched.</p> <code>annotation_property_iris</code> <code>List[str]</code> <p>The annotation property IRIs used for extracting synonyms and nonsynonyms.</p> <code>src_annotation_index</code> <code>dict</code> <p>A dictionary that stores the <code>(class_iri, class_annotations)</code> pairs from <code>src_onto</code> according to <code>annotation_property_iris</code>.</p> <code>tgt_annotation_index</code> <code>dict</code> <p>A dictionary that stores the <code>(class_iri, class_annotations)</code> pairs from <code>tgt_onto</code> according to <code>annotation_property_iris</code>.</p> <code>known_mappings</code> <code>List[ReferenceMapping]</code> <p>List of known mappings for constructing the cross-ontology corpus.</p> <code>auxliary_ontos</code> <code>List[Ontology]</code> <p>List of auxiliary ontolgoies for constructing any auxiliary corpus.</p> <code>corpora</code> <code>dict</code> <p>A dictionary that stores the <code>summary</code> of built text semantics corpora and the sampled <code>synonyms</code> and <code>nonsynonyms</code>.</p> <code>finetune_data</code> <code>dict</code> <p>A dictionary that stores the <code>training</code> and <code>validation</code> splits of samples from <code>corpora</code>.</p> <code>bert</code> <code>BERTSynonymClassifier</code> <p>A BERT model for synonym classification and mapping prediction.</p> <code>best_checkpoint</code> <code>str</code> <p>The path to the best BERT checkpoint which will be loaded after training.</p> <code>mapping_predictor</code> <code>MappingPredictor</code> <p>The predictor function based on class annotations, used for global matching or mapping scoring.</p> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>class BERTMapPipeline:\nr\"\"\"Class for the whole ontology alignment pipeline of $\\textsf{BERTMap}$ and $\\textsf{BERTMapLt}$ models.\n\n    !!! note\n\n        Parameters related to BERT training are `None` by default. They will be constructed for\n        $\\textsf{BERTMap}$ and stay as `None` for $\\textsf{BERTMapLt}$.\n\n    Attributes:\n        config (CfgNode): The configuration for BERTMap or BERTMapLt.\n        name (str): The name of the model, either `bertmap` or `bertmaplt`.\n        output_path (str): The path to the output directory.\n        src_onto (Ontology): The source ontology to be matched.\n        tgt_onto (Ontology): The target ontology to be matched.\n        annotation_property_iris (List[str]): The annotation property IRIs used for extracting synonyms and nonsynonyms.\n        src_annotation_index (dict): A dictionary that stores the `(class_iri, class_annotations)` pairs from `src_onto` according to `annotation_property_iris`.\n        tgt_annotation_index (dict): A dictionary that stores the `(class_iri, class_annotations)` pairs from `tgt_onto` according to `annotation_property_iris`.\n        known_mappings (List[ReferenceMapping], optional): List of known mappings for constructing the **cross-ontology corpus**.\n        auxliary_ontos (List[Ontology], optional): List of auxiliary ontolgoies for constructing any **auxiliary corpus**.\n        corpora (dict, optional): A dictionary that stores the `summary` of built text semantics corpora and the sampled `synonyms` and `nonsynonyms`.\n        finetune_data (dict, optional): A dictionary that stores the `training` and `validation` splits of samples from `corpora`.\n        bert (BERTSynonymClassifier, optional): A BERT model for synonym classification and mapping prediction.\n        best_checkpoint (str, optional): The path to the best BERT checkpoint which will be loaded after training.\n        mapping_predictor (MappingPredictor): The predictor function based on class annotations, used for **global matching** or **mapping scoring**.\n\n    \"\"\"\n\n    def __init__(self, src_onto: Ontology, tgt_onto: Ontology, config: CfgNode):\n\"\"\"Initialize the BERTMap or BERTMapLt model.\n\n        Args:\n            src_onto (Ontology): The source ontology for alignment.\n            tgt_onto (Ontology): The target ontology for alignment.\n            config (CfgNode): The configuration for BERTMap or BERTMapLt.\n        \"\"\"\n        # load the configuration and confirm model name is valid\n        self.config = config\n        self.name = self.config.model\n        if not self.name in MODEL_OPTIONS.keys():\n            raise RuntimeError(f\"`model` {self.name} in the config file is not one of the supported.\")\n\n        # create the output directory, e.g., experiments/bertmap\n        self.config.output_path = \".\" if not self.config.output_path else self.config.output_path\n        self.config.output_path = os.path.abspath(self.config.output_path)\n        self.output_path = os.path.join(self.config.output_path, self.name)\n        FileUtils.create_path(self.output_path)\n\n        # create logger and progress manager (hidden attribute) \n        self.logger = create_logger(self.name, self.output_path)\n        self.enlighten_manager = enlighten.get_manager()\n\n        # ontology\n        self.src_onto = src_onto\n        self.tgt_onto = tgt_onto\n        self.annotation_property_iris = self.config.annotation_property_iris\n        self.logger.info(f\"Load the following configurations:\\n{FileUtils.print_dict(self.config)}\")\n        config_path = os.path.join(self.output_path, \"config.yaml\")\n        self.logger.info(f\"Save the configuration file at {config_path}.\")\n        self.save_bertmap_config(self.config, config_path)\n\n        # build the annotation thesaurus\n        self.src_annotation_index, _ = self.src_onto.build_annotation_index(self.annotation_property_iris)\n        self.tgt_annotation_index, _ = self.tgt_onto.build_annotation_index(self.annotation_property_iris)\n\n        # provided mappings if any\n        self.known_mappings = self.config.known_mappings\n        if self.known_mappings:\n            self.known_mappings = ReferenceMapping.read_table_mappings(self.known_mappings)\n\n        # auxiliary ontologies if any\n        self.auxiliary_ontos = self.config.auxiliary_ontos\n        if self.auxiliary_ontos:\n            self.auxiliary_ontos = [Ontology(ao) for ao in self.auxiliary_ontos]\n\n        self.data_path = os.path.join(self.output_path, \"data\")\n        # load or construct the corpora\n        self.corpora_path = os.path.join(self.data_path, \"text-semantics.corpora.json\")\n        self.corpora = self.load_text_semantics_corpora()\n\n        # load or construct fine-tune data\n        self.finetune_data_path = os.path.join(self.data_path, \"fine-tune.data.json\")\n        self.finetune_data = self.load_finetune_data()\n\n        # load the bert model and train\n        self.bert_config = self.config.bert\n        self.bert_pretrained_path = self.bert_config.pretrained_path\n        self.bert_finetuned_path = os.path.join(self.output_path, \"bert\")\n        self.bert_resume_training = self.bert_config.resume_training\n        self.bert_synonym_classifier = None\n        self.best_checkpoint = None\n        if self.name == \"bertmap\":\n            self.bert_synonym_classifier = self.load_bert_synonym_classifier()\n            # train if the loaded classifier is not in eval mode\n            if self.bert_synonym_classifier.eval_mode == False:\n                self.logger.info(\n                    f\"Data statistics:\\n \\\n{FileUtils.print_dict(self.bert_synonym_classifier.data_stat)}\"\n                )\n                self.bert_synonym_classifier.train(self.bert_resume_training)\n                # turn on eval mode after training\n                self.bert_synonym_classifier.eval()\n            # NOTE potential redundancy here: after training, load the best checkpoint\n            self.best_checkpoint = self.load_best_checkpoint()\n            if not self.best_checkpoint:\n                raise RuntimeError(f\"No best checkpoint found for the BERT synonym classifier model.\")\n            self.logger.info(f\"Fine-tuning finished, found best checkpoint at {self.best_checkpoint}.\")\n        else:\n            self.logger.info(f\"No training needed; skip BERT fine-tuning.\")\n\n        # pretty progress bar tracking\n        self.enlighten_status = self.enlighten_manager.status_bar(\n            status_format=u'Global Matching{fill}Stage: {demo}{fill}{elapsed}',\n            color='bold_underline_bright_white_on_lightslategray',\n            justify=enlighten.Justify.CENTER, demo='Initializing',\n            autorefresh=True, min_delta=0.5\n        )\n\n        # mapping predictions\n        self.global_matching_config = self.config.global_matching\n        self.mapping_predictor = MappingPredictor(\n            output_path=self.output_path,\n            tokenizer_path=self.bert_config.pretrained_path,\n            src_annotation_index=self.src_annotation_index,\n            tgt_annotation_index=self.tgt_annotation_index,\n            bert_synonym_classifier=self.bert_synonym_classifier,\n            num_raw_candidates=self.global_matching_config.num_raw_candidates,\n            num_best_predictions=self.global_matching_config.num_best_predictions,\n            batch_size_for_prediction=self.bert_config.batch_size_for_prediction,\n            logger=self.logger,\n            enlighten_manager=self.enlighten_manager,\n            enlighten_status=self.enlighten_status\n        )\n        self.mapping_refiner = None\n\n        # if global matching is disabled (potentially used for class pair scoring)\n        if self.config.global_matching.enabled:\n            self.mapping_predictor.mapping_prediction()  # mapping prediction\n            if self.name == \"bertmap\":\n                self.mapping_refiner = MappingRefiner(\n                    output_path=self.output_path,\n                    src_onto=self.src_onto,\n                    tgt_onto=self.tgt_onto,\n                    mapping_predictor=self.mapping_predictor,\n                    mapping_extension_threshold=self.global_matching_config.mapping_extension_threshold,\n                    mapping_filtered_threshold=self.global_matching_config.mapping_filtered_threshold,\n                    logger=self.logger,\n                    enlighten_manager=self.enlighten_manager,\n                    enlighten_status=self.enlighten_status\n                )\n                self.mapping_refiner.mapping_extension()  # mapping extension\n                self.mapping_refiner.mapping_repair()  # mapping repair\n            self.enlighten_status.update(demo=\"Finished\")  \n        else:\n            self.enlighten_status.update(demo=\"Skipped\")  \n\n        self.enlighten_status.close()\n\n        # class pair scoring is invoked outside\n\n    def load_or_construct(self, data_file: str, data_name: str, construct_func: Callable, *args, **kwargs):\n\"\"\"Load existing data or construct a new one.\n\n        An auxlirary function that checks the existence of a data file and loads it if it exists.\n        Otherwise, construct new data with the input `construct_func` which is supported generate\n        a local data file.\n        \"\"\"\n        if os.path.exists(data_file):\n            self.logger.info(f\"Load existing {data_name} from {data_file}.\")\n        else:\n            self.logger.info(f\"Construct new {data_name} and save at {data_file}.\")\n            construct_func(*args, **kwargs)\n        # load the data file that is supposed to be saved locally\n        return FileUtils.load_file(data_file)\n\n    def load_text_semantics_corpora(self):\n\"\"\"Load or construct text semantics corpora.\n\n        See [`TextSemanticsCorpora`][deeponto.align.bertmap.text_semantics.TextSemanticsCorpora].\n        \"\"\"\n        data_name = \"text semantics corpora\"\n\n        if self.name == \"bertmap\":\n\n            def construct():\n                corpora = TextSemanticsCorpora(\n                    src_onto=self.src_onto,\n                    tgt_onto=self.tgt_onto,\n                    annotation_property_iris=self.annotation_property_iris,\n                    class_mappings=self.known_mappings,\n                    auxiliary_ontos=self.auxiliary_ontos,\n                )\n                self.logger.info(str(corpora))\n                corpora.save(self.data_path)\n\n            return self.load_or_construct(self.corpora_path, data_name, construct)\n\n        self.logger.info(f\"No training needed; skip the construction of {data_name}.\")\n        return None\n\n    def load_finetune_data(self):\nr\"\"\"Load or construct fine-tuning data from text semantics corpora.\n\n        Steps of constructing fine-tuning data from text semantics:\n\n        1. Mix synonym and nonsynonym data.\n        2. Randomly sample 90% as training samples and 10% as validation.\n        \"\"\"\n        data_name = \"fine-tuning data\"\n\n        if self.name == \"bertmap\":\n\n            def construct():\n                finetune_data = dict()\n                samples = self.corpora[\"synonyms\"] + self.corpora[\"nonsynonyms\"]\n                random.shuffle(samples)\n                split_index = int(0.9 * len(samples))  # split at 90%\n                finetune_data[\"training\"] = samples[:split_index]\n                finetune_data[\"validation\"] = samples[split_index:]\n                FileUtils.save_file(finetune_data, self.finetune_data_path)\n\n            return self.load_or_construct(self.finetune_data_path, data_name, construct)\n\n        self.logger.info(f\"No training needed; skip the construction of {data_name}.\")\n        return None\n\n    def load_bert_synonym_classifier(self):\n\"\"\"Load the BERT model from a pre-trained or a local checkpoint.\n\n        - If loaded from pre-trained, it means to start training from a pre-trained model such as `bert-uncased`.\n        - If loaded from local, turn on the `eval` mode for mapping predictions.\n        - If `self.bert_resume_training` is `True`, it will be loaded from the latest saved checkpoint.\n        \"\"\"\n        checkpoint = self.load_best_checkpoint()  # load the best checkpoint or nothing\n        eval_mode = True\n        # if no checkpoint has been found, start training from scratch OR resume training\n        # no point to load the best checkpoint if resume training (will automatically search for the latest checkpoint)\n        if not checkpoint or self.bert_resume_training:\n            checkpoint = self.bert_pretrained_path\n            eval_mode = False  # since it is for training now\n\n        return BERTSynonymClassifier(\n            loaded_path=checkpoint,\n            output_path=self.bert_finetuned_path,\n            eval_mode=eval_mode,\n            max_length_for_input=self.bert_config.max_length_for_input,\n            num_epochs_for_training=self.bert_config.num_epochs_for_training,\n            batch_size_for_training=self.bert_config.batch_size_for_training,\n            batch_size_for_prediction=self.bert_config.batch_size_for_prediction,\n            training_data=self.finetune_data[\"training\"],\n            validation_data=self.finetune_data[\"validation\"],\n        )\n\n    def load_best_checkpoint(self) -&gt; Optional[str]:\n\"\"\"Find the best checkpoint by searching for trainer states in each checkpoint file.\"\"\"\n        best_checkpoint = -1\n\n        if os.path.exists(self.bert_finetuned_path):\n            for file in os.listdir(self.bert_finetuned_path):\n                # load trainer states from each checkpoint file\n                if file.startswith(\"checkpoint\"):\n                    trainer_state = FileUtils.load_file(\n                        os.path.join(self.bert_finetuned_path, file, \"trainer_state.json\")\n                    )\n                    checkpoint = int(trainer_state[\"best_model_checkpoint\"].split(\"/\")[-1].split(\"-\")[-1])\n                    # find the latest best checkpoint\n                    if checkpoint &gt; best_checkpoint:\n                        best_checkpoint = checkpoint\n\n        if best_checkpoint == -1:\n            best_checkpoint = None\n        else:\n            best_checkpoint = os.path.join(self.bert_finetuned_path, f\"checkpoint-{best_checkpoint}\")\n\n        return best_checkpoint\n\n    @staticmethod\n    def load_bertmap_config(config_file: Optional[str] = None):\n\"\"\"Load the BERTMap configuration in `.yaml`. If the file\n        is not provided, use the default configuration.\n        \"\"\"\n        if not config_file:\n            config_file = DEFAULT_CONFIG_FILE\n            print(f\"Use the default configuration at {DEFAULT_CONFIG_FILE}.\")  \n        if not config_file.endswith(\".yaml\"):\n            raise RuntimeError(\"Configuration file should be in `yaml` format.\")\n        return CfgNode(FileUtils.load_file(config_file))\n\n    @staticmethod\n    def save_bertmap_config(config: CfgNode, config_file: str):\n\"\"\"Save the BERTMap configuration in `.yaml`.\"\"\"\n        with open(config_file, \"w\") as c:\n            config.dump(stream=c, sort_keys=False, default_flow_style=False)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.__init__","title":"<code>__init__(src_onto, tgt_onto, config)</code>","text":"<p>Initialize the BERTMap or BERTMapLt model.</p> <p>Parameters:</p> Name Type Description Default <code>src_onto</code> <code>Ontology</code> <p>The source ontology for alignment.</p> required <code>tgt_onto</code> <code>Ontology</code> <p>The target ontology for alignment.</p> required <code>config</code> <code>CfgNode</code> <p>The configuration for BERTMap or BERTMapLt.</p> required Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>def __init__(self, src_onto: Ontology, tgt_onto: Ontology, config: CfgNode):\n\"\"\"Initialize the BERTMap or BERTMapLt model.\n\n    Args:\n        src_onto (Ontology): The source ontology for alignment.\n        tgt_onto (Ontology): The target ontology for alignment.\n        config (CfgNode): The configuration for BERTMap or BERTMapLt.\n    \"\"\"\n    # load the configuration and confirm model name is valid\n    self.config = config\n    self.name = self.config.model\n    if not self.name in MODEL_OPTIONS.keys():\n        raise RuntimeError(f\"`model` {self.name} in the config file is not one of the supported.\")\n\n    # create the output directory, e.g., experiments/bertmap\n    self.config.output_path = \".\" if not self.config.output_path else self.config.output_path\n    self.config.output_path = os.path.abspath(self.config.output_path)\n    self.output_path = os.path.join(self.config.output_path, self.name)\n    FileUtils.create_path(self.output_path)\n\n    # create logger and progress manager (hidden attribute) \n    self.logger = create_logger(self.name, self.output_path)\n    self.enlighten_manager = enlighten.get_manager()\n\n    # ontology\n    self.src_onto = src_onto\n    self.tgt_onto = tgt_onto\n    self.annotation_property_iris = self.config.annotation_property_iris\n    self.logger.info(f\"Load the following configurations:\\n{FileUtils.print_dict(self.config)}\")\n    config_path = os.path.join(self.output_path, \"config.yaml\")\n    self.logger.info(f\"Save the configuration file at {config_path}.\")\n    self.save_bertmap_config(self.config, config_path)\n\n    # build the annotation thesaurus\n    self.src_annotation_index, _ = self.src_onto.build_annotation_index(self.annotation_property_iris)\n    self.tgt_annotation_index, _ = self.tgt_onto.build_annotation_index(self.annotation_property_iris)\n\n    # provided mappings if any\n    self.known_mappings = self.config.known_mappings\n    if self.known_mappings:\n        self.known_mappings = ReferenceMapping.read_table_mappings(self.known_mappings)\n\n    # auxiliary ontologies if any\n    self.auxiliary_ontos = self.config.auxiliary_ontos\n    if self.auxiliary_ontos:\n        self.auxiliary_ontos = [Ontology(ao) for ao in self.auxiliary_ontos]\n\n    self.data_path = os.path.join(self.output_path, \"data\")\n    # load or construct the corpora\n    self.corpora_path = os.path.join(self.data_path, \"text-semantics.corpora.json\")\n    self.corpora = self.load_text_semantics_corpora()\n\n    # load or construct fine-tune data\n    self.finetune_data_path = os.path.join(self.data_path, \"fine-tune.data.json\")\n    self.finetune_data = self.load_finetune_data()\n\n    # load the bert model and train\n    self.bert_config = self.config.bert\n    self.bert_pretrained_path = self.bert_config.pretrained_path\n    self.bert_finetuned_path = os.path.join(self.output_path, \"bert\")\n    self.bert_resume_training = self.bert_config.resume_training\n    self.bert_synonym_classifier = None\n    self.best_checkpoint = None\n    if self.name == \"bertmap\":\n        self.bert_synonym_classifier = self.load_bert_synonym_classifier()\n        # train if the loaded classifier is not in eval mode\n        if self.bert_synonym_classifier.eval_mode == False:\n            self.logger.info(\n                f\"Data statistics:\\n \\\n{FileUtils.print_dict(self.bert_synonym_classifier.data_stat)}\"\n            )\n            self.bert_synonym_classifier.train(self.bert_resume_training)\n            # turn on eval mode after training\n            self.bert_synonym_classifier.eval()\n        # NOTE potential redundancy here: after training, load the best checkpoint\n        self.best_checkpoint = self.load_best_checkpoint()\n        if not self.best_checkpoint:\n            raise RuntimeError(f\"No best checkpoint found for the BERT synonym classifier model.\")\n        self.logger.info(f\"Fine-tuning finished, found best checkpoint at {self.best_checkpoint}.\")\n    else:\n        self.logger.info(f\"No training needed; skip BERT fine-tuning.\")\n\n    # pretty progress bar tracking\n    self.enlighten_status = self.enlighten_manager.status_bar(\n        status_format=u'Global Matching{fill}Stage: {demo}{fill}{elapsed}',\n        color='bold_underline_bright_white_on_lightslategray',\n        justify=enlighten.Justify.CENTER, demo='Initializing',\n        autorefresh=True, min_delta=0.5\n    )\n\n    # mapping predictions\n    self.global_matching_config = self.config.global_matching\n    self.mapping_predictor = MappingPredictor(\n        output_path=self.output_path,\n        tokenizer_path=self.bert_config.pretrained_path,\n        src_annotation_index=self.src_annotation_index,\n        tgt_annotation_index=self.tgt_annotation_index,\n        bert_synonym_classifier=self.bert_synonym_classifier,\n        num_raw_candidates=self.global_matching_config.num_raw_candidates,\n        num_best_predictions=self.global_matching_config.num_best_predictions,\n        batch_size_for_prediction=self.bert_config.batch_size_for_prediction,\n        logger=self.logger,\n        enlighten_manager=self.enlighten_manager,\n        enlighten_status=self.enlighten_status\n    )\n    self.mapping_refiner = None\n\n    # if global matching is disabled (potentially used for class pair scoring)\n    if self.config.global_matching.enabled:\n        self.mapping_predictor.mapping_prediction()  # mapping prediction\n        if self.name == \"bertmap\":\n            self.mapping_refiner = MappingRefiner(\n                output_path=self.output_path,\n                src_onto=self.src_onto,\n                tgt_onto=self.tgt_onto,\n                mapping_predictor=self.mapping_predictor,\n                mapping_extension_threshold=self.global_matching_config.mapping_extension_threshold,\n                mapping_filtered_threshold=self.global_matching_config.mapping_filtered_threshold,\n                logger=self.logger,\n                enlighten_manager=self.enlighten_manager,\n                enlighten_status=self.enlighten_status\n            )\n            self.mapping_refiner.mapping_extension()  # mapping extension\n            self.mapping_refiner.mapping_repair()  # mapping repair\n        self.enlighten_status.update(demo=\"Finished\")  \n    else:\n        self.enlighten_status.update(demo=\"Skipped\")  \n\n    self.enlighten_status.close()\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.load_or_construct","title":"<code>load_or_construct(data_file, data_name, construct_func, *args, **kwargs)</code>","text":"<p>Load existing data or construct a new one.</p> <p>An auxlirary function that checks the existence of a data file and loads it if it exists. Otherwise, construct new data with the input <code>construct_func</code> which is supported generate a local data file.</p> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>def load_or_construct(self, data_file: str, data_name: str, construct_func: Callable, *args, **kwargs):\n\"\"\"Load existing data or construct a new one.\n\n    An auxlirary function that checks the existence of a data file and loads it if it exists.\n    Otherwise, construct new data with the input `construct_func` which is supported generate\n    a local data file.\n    \"\"\"\n    if os.path.exists(data_file):\n        self.logger.info(f\"Load existing {data_name} from {data_file}.\")\n    else:\n        self.logger.info(f\"Construct new {data_name} and save at {data_file}.\")\n        construct_func(*args, **kwargs)\n    # load the data file that is supposed to be saved locally\n    return FileUtils.load_file(data_file)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.load_text_semantics_corpora","title":"<code>load_text_semantics_corpora()</code>","text":"<p>Load or construct text semantics corpora.</p> <p>See <code>TextSemanticsCorpora</code>.</p> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>def load_text_semantics_corpora(self):\n\"\"\"Load or construct text semantics corpora.\n\n    See [`TextSemanticsCorpora`][deeponto.align.bertmap.text_semantics.TextSemanticsCorpora].\n    \"\"\"\n    data_name = \"text semantics corpora\"\n\n    if self.name == \"bertmap\":\n\n        def construct():\n            corpora = TextSemanticsCorpora(\n                src_onto=self.src_onto,\n                tgt_onto=self.tgt_onto,\n                annotation_property_iris=self.annotation_property_iris,\n                class_mappings=self.known_mappings,\n                auxiliary_ontos=self.auxiliary_ontos,\n            )\n            self.logger.info(str(corpora))\n            corpora.save(self.data_path)\n\n        return self.load_or_construct(self.corpora_path, data_name, construct)\n\n    self.logger.info(f\"No training needed; skip the construction of {data_name}.\")\n    return None\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.load_finetune_data","title":"<code>load_finetune_data()</code>","text":"<p>Load or construct fine-tuning data from text semantics corpora.</p> <p>Steps of constructing fine-tuning data from text semantics:</p> <ol> <li>Mix synonym and nonsynonym data.</li> <li>Randomly sample 90% as training samples and 10% as validation.</li> </ol> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>def load_finetune_data(self):\nr\"\"\"Load or construct fine-tuning data from text semantics corpora.\n\n    Steps of constructing fine-tuning data from text semantics:\n\n    1. Mix synonym and nonsynonym data.\n    2. Randomly sample 90% as training samples and 10% as validation.\n    \"\"\"\n    data_name = \"fine-tuning data\"\n\n    if self.name == \"bertmap\":\n\n        def construct():\n            finetune_data = dict()\n            samples = self.corpora[\"synonyms\"] + self.corpora[\"nonsynonyms\"]\n            random.shuffle(samples)\n            split_index = int(0.9 * len(samples))  # split at 90%\n            finetune_data[\"training\"] = samples[:split_index]\n            finetune_data[\"validation\"] = samples[split_index:]\n            FileUtils.save_file(finetune_data, self.finetune_data_path)\n\n        return self.load_or_construct(self.finetune_data_path, data_name, construct)\n\n    self.logger.info(f\"No training needed; skip the construction of {data_name}.\")\n    return None\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.load_bert_synonym_classifier","title":"<code>load_bert_synonym_classifier()</code>","text":"<p>Load the BERT model from a pre-trained or a local checkpoint.</p> <ul> <li>If loaded from pre-trained, it means to start training from a pre-trained model such as <code>bert-uncased</code>.</li> <li>If loaded from local, turn on the <code>eval</code> mode for mapping predictions.</li> <li>If <code>self.bert_resume_training</code> is <code>True</code>, it will be loaded from the latest saved checkpoint.</li> </ul> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>def load_bert_synonym_classifier(self):\n\"\"\"Load the BERT model from a pre-trained or a local checkpoint.\n\n    - If loaded from pre-trained, it means to start training from a pre-trained model such as `bert-uncased`.\n    - If loaded from local, turn on the `eval` mode for mapping predictions.\n    - If `self.bert_resume_training` is `True`, it will be loaded from the latest saved checkpoint.\n    \"\"\"\n    checkpoint = self.load_best_checkpoint()  # load the best checkpoint or nothing\n    eval_mode = True\n    # if no checkpoint has been found, start training from scratch OR resume training\n    # no point to load the best checkpoint if resume training (will automatically search for the latest checkpoint)\n    if not checkpoint or self.bert_resume_training:\n        checkpoint = self.bert_pretrained_path\n        eval_mode = False  # since it is for training now\n\n    return BERTSynonymClassifier(\n        loaded_path=checkpoint,\n        output_path=self.bert_finetuned_path,\n        eval_mode=eval_mode,\n        max_length_for_input=self.bert_config.max_length_for_input,\n        num_epochs_for_training=self.bert_config.num_epochs_for_training,\n        batch_size_for_training=self.bert_config.batch_size_for_training,\n        batch_size_for_prediction=self.bert_config.batch_size_for_prediction,\n        training_data=self.finetune_data[\"training\"],\n        validation_data=self.finetune_data[\"validation\"],\n    )\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.load_best_checkpoint","title":"<code>load_best_checkpoint()</code>","text":"<p>Find the best checkpoint by searching for trainer states in each checkpoint file.</p> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>def load_best_checkpoint(self) -&gt; Optional[str]:\n\"\"\"Find the best checkpoint by searching for trainer states in each checkpoint file.\"\"\"\n    best_checkpoint = -1\n\n    if os.path.exists(self.bert_finetuned_path):\n        for file in os.listdir(self.bert_finetuned_path):\n            # load trainer states from each checkpoint file\n            if file.startswith(\"checkpoint\"):\n                trainer_state = FileUtils.load_file(\n                    os.path.join(self.bert_finetuned_path, file, \"trainer_state.json\")\n                )\n                checkpoint = int(trainer_state[\"best_model_checkpoint\"].split(\"/\")[-1].split(\"-\")[-1])\n                # find the latest best checkpoint\n                if checkpoint &gt; best_checkpoint:\n                    best_checkpoint = checkpoint\n\n    if best_checkpoint == -1:\n        best_checkpoint = None\n    else:\n        best_checkpoint = os.path.join(self.bert_finetuned_path, f\"checkpoint-{best_checkpoint}\")\n\n    return best_checkpoint\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.load_bertmap_config","title":"<code>load_bertmap_config(config_file=None)</code>  <code>staticmethod</code>","text":"<p>Load the BERTMap configuration in <code>.yaml</code>. If the file is not provided, use the default configuration.</p> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>@staticmethod\ndef load_bertmap_config(config_file: Optional[str] = None):\n\"\"\"Load the BERTMap configuration in `.yaml`. If the file\n    is not provided, use the default configuration.\n    \"\"\"\n    if not config_file:\n        config_file = DEFAULT_CONFIG_FILE\n        print(f\"Use the default configuration at {DEFAULT_CONFIG_FILE}.\")  \n    if not config_file.endswith(\".yaml\"):\n        raise RuntimeError(\"Configuration file should be in `yaml` format.\")\n    return CfgNode(FileUtils.load_file(config_file))\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.pipeline.BERTMapPipeline.save_bertmap_config","title":"<code>save_bertmap_config(config, config_file)</code>  <code>staticmethod</code>","text":"<p>Save the BERTMap configuration in <code>.yaml</code>.</p> Source code in <code>deeponto/align/bertmap/pipeline.py</code> <pre><code>@staticmethod\ndef save_bertmap_config(config: CfgNode, config_file: str):\n\"\"\"Save the BERTMap configuration in `.yaml`.\"\"\"\n    with open(config_file, \"w\") as c:\n        config.dump(stream=c, sort_keys=False, default_flow_style=False)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus","title":"<code>AnnotationThesaurus</code>","text":"<p>A thesaurus class for synonyms and non-synonyms extracted from an ontology.</p> <p>Some related definitions of arguements here:</p> <ul> <li>A <code>synonym_group</code> is a set of annotation phrases that are synonymous to each other;</li> <li>The <code>transitivity</code> of synonyms means if A and B are synonymous and B and C are synonymous, then A and C are synonymous. This is achieved by a connected graph-based algorithm.</li> <li>A <code>synonym_pair</code> is a pair synonymous annotation phrase which can be extracted from the cartesian product of a <code>synonym_group</code> and itself. NOTE that reflexivity and symmetry are preserved meaning that (i) every phrase A is a synonym of itself and (ii) if (A, B) is a synonym pair then (B, A) is a synonym pair, too.</li> </ul> <p>Attributes:</p> Name Type Description <code>onto</code> <code>Ontology</code> <p>An ontology to construct the annotation thesaurus from.</p> <code>annotation_index</code> <code>Dict[str, Set[str]]</code> <p>An index of the class annotations with <code>(class_iri, annotations)</code> pairs.</p> <code>annotation_property_iris</code> <code>List[str]</code> <p>A list of annotation property IRIs used to extract the annotations.</p> <code>average_number_of_annotations_per_class</code> <code>int</code> <p>The average number of (extracted) annotations per ontology class.</p> <code>apply_transitivity</code> <code>bool</code> <p>Apply synonym transitivity to merge synonym groups or not.</p> <code>synonym_groups</code> <code>List[Set[str]]</code> <p>The list of synonym groups extracted from the ontology according to specified annotation properties.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>class AnnotationThesaurus:\n\"\"\"A thesaurus class for synonyms and non-synonyms extracted from an ontology.\n\n    Some related definitions of arguements here:\n\n    - A `synonym_group` is a set of annotation phrases that are synonymous to each other;\n    - The `transitivity` of synonyms means if A and B are synonymous and B and C are synonymous,\n    then A and C are synonymous. This is achieved by a connected graph-based algorithm.\n    - A `synonym_pair` is a pair synonymous annotation phrase which can be extracted from\n    the cartesian product of a `synonym_group` and itself. NOTE that **reflexivity** and **symmetry**\n    are preserved meaning that *(i)* every phrase A is a synonym of itself and *(ii)* if (A, B) is\n    a synonym pair then (B, A) is a synonym pair, too.\n\n    Attributes:\n        onto (Ontology): An ontology to construct the annotation thesaurus from.\n        annotation_index (Dict[str, Set[str]]): An index of the class annotations with `(class_iri, annotations)` pairs.\n        annotation_property_iris (List[str]): A list of annotation property IRIs used to extract the annotations.\n        average_number_of_annotations_per_class (int): The average number of (extracted) annotations per ontology class.\n        apply_transitivity (bool): Apply synonym transitivity to merge synonym groups or not.\n        synonym_groups (List[Set[str]]): The list of synonym groups extracted from the ontology according to specified annotation properties.\n    \"\"\"\n\n    def __init__(self, onto: Ontology, annotation_property_iris: List[str], apply_transitivity: bool = False):\nr\"\"\"Initialise a thesaurus for ontology class annotations.\n\n        Args:\n            onto (Ontology): The input ontology to extract annotations from.\n            annotation_property_iris (List[str]): Specify which annotation properties to be used.\n            apply_transitivity (bool, optional): Apply synonym transitivity to merge synonym groups or not. Defaults to `False`.\n        \"\"\"\n\n        self.onto = onto\n        # build the annotation index to extract synonyms from `onto`\n        # the input property iris may not exist in this ontology\n        # the output property iris will be truncated to the existing ones\n        index, iris = self.onto.build_annotation_index(\n            annotation_property_iris=annotation_property_iris,\n            entity_type=\"Classes\",\n            apply_lowercasing=True,\n        )\n        self.annotation_index = index\n        self.annotation_property_iris = iris\n        total_number_of_annotations = sum([len(v) for v in self.annotation_index.values()])\n        self.average_number_of_annotations_per_class = total_number_of_annotations / len(self.annotation_index)\n\n        # synonym groups\n        self.apply_transitivity = apply_transitivity\n        self.synonym_groups = list(self.annotation_index.values())\n        if self.apply_transitivity:\n            self.synonym_groups = self.merge_synonym_groups_by_transitivity(self.synonym_groups)\n\n        # summary\n        self.info = {\n            type(self).__name__: {\n                \"ontology\": self.onto.info[type(self.onto).__name__],\n                \"average_number_of_annotations_per_class\": round(self.average_number_of_annotations_per_class, 3),\n                \"number_of_synonym_groups\": len(self.synonym_groups),\n            }\n        }\n\n    def __str__(self):\n        str(self.onto)  # the info of ontology is updated upon calling its __str__ method\n        return FileUtils.print_dict(self.info)\n\n    @staticmethod\n    def get_synonym_pairs(synonym_group: Set[str], remove_duplicates: bool = True):\n\"\"\"Get synonym pairs from a synonym group through a cartesian product.\n\n        Args:\n            synonym_group (Set[str]): A set of annotation phrases that are synonymous to each other.\n\n        Returns:\n            (List[Tuple[str, str]]): A list of synonym pairs.\n        \"\"\"\n        synonym_pairs = list(itertools.product(synonym_group, synonym_group))\n        if remove_duplicates:\n            return DataUtils.uniqify(synonym_pairs)\n        else:\n            return synonym_pairs\n\n    @staticmethod\n    def merge_synonym_groups_by_transitivity(synonym_groups: List[Set[str]]):\nr\"\"\"Merge synonym groups by transitivity.\n\n        Synonym groups that share a common annotation phrase will be merged. NOTE that for\n        multiple ontologies, we can merge their synonym groups by first concatenating them\n        then use this function.\n\n        !!! note\n\n            In $\\textsf{BERTMap}$ experiments we have considered this as a data augmentation approach\n            but it does not bring a significant performance improvement. However, if the\n            overall number of annotations is not large enough then this could be a good option.\n\n        Args:\n            *synonym_groups (List[Set[str]]): A sequence of synonym groups to be merged.\n\n        Returns:\n            (List[Set[str]]): A list of merged synonym groups.\n        \"\"\"\n        synonym_pairs = []\n        for synonym_group in synonym_groups:\n            # gather synonym pairs from the self-product of a synonym group\n            synonym_pairs += AnnotationThesaurus.get_synonym_pairs(synonym_group, remove_duplicates=False)\n        synonym_pairs = DataUtils.uniqify(synonym_pairs)\n        merged_grouped_synonyms = AnnotationThesaurus.connected_labels(synonym_pairs)\n        return merged_grouped_synonyms\n\n    @staticmethod\n    def connected_annotations(synonym_pairs: List[Tuple[str, str]]):\n\"\"\"Build a graph for adjacency among the class annotations (labels) such that\n        the **transitivity** of synonyms is ensured.\n\n        Auxiliary function for [`merge_synonym_groups_by_transitivity`][deeponto.align.bertmap.text_semantics.AnnotationThesaurus.merge_synonym_groups_by_transitivity].\n\n        Args:\n            synonym_pairs (List[Tuple[str, str]]): List of pairs of phrases that are synonymous.\n\n        Returns:\n            (List[Set[str]]): A list of synonym groups.\n        \"\"\"\n        graph = nx.Graph()\n        graph.add_edges_from(synonym_pairs)\n        # nx.draw(G, with_labels = True)\n        connected = list(nx.connected_components(graph))\n        return connected\n\n    def synonym_sampling(self, num_samples: Optional[int] = None):\nr\"\"\"Sample synonym pairs from a list of synonym groups extracted from the input ontology.\n\n        According to the $\\textsf{BERTMap}$ paper, **synonyms** are defined as label pairs that belong\n        to the same ontology class.\n\n        NOTE this has been validated for getting the same results as in the original $\\textsf{BERTMap}$ repository.\n\n        Args:\n            num_samples (int, optional): The (maximum) number of **unique** samples extracted. Defaults to `None`.\n\n        Returns:\n            (List[Tuple[str, str]]): A list of unique synonym pair samples.\n        \"\"\"\n        synonym_pool = []\n        for synonym_group in self.synonym_groups:\n            # do not remove duplicates in the loop to save time\n            synonym_pairs = self.get_synonym_pairs(synonym_group, remove_duplicates=False)\n            synonym_pool += synonym_pairs\n        # remove duplicates afer the loop\n        synonym_pool = DataUtils.uniqify(synonym_pool)\n\n        if (not num_samples) or (num_samples &gt;= len(synonym_pool)):\n            # print(\"Return all synonym pairs without downsampling.\")\n            return synonym_pool\n        else:\n            return random.sample(synonym_pool, num_samples)\n\n    def soft_nonsynonym_sampling(self, num_samples: int, max_iter: int = 5):\nr\"\"\"Sample **soft** non-synonyms from a list of synonym groups extracted from the input ontology.\n\n        According to the $\\textsf{BERTMap}$ paper, **soft non-synonyms** are defined as label pairs\n        from two *different* synonym groups that are **randomly** selected.\n\n        Args:\n            num_samples (int): The (maximum) number of **unique** samples extracted; this is\n                required **unlike for synonym sampling** because the non-synonym pool is **significantly\n                larger** (considering random combinations of different synonym groups).\n            max_iter (int): The maximum number of iterations for conducting sampling. Defaults to `5`.\n\n        Returns:\n            (List[Tuple[str, str]]): A list of unique (soft) non-synonym pair samples.\n        \"\"\"\n        nonsyonym_pool = []\n        # randomly select disjoint synonym group pairs from all\n        for _ in range(num_samples):\n            left_synonym_group, right_synonym_group = tuple(random.sample(self.synonym_groups, 2))\n            # randomly choose one label from a synonym group\n            left_label = random.choice(list(left_synonym_group))\n            right_label = random.choice(list(right_synonym_group))\n            nonsyonym_pool.append((left_label, right_label))\n\n        # DataUtils.uniqify is too slow so we should avoid operating it too often\n        nonsyonym_pool = DataUtils.uniqify(nonsyonym_pool)\n\n        while len(nonsyonym_pool) &lt; num_samples and max_iter &gt; 0:\n            max_iter = max_iter - 1  # reduce the iteration to prevent exhausting loop\n            nonsyonym_pool += self.soft_nonsynonym_sampling(num_samples - len(nonsyonym_pool), max_iter)\n            nonsyonym_pool = DataUtils.uniqify(nonsyonym_pool)\n\n        return nonsyonym_pool\n\n    def weighted_random_choices_of_sibling_groups(self, k: int = 1):\n\"\"\"Randomly (weighted) select a number of sibling class groups.\n\n        The weights are computed according to the sizes of the sibling class groups.\n        \"\"\"\n        weights = [len(s) for s in self.onto.sibling_class_groups]\n        weights = [w / sum(weights) for w in weights]  # normalised\n        return random.choices(self.onto.sibling_class_groups, weights=weights, k=k)\n\n    def hard_nonsynonym_sampling(self, num_samples: int, max_iter: int = 5):\nr\"\"\"Sample **hard** non-synonyms from sibling classes of the input ontology.\n\n        According to the $\\textsf{BERTMap}$ paper, **hard non-synonyms** are defined as label pairs\n        that belong to two **disjoint** ontology classes. For practical reason, the condition\n        is eased to two **sibling** ontology classes.\n\n        Args:\n            num_samples (int): The (maximum) number of **unique** samples extracted; this is\n                required **unlike for synonym sampling** because the non-synonym pool is **significantly\n                larger** (considering random combinations of different synonym groups).\n            max_iter (int): The maximum number of iterations for conducting sampling. Defaults to `5`.\n\n        Returns:\n            (List[Tuple[str, str]]): A list of unique (hard) non-synonym pair samples.\n        \"\"\"\n        # intialise the sibling class groups\n        self.onto.sibling_class_groups\n\n        # flatten the disjointness groups into all pairs of hard neagtives\n        nonsynonym_pool = []\n        # randomly (weighted) select a number of sibling class groups with replacement\n        sibling_class_groups = self.weighted_random_choices_of_sibling_groups(k=num_samples)\n\n        for sibling_class_group in sibling_class_groups:\n            # random select two sibling classes; no weights this time\n            left_class_iri, right_class_iri = tuple(random.sample(sibling_class_group, 2))\n            # random select a label for each of them\n            left_label = random.choice(list(self.annotation_index[left_class_iri]))\n            right_label = random.choice(list(self.annotation_index[right_class_iri]))\n            # add the label pair to the pool\n            nonsynonym_pool.append((left_label, right_label))\n\n        # DataUtils.uniqify is too slow so we should avoid operating it too often\n        nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n\n        while len(nonsynonym_pool) &lt; num_samples and max_iter &gt; 0:\n            max_iter = max_iter - 1  # reduce the iteration to prevent exhausting loop\n            nonsynonym_pool += self.hard_nonsynonym_sampling(num_samples - len(nonsynonym_pool), max_iter)\n            nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n\n        return nonsynonym_pool\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.__init__","title":"<code>__init__(onto, annotation_property_iris, apply_transitivity=False)</code>","text":"<p>Initialise a thesaurus for ontology class annotations.</p> <p>Parameters:</p> Name Type Description Default <code>onto</code> <code>Ontology</code> <p>The input ontology to extract annotations from.</p> required <code>annotation_property_iris</code> <code>List[str]</code> <p>Specify which annotation properties to be used.</p> required <code>apply_transitivity</code> <code>bool</code> <p>Apply synonym transitivity to merge synonym groups or not. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def __init__(self, onto: Ontology, annotation_property_iris: List[str], apply_transitivity: bool = False):\nr\"\"\"Initialise a thesaurus for ontology class annotations.\n\n    Args:\n        onto (Ontology): The input ontology to extract annotations from.\n        annotation_property_iris (List[str]): Specify which annotation properties to be used.\n        apply_transitivity (bool, optional): Apply synonym transitivity to merge synonym groups or not. Defaults to `False`.\n    \"\"\"\n\n    self.onto = onto\n    # build the annotation index to extract synonyms from `onto`\n    # the input property iris may not exist in this ontology\n    # the output property iris will be truncated to the existing ones\n    index, iris = self.onto.build_annotation_index(\n        annotation_property_iris=annotation_property_iris,\n        entity_type=\"Classes\",\n        apply_lowercasing=True,\n    )\n    self.annotation_index = index\n    self.annotation_property_iris = iris\n    total_number_of_annotations = sum([len(v) for v in self.annotation_index.values()])\n    self.average_number_of_annotations_per_class = total_number_of_annotations / len(self.annotation_index)\n\n    # synonym groups\n    self.apply_transitivity = apply_transitivity\n    self.synonym_groups = list(self.annotation_index.values())\n    if self.apply_transitivity:\n        self.synonym_groups = self.merge_synonym_groups_by_transitivity(self.synonym_groups)\n\n    # summary\n    self.info = {\n        type(self).__name__: {\n            \"ontology\": self.onto.info[type(self.onto).__name__],\n            \"average_number_of_annotations_per_class\": round(self.average_number_of_annotations_per_class, 3),\n            \"number_of_synonym_groups\": len(self.synonym_groups),\n        }\n    }\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.get_synonym_pairs","title":"<code>get_synonym_pairs(synonym_group, remove_duplicates=True)</code>  <code>staticmethod</code>","text":"<p>Get synonym pairs from a synonym group through a cartesian product.</p> <p>Parameters:</p> Name Type Description Default <code>synonym_group</code> <code>Set[str]</code> <p>A set of annotation phrases that are synonymous to each other.</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>A list of synonym pairs.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>@staticmethod\ndef get_synonym_pairs(synonym_group: Set[str], remove_duplicates: bool = True):\n\"\"\"Get synonym pairs from a synonym group through a cartesian product.\n\n    Args:\n        synonym_group (Set[str]): A set of annotation phrases that are synonymous to each other.\n\n    Returns:\n        (List[Tuple[str, str]]): A list of synonym pairs.\n    \"\"\"\n    synonym_pairs = list(itertools.product(synonym_group, synonym_group))\n    if remove_duplicates:\n        return DataUtils.uniqify(synonym_pairs)\n    else:\n        return synonym_pairs\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.merge_synonym_groups_by_transitivity","title":"<code>merge_synonym_groups_by_transitivity(synonym_groups)</code>  <code>staticmethod</code>","text":"<p>Merge synonym groups by transitivity.</p> <p>Synonym groups that share a common annotation phrase will be merged. NOTE that for multiple ontologies, we can merge their synonym groups by first concatenating them then use this function.</p> <p>Note</p> <p>In \\(\\textsf{BERTMap}\\) experiments we have considered this as a data augmentation approach but it does not bring a significant performance improvement. However, if the overall number of annotations is not large enough then this could be a good option.</p> <p>Parameters:</p> Name Type Description Default <code>*synonym_groups</code> <code>List[Set[str]]</code> <p>A sequence of synonym groups to be merged.</p> required <p>Returns:</p> Type Description <code>List[Set[str]]</code> <p>A list of merged synonym groups.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>@staticmethod\ndef merge_synonym_groups_by_transitivity(synonym_groups: List[Set[str]]):\nr\"\"\"Merge synonym groups by transitivity.\n\n    Synonym groups that share a common annotation phrase will be merged. NOTE that for\n    multiple ontologies, we can merge their synonym groups by first concatenating them\n    then use this function.\n\n    !!! note\n\n        In $\\textsf{BERTMap}$ experiments we have considered this as a data augmentation approach\n        but it does not bring a significant performance improvement. However, if the\n        overall number of annotations is not large enough then this could be a good option.\n\n    Args:\n        *synonym_groups (List[Set[str]]): A sequence of synonym groups to be merged.\n\n    Returns:\n        (List[Set[str]]): A list of merged synonym groups.\n    \"\"\"\n    synonym_pairs = []\n    for synonym_group in synonym_groups:\n        # gather synonym pairs from the self-product of a synonym group\n        synonym_pairs += AnnotationThesaurus.get_synonym_pairs(synonym_group, remove_duplicates=False)\n    synonym_pairs = DataUtils.uniqify(synonym_pairs)\n    merged_grouped_synonyms = AnnotationThesaurus.connected_labels(synonym_pairs)\n    return merged_grouped_synonyms\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.connected_annotations","title":"<code>connected_annotations(synonym_pairs)</code>  <code>staticmethod</code>","text":"<p>Build a graph for adjacency among the class annotations (labels) such that the transitivity of synonyms is ensured.</p> <p>Auxiliary function for <code>merge_synonym_groups_by_transitivity</code>.</p> <p>Parameters:</p> Name Type Description Default <code>synonym_pairs</code> <code>List[Tuple[str, str]]</code> <p>List of pairs of phrases that are synonymous.</p> required <p>Returns:</p> Type Description <code>List[Set[str]]</code> <p>A list of synonym groups.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>@staticmethod\ndef connected_annotations(synonym_pairs: List[Tuple[str, str]]):\n\"\"\"Build a graph for adjacency among the class annotations (labels) such that\n    the **transitivity** of synonyms is ensured.\n\n    Auxiliary function for [`merge_synonym_groups_by_transitivity`][deeponto.align.bertmap.text_semantics.AnnotationThesaurus.merge_synonym_groups_by_transitivity].\n\n    Args:\n        synonym_pairs (List[Tuple[str, str]]): List of pairs of phrases that are synonymous.\n\n    Returns:\n        (List[Set[str]]): A list of synonym groups.\n    \"\"\"\n    graph = nx.Graph()\n    graph.add_edges_from(synonym_pairs)\n    # nx.draw(G, with_labels = True)\n    connected = list(nx.connected_components(graph))\n    return connected\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.synonym_sampling","title":"<code>synonym_sampling(num_samples=None)</code>","text":"<p>Sample synonym pairs from a list of synonym groups extracted from the input ontology.</p> <p>According to the \\(\\textsf{BERTMap}\\) paper, synonyms are defined as label pairs that belong to the same ontology class.</p> <p>NOTE this has been validated for getting the same results as in the original \\(\\textsf{BERTMap}\\) repository.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The (maximum) number of unique samples extracted. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>A list of unique synonym pair samples.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def synonym_sampling(self, num_samples: Optional[int] = None):\nr\"\"\"Sample synonym pairs from a list of synonym groups extracted from the input ontology.\n\n    According to the $\\textsf{BERTMap}$ paper, **synonyms** are defined as label pairs that belong\n    to the same ontology class.\n\n    NOTE this has been validated for getting the same results as in the original $\\textsf{BERTMap}$ repository.\n\n    Args:\n        num_samples (int, optional): The (maximum) number of **unique** samples extracted. Defaults to `None`.\n\n    Returns:\n        (List[Tuple[str, str]]): A list of unique synonym pair samples.\n    \"\"\"\n    synonym_pool = []\n    for synonym_group in self.synonym_groups:\n        # do not remove duplicates in the loop to save time\n        synonym_pairs = self.get_synonym_pairs(synonym_group, remove_duplicates=False)\n        synonym_pool += synonym_pairs\n    # remove duplicates afer the loop\n    synonym_pool = DataUtils.uniqify(synonym_pool)\n\n    if (not num_samples) or (num_samples &gt;= len(synonym_pool)):\n        # print(\"Return all synonym pairs without downsampling.\")\n        return synonym_pool\n    else:\n        return random.sample(synonym_pool, num_samples)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.soft_nonsynonym_sampling","title":"<code>soft_nonsynonym_sampling(num_samples, max_iter=5)</code>","text":"<p>Sample soft non-synonyms from a list of synonym groups extracted from the input ontology.</p> <p>According to the \\(\\textsf{BERTMap}\\) paper, soft non-synonyms are defined as label pairs from two different synonym groups that are randomly selected.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The (maximum) number of unique samples extracted; this is required unlike for synonym sampling because the non-synonym pool is significantly larger (considering random combinations of different synonym groups).</p> required <code>max_iter</code> <code>int</code> <p>The maximum number of iterations for conducting sampling. Defaults to <code>5</code>.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>A list of unique (soft) non-synonym pair samples.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def soft_nonsynonym_sampling(self, num_samples: int, max_iter: int = 5):\nr\"\"\"Sample **soft** non-synonyms from a list of synonym groups extracted from the input ontology.\n\n    According to the $\\textsf{BERTMap}$ paper, **soft non-synonyms** are defined as label pairs\n    from two *different* synonym groups that are **randomly** selected.\n\n    Args:\n        num_samples (int): The (maximum) number of **unique** samples extracted; this is\n            required **unlike for synonym sampling** because the non-synonym pool is **significantly\n            larger** (considering random combinations of different synonym groups).\n        max_iter (int): The maximum number of iterations for conducting sampling. Defaults to `5`.\n\n    Returns:\n        (List[Tuple[str, str]]): A list of unique (soft) non-synonym pair samples.\n    \"\"\"\n    nonsyonym_pool = []\n    # randomly select disjoint synonym group pairs from all\n    for _ in range(num_samples):\n        left_synonym_group, right_synonym_group = tuple(random.sample(self.synonym_groups, 2))\n        # randomly choose one label from a synonym group\n        left_label = random.choice(list(left_synonym_group))\n        right_label = random.choice(list(right_synonym_group))\n        nonsyonym_pool.append((left_label, right_label))\n\n    # DataUtils.uniqify is too slow so we should avoid operating it too often\n    nonsyonym_pool = DataUtils.uniqify(nonsyonym_pool)\n\n    while len(nonsyonym_pool) &lt; num_samples and max_iter &gt; 0:\n        max_iter = max_iter - 1  # reduce the iteration to prevent exhausting loop\n        nonsyonym_pool += self.soft_nonsynonym_sampling(num_samples - len(nonsyonym_pool), max_iter)\n        nonsyonym_pool = DataUtils.uniqify(nonsyonym_pool)\n\n    return nonsyonym_pool\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.weighted_random_choices_of_sibling_groups","title":"<code>weighted_random_choices_of_sibling_groups(k=1)</code>","text":"<p>Randomly (weighted) select a number of sibling class groups.</p> <p>The weights are computed according to the sizes of the sibling class groups.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def weighted_random_choices_of_sibling_groups(self, k: int = 1):\n\"\"\"Randomly (weighted) select a number of sibling class groups.\n\n    The weights are computed according to the sizes of the sibling class groups.\n    \"\"\"\n    weights = [len(s) for s in self.onto.sibling_class_groups]\n    weights = [w / sum(weights) for w in weights]  # normalised\n    return random.choices(self.onto.sibling_class_groups, weights=weights, k=k)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.AnnotationThesaurus.hard_nonsynonym_sampling","title":"<code>hard_nonsynonym_sampling(num_samples, max_iter=5)</code>","text":"<p>Sample hard non-synonyms from sibling classes of the input ontology.</p> <p>According to the \\(\\textsf{BERTMap}\\) paper, hard non-synonyms are defined as label pairs that belong to two disjoint ontology classes. For practical reason, the condition is eased to two sibling ontology classes.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The (maximum) number of unique samples extracted; this is required unlike for synonym sampling because the non-synonym pool is significantly larger (considering random combinations of different synonym groups).</p> required <code>max_iter</code> <code>int</code> <p>The maximum number of iterations for conducting sampling. Defaults to <code>5</code>.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>A list of unique (hard) non-synonym pair samples.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def hard_nonsynonym_sampling(self, num_samples: int, max_iter: int = 5):\nr\"\"\"Sample **hard** non-synonyms from sibling classes of the input ontology.\n\n    According to the $\\textsf{BERTMap}$ paper, **hard non-synonyms** are defined as label pairs\n    that belong to two **disjoint** ontology classes. For practical reason, the condition\n    is eased to two **sibling** ontology classes.\n\n    Args:\n        num_samples (int): The (maximum) number of **unique** samples extracted; this is\n            required **unlike for synonym sampling** because the non-synonym pool is **significantly\n            larger** (considering random combinations of different synonym groups).\n        max_iter (int): The maximum number of iterations for conducting sampling. Defaults to `5`.\n\n    Returns:\n        (List[Tuple[str, str]]): A list of unique (hard) non-synonym pair samples.\n    \"\"\"\n    # intialise the sibling class groups\n    self.onto.sibling_class_groups\n\n    # flatten the disjointness groups into all pairs of hard neagtives\n    nonsynonym_pool = []\n    # randomly (weighted) select a number of sibling class groups with replacement\n    sibling_class_groups = self.weighted_random_choices_of_sibling_groups(k=num_samples)\n\n    for sibling_class_group in sibling_class_groups:\n        # random select two sibling classes; no weights this time\n        left_class_iri, right_class_iri = tuple(random.sample(sibling_class_group, 2))\n        # random select a label for each of them\n        left_label = random.choice(list(self.annotation_index[left_class_iri]))\n        right_label = random.choice(list(self.annotation_index[right_class_iri]))\n        # add the label pair to the pool\n        nonsynonym_pool.append((left_label, right_label))\n\n    # DataUtils.uniqify is too slow so we should avoid operating it too often\n    nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n\n    while len(nonsynonym_pool) &lt; num_samples and max_iter &gt; 0:\n        max_iter = max_iter - 1  # reduce the iteration to prevent exhausting loop\n        nonsynonym_pool += self.hard_nonsynonym_sampling(num_samples - len(nonsynonym_pool), max_iter)\n        nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n\n    return nonsynonym_pool\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.IntraOntologyTextSemanticsCorpus","title":"<code>IntraOntologyTextSemanticsCorpus</code>","text":"<p>Class for creating the intra-ontology text semantics corpus from an ontology.</p> <p>As defined in the \\(\\textsf{BERTMap}\\) paper, the intra-ontology text semantics corpus consists of synonym and non-synonym pairs extracted from the ontology class annotations.</p> <p>Attributes:</p> Name Type Description <code>onto</code> <code>Ontology</code> <p>An ontology to construct the intra-ontology text semantics corpus from.</p> <code>annotation_property_iris</code> <code>List[str]</code> <p>Specify which annotation properties to be used.</p> <code>soft_negative_ratio</code> <code>int</code> <p>The expected negative sample ratio of the soft non-synonyms to the extracted synonyms. Defaults to <code>2</code>.</p> <code>hard_negative_ratio</code> <code>int</code> <p>The expected negative sample ratio of the hard non-synonyms to the extracted synonyms. Defaults to <code>2</code>. However, hard non-synonyms are sometimes insufficient given an ontology's hierarchy, the soft ones are used to compensate the number in this case.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>class IntraOntologyTextSemanticsCorpus:\nr\"\"\"Class for creating the intra-ontology text semantics corpus from an ontology.\n\n    As defined in the $\\textsf{BERTMap}$ paper, the **intra-ontology** text semantics corpus consists\n    of synonym and non-synonym pairs extracted from the ontology class annotations.\n\n    Attributes:\n        onto (Ontology): An ontology to construct the intra-ontology text semantics corpus from.\n        annotation_property_iris (List[str]): Specify which annotation properties to be used.\n        soft_negative_ratio (int): The expected negative sample ratio of the soft non-synonyms to the extracted synonyms. Defaults to `2`.\n        hard_negative_ratio (int): The expected negative sample ratio of the hard non-synonyms to the extracted synonyms. Defaults to `2`.\n            However, hard non-synonyms are sometimes insufficient given an ontology's hierarchy, the soft ones are used to compensate\n            the number in this case.\n    \"\"\"\n\n    def __init__(\n        self,\n        onto: Ontology,\n        annotation_property_iris: List[str],\n        soft_negative_ratio: int = 2,\n        hard_negative_ratio: int = 2,\n    ):\n\n        self.onto = onto\n        # $\\textsf{BERTMap}$ does not apply synonym transitivity\n        self.thesaurus = AnnotationThesaurus(onto, annotation_property_iris, apply_transitivity=False)\n\n        self.synonyms = self.thesaurus.synonym_sampling()\n        # sample hard negatives first as they might not be enough\n        num_hard = hard_negative_ratio * len(self.synonyms)\n        self.hard_nonsynonyms = self.thesaurus.hard_nonsynonym_sampling(num_hard)\n        # compensate the number of hard negatives as soft negatives are almost always available\n        num_soft = (soft_negative_ratio + hard_negative_ratio) * len(self.synonyms) - len(self.hard_nonsynonyms)\n        self.soft_nonsynonyms = self.thesaurus.soft_nonsynonym_sampling(num_soft)\n\n        self.info = {\n            type(self).__name__: {\n                \"num_synonyms\": len(self.synonyms),\n                \"num_nonsynonyms\": len(self.soft_nonsynonyms) + len(self.hard_nonsynonyms),\n                \"num_soft_nonsynonyms\": len(self.soft_nonsynonyms),\n                \"num_hard_nonsynonyms\": len(self.hard_nonsynonyms),\n                \"annotation_thesaurus\": self.thesaurus.info[\"AnnotationThesaurus\"],\n            }\n        }\n\n    def __str__(self):\n        return FileUtils.print_dict(self.info)\n\n    def save(self, save_path: str):\n\"\"\"Save the intra-ontology corpus (a `.json` file for label pairs\n        and its summary) in the specified directory.\n        \"\"\"\n        FileUtils.create_path(save_path)\n        save_json = {\n            \"summary\": self.info,\n            \"synonyms\": [(pos[0], pos[1], 1) for pos in self.synonyms],\n            \"nonsynonyms\": [(neg[0], neg[1], 0) for neg in self.soft_nonsynonyms + self.hard_nonsynonyms],\n        }\n        FileUtils.save_file(save_json, os.path.join(save_path, \"intra-onto.corpus.json\"))\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.IntraOntologyTextSemanticsCorpus.save","title":"<code>save(save_path)</code>","text":"<p>Save the intra-ontology corpus (a <code>.json</code> file for label pairs and its summary) in the specified directory.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def save(self, save_path: str):\n\"\"\"Save the intra-ontology corpus (a `.json` file for label pairs\n    and its summary) in the specified directory.\n    \"\"\"\n    FileUtils.create_path(save_path)\n    save_json = {\n        \"summary\": self.info,\n        \"synonyms\": [(pos[0], pos[1], 1) for pos in self.synonyms],\n        \"nonsynonyms\": [(neg[0], neg[1], 0) for neg in self.soft_nonsynonyms + self.hard_nonsynonyms],\n    }\n    FileUtils.save_file(save_json, os.path.join(save_path, \"intra-onto.corpus.json\"))\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus","title":"<code>CrossOntologyTextSemanticsCorpus</code>","text":"<p>Class for creating the cross-ontology text semantics corpus from two ontologies and provided mappings between them.</p> <p>As defined in the \\(\\textsf{BERTMap}\\) paper, the cross-ontology text semantics corpus consists of synonym and non-synonym pairs extracted from the annotations/labels of class pairs involved in the provided cross-ontology mappigns.</p> <p>Attributes:</p> Name Type Description <code>class_mappings</code> <code>List[ReferenceMapping]</code> <p>A list of cross-ontology class mappings.</p> <code>src_onto</code> <code>Ontology</code> <p>The source ontology whose class IRIs are heads of the <code>class_mappings</code>.</p> <code>tgt_onto</code> <code>Ontology</code> <p>The target ontology whose class IRIs are tails of the <code>class_mappings</code>.</p> <code>annotation_property_iris</code> <code>List[str]</code> <p>A list of annotation property IRIs used to extract the annotations.</p> <code>negative_ratio</code> <code>int</code> <p>The expected negative sample ratio of the non-synonyms to the extracted synonyms. Defaults to <code>4</code>. NOTE that we do not have hard non-synonyms at the cross-ontology level.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>class CrossOntologyTextSemanticsCorpus:\nr\"\"\"Class for creating the cross-ontology text semantics corpus from two ontologies and provided mappings between them.\n\n    As defined in the $\\textsf{BERTMap}$ paper, the **cross-ontology** text semantics corpus consists\n    of synonym and non-synonym pairs extracted from the annotations/labels of class pairs\n    involved in the provided cross-ontology mappigns.\n\n    Attributes:\n        class_mappings (List[ReferenceMapping]): A list of cross-ontology class mappings.\n        src_onto (Ontology): The source ontology whose class IRIs are heads of the `class_mappings`.\n        tgt_onto (Ontology): The target ontology whose class IRIs are tails of the `class_mappings`.\n        annotation_property_iris (List[str]): A list of annotation property IRIs used to extract the annotations.\n        negative_ratio (int): The expected negative sample ratio of the non-synonyms to the extracted synonyms. Defaults to `4`. NOTE\n            that we do not have *hard* non-synonyms at the cross-ontology level.\n    \"\"\"\n\n    def __init__(\n        self,\n        class_mappings: List[ReferenceMapping],\n        src_onto: Ontology,\n        tgt_onto: Ontology,\n        annotation_property_iris: List[str],\n        negative_ratio: int = 4,\n    ):\n        self.class_mappings = class_mappings\n        self.src_onto = src_onto\n        self.tgt_onto = tgt_onto\n        # build the annotation thesaurus for each ontology\n        self.src_thesaurus = AnnotationThesaurus(src_onto, annotation_property_iris)\n        self.tgt_thesaurus = AnnotationThesaurus(tgt_onto, annotation_property_iris)\n        self.negative_ratio = negative_ratio\n\n        self.synonyms = self.synonym_sampling_from_mappings()\n        num_negative = negative_ratio * len(self.synonyms)\n        self.nonsynonyms = self.nonsynonym_sampling_from_mappings(num_negative)\n\n        self.info = {\n            type(self).__name__: {\n                \"num_synonyms\": len(self.synonyms),\n                \"num_nonsynonyms\": len(self.nonsynonyms),\n                \"num_mappings\": len(self.class_mappings),\n                \"src_annotation_thesaurus\": self.src_thesaurus.info[\"AnnotationThesaurus\"],\n                \"tgt_annotation_thesaurus\": self.tgt_thesaurus.info[\"AnnotationThesaurus\"],\n            }\n        }\n\n    def __str__(self):\n        return FileUtils.print_dict(self.info)\n\n    def save(self, save_path: str):\n\"\"\"Save the cross-ontology corpus (a `.json` file for label pairs\n        and its summary) in the specified directory.\n        \"\"\"\n        FileUtils.create_path(save_path)\n        save_json = {\n            \"summary\": self.info,\n            \"synonyms\": [(pos[0], pos[1], 1) for pos in self.synonyms],\n            \"nonsynonyms\": [(neg[0], neg[1], 0) for neg in self.nonsynonyms],\n        }\n        FileUtils.save_file(save_json, os.path.join(save_path, \"cross-onto.corpus.json\"))\n\n    def synonym_sampling_from_mappings(self):\nr\"\"\"Sample synonyms from cross-ontology class mappings.\n\n        Arguements of this method are all class attributes.\n        See [`CrossOntologyTextSemanticsCorpus`][deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus].\n\n        According to the $\\textsf{BERTMap}$ paper, **cross-ontology synonyms** are defined as label pairs\n        that belong to two **matched** classes. Suppose the class $C$ from the source ontology\n        and the class $D$ from the target ontology are matched according to one of the `class_mappings`,\n        then the cartesian product of labels of $C$ and labels of $D$ form cross-ontology synonyms.\n        Note that **identity synonyms** in the form of $(a, a)$ are removed because they have been covered\n        in the intra-ontology case.\n\n        Returns:\n            (List[Tuple[str, str]]): A list of unique synonym pair samples from ontology class mappings.\n        \"\"\"\n        synonym_pool = []\n\n        for class_mapping in self.class_mappings:\n            src_class_iri, tgt_class_iri = class_mapping.to_tuple()\n            src_class_annotations = self.src_thesaurus.annotation_index[src_class_iri]\n            tgt_class_annotations = self.tgt_thesaurus.annotation_index[tgt_class_iri]\n            synonym_pairs = list(itertools.product(src_class_annotations, tgt_class_annotations))\n            # remove the identity synonyms as the have been covered in the intra-ontology case\n            synonym_pairs = [(l, r) for l, r in synonym_pairs if l != r]\n            backward_synonym_pairs = [(r, l) for l, r in synonym_pairs]\n            synonym_pool += synonym_pairs + backward_synonym_pairs\n\n        synonym_pool = DataUtils.uniqify(synonym_pool)\n        return synonym_pool\n\n    def nonsynonym_sampling_from_mappings(self, num_samples: int, max_iter: int = 5):\nr\"\"\"Sample non-synonyms from cross-ontology class mappings.\n\n        Arguements of this method are all class attributes.\n        See [`CrossOntologyTextSemanticsCorpus`][deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus].\n\n        According to the $\\textsf{BERTMap}$ paper, **cross-ontology non-synonyms** are defined as label pairs\n        that belong to two **unmatched** classes. Assume that the provided class mappings are self-contained\n        in the sense that they are complete for the classes involved in them, then we can randomly\n        sample two cross-ontology classes that are not matched according to the mappings and take\n        their labels as nonsynonyms. In practice, it is quite unlikely to obtain false negatives since\n        the number of incorrect mappings is much larger than the number of correct ones.\n\n        Returns:\n            (List[Tuple[str, str]]): A list of unique nonsynonym pair samples from ontology class mappings.\n        \"\"\"\n        nonsynonym_pool = []\n\n        # form cross-ontology synonym groups\n        cross_onto_synonym_group_pair = []\n        for class_mapping in self.class_mappings:\n            src_class_iri, tgt_class_iri = class_mapping.to_tuple()\n            src_class_annotations = self.src_thesaurus.annotation_index[src_class_iri]\n            tgt_class_annotations = self.tgt_thesaurus.annotation_index[tgt_class_iri]\n            # let each matched class pair's annotations form a synonym group_pair\n            cross_onto_synonym_group_pair.append((src_class_annotations, tgt_class_annotations))\n\n        # randomly select disjoint synonym group pairs from all\n        for _ in range(num_samples):\n            left_class_pair, right_class_pair = tuple(random.sample(cross_onto_synonym_group_pair, 2))\n            # randomly choose one label from a synonym group\n            left_label = random.choice(list(left_class_pair[0]))  # choosing the src side by [0]\n            right_label = random.choice(list(right_class_pair[1]))  # choosing the tgt side by [1]\n            nonsynonym_pool.append((left_label, right_label))\n\n        # DataUtils.uniqify is too slow so we should avoid operating it too often\n        nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n        while len(nonsynonym_pool) &lt; num_samples and max_iter &gt; 0:\n            max_iter = max_iter - 1  # reduce the iteration to prevent exhausting loop\n            nonsynonym_pool += self.nonsynonym_sampling_from_mappings(num_samples - len(nonsynonym_pool), max_iter)\n            nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n        return nonsynonym_pool\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus.save","title":"<code>save(save_path)</code>","text":"<p>Save the cross-ontology corpus (a <code>.json</code> file for label pairs and its summary) in the specified directory.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def save(self, save_path: str):\n\"\"\"Save the cross-ontology corpus (a `.json` file for label pairs\n    and its summary) in the specified directory.\n    \"\"\"\n    FileUtils.create_path(save_path)\n    save_json = {\n        \"summary\": self.info,\n        \"synonyms\": [(pos[0], pos[1], 1) for pos in self.synonyms],\n        \"nonsynonyms\": [(neg[0], neg[1], 0) for neg in self.nonsynonyms],\n    }\n    FileUtils.save_file(save_json, os.path.join(save_path, \"cross-onto.corpus.json\"))\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus.synonym_sampling_from_mappings","title":"<code>synonym_sampling_from_mappings()</code>","text":"<p>Sample synonyms from cross-ontology class mappings.</p> <p>Arguements of this method are all class attributes. See <code>CrossOntologyTextSemanticsCorpus</code>.</p> <p>According to the \\(\\textsf{BERTMap}\\) paper, cross-ontology synonyms are defined as label pairs that belong to two matched classes. Suppose the class \\(C\\) from the source ontology and the class \\(D\\) from the target ontology are matched according to one of the <code>class_mappings</code>, then the cartesian product of labels of \\(C\\) and labels of \\(D\\) form cross-ontology synonyms. Note that identity synonyms in the form of \\((a, a)\\) are removed because they have been covered in the intra-ontology case.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>A list of unique synonym pair samples from ontology class mappings.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def synonym_sampling_from_mappings(self):\nr\"\"\"Sample synonyms from cross-ontology class mappings.\n\n    Arguements of this method are all class attributes.\n    See [`CrossOntologyTextSemanticsCorpus`][deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus].\n\n    According to the $\\textsf{BERTMap}$ paper, **cross-ontology synonyms** are defined as label pairs\n    that belong to two **matched** classes. Suppose the class $C$ from the source ontology\n    and the class $D$ from the target ontology are matched according to one of the `class_mappings`,\n    then the cartesian product of labels of $C$ and labels of $D$ form cross-ontology synonyms.\n    Note that **identity synonyms** in the form of $(a, a)$ are removed because they have been covered\n    in the intra-ontology case.\n\n    Returns:\n        (List[Tuple[str, str]]): A list of unique synonym pair samples from ontology class mappings.\n    \"\"\"\n    synonym_pool = []\n\n    for class_mapping in self.class_mappings:\n        src_class_iri, tgt_class_iri = class_mapping.to_tuple()\n        src_class_annotations = self.src_thesaurus.annotation_index[src_class_iri]\n        tgt_class_annotations = self.tgt_thesaurus.annotation_index[tgt_class_iri]\n        synonym_pairs = list(itertools.product(src_class_annotations, tgt_class_annotations))\n        # remove the identity synonyms as the have been covered in the intra-ontology case\n        synonym_pairs = [(l, r) for l, r in synonym_pairs if l != r]\n        backward_synonym_pairs = [(r, l) for l, r in synonym_pairs]\n        synonym_pool += synonym_pairs + backward_synonym_pairs\n\n    synonym_pool = DataUtils.uniqify(synonym_pool)\n    return synonym_pool\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus.nonsynonym_sampling_from_mappings","title":"<code>nonsynonym_sampling_from_mappings(num_samples, max_iter=5)</code>","text":"<p>Sample non-synonyms from cross-ontology class mappings.</p> <p>Arguements of this method are all class attributes. See <code>CrossOntologyTextSemanticsCorpus</code>.</p> <p>According to the \\(\\textsf{BERTMap}\\) paper, cross-ontology non-synonyms are defined as label pairs that belong to two unmatched classes. Assume that the provided class mappings are self-contained in the sense that they are complete for the classes involved in them, then we can randomly sample two cross-ontology classes that are not matched according to the mappings and take their labels as nonsynonyms. In practice, it is quite unlikely to obtain false negatives since the number of incorrect mappings is much larger than the number of correct ones.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>A list of unique nonsynonym pair samples from ontology class mappings.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def nonsynonym_sampling_from_mappings(self, num_samples: int, max_iter: int = 5):\nr\"\"\"Sample non-synonyms from cross-ontology class mappings.\n\n    Arguements of this method are all class attributes.\n    See [`CrossOntologyTextSemanticsCorpus`][deeponto.align.bertmap.text_semantics.CrossOntologyTextSemanticsCorpus].\n\n    According to the $\\textsf{BERTMap}$ paper, **cross-ontology non-synonyms** are defined as label pairs\n    that belong to two **unmatched** classes. Assume that the provided class mappings are self-contained\n    in the sense that they are complete for the classes involved in them, then we can randomly\n    sample two cross-ontology classes that are not matched according to the mappings and take\n    their labels as nonsynonyms. In practice, it is quite unlikely to obtain false negatives since\n    the number of incorrect mappings is much larger than the number of correct ones.\n\n    Returns:\n        (List[Tuple[str, str]]): A list of unique nonsynonym pair samples from ontology class mappings.\n    \"\"\"\n    nonsynonym_pool = []\n\n    # form cross-ontology synonym groups\n    cross_onto_synonym_group_pair = []\n    for class_mapping in self.class_mappings:\n        src_class_iri, tgt_class_iri = class_mapping.to_tuple()\n        src_class_annotations = self.src_thesaurus.annotation_index[src_class_iri]\n        tgt_class_annotations = self.tgt_thesaurus.annotation_index[tgt_class_iri]\n        # let each matched class pair's annotations form a synonym group_pair\n        cross_onto_synonym_group_pair.append((src_class_annotations, tgt_class_annotations))\n\n    # randomly select disjoint synonym group pairs from all\n    for _ in range(num_samples):\n        left_class_pair, right_class_pair = tuple(random.sample(cross_onto_synonym_group_pair, 2))\n        # randomly choose one label from a synonym group\n        left_label = random.choice(list(left_class_pair[0]))  # choosing the src side by [0]\n        right_label = random.choice(list(right_class_pair[1]))  # choosing the tgt side by [1]\n        nonsynonym_pool.append((left_label, right_label))\n\n    # DataUtils.uniqify is too slow so we should avoid operating it too often\n    nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n    while len(nonsynonym_pool) &lt; num_samples and max_iter &gt; 0:\n        max_iter = max_iter - 1  # reduce the iteration to prevent exhausting loop\n        nonsynonym_pool += self.nonsynonym_sampling_from_mappings(num_samples - len(nonsynonym_pool), max_iter)\n        nonsynonym_pool = DataUtils.uniqify(nonsynonym_pool)\n    return nonsynonym_pool\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.TextSemanticsCorpora","title":"<code>TextSemanticsCorpora</code>","text":"<p>Class for creating the collection text semantics corpora.</p> <p>As defined in the \\(\\textsf{BERTMap}\\) paper, the collection of text semantics corpora contains at least two intra-ontology sub-corpora from the source and target ontologies, respectively. If some class mappings are provided, then a cross-ontology sub-corpus will be created. If some additional auxiliary ontologies are provided, the intra-ontology corpora created from them will serve as the auxiliary sub-corpora.</p> <p>Attributes:</p> Name Type Description <code>src_onto</code> <code>Ontology</code> <p>The source ontology to be matched or aligned.</p> <code>tgt_onto</code> <code>Ontology</code> <p>The target ontology to be matched or aligned.</p> <code>annotation_property_iris</code> <code>List[str]</code> <p>A list of annotation property IRIs used to extract the annotations.</p> <code>class_mappings</code> <code>List[ReferenceMapping]</code> <p>A list of cross-ontology class mappings between the source and the target ontologies. Defaults to <code>None</code>.</p> <code>auxiliary_ontos</code> <code>List[Ontology]</code> <p>A list of auxiliary ontologies for augmenting more synonym/non-synonym samples. Defaults to <code>None</code>.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>class TextSemanticsCorpora:\nr\"\"\"Class for creating the collection text semantics corpora.\n\n    As defined in the $\\textsf{BERTMap}$ paper, the collection of text semantics corpora contains\n    **at least two intra-ontology sub-corpora** from the source and target ontologies, respectively.\n    If some class mappings are provided, then a **cross-ontology sub-corpus** will be created.\n    If some additional auxiliary ontologies are provided, the intra-ontology corpora created from them\n    will serve as the **auxiliary sub-corpora**.\n\n    Attributes:\n        src_onto (Ontology): The source ontology to be matched or aligned.\n        tgt_onto (Ontology): The target ontology to be matched or aligned.\n        annotation_property_iris (List[str]): A list of annotation property IRIs used to extract the annotations.\n        class_mappings (List[ReferenceMapping], optional): A list of cross-ontology class mappings between the\n            source and the target ontologies. Defaults to `None`.\n        auxiliary_ontos (List[Ontology], optional): A list of auxiliary ontologies for augmenting more synonym/non-synonym samples. Defaults to `None`.\n    \"\"\"\n\n    def __init__(\n        self,\n        src_onto: Ontology,\n        tgt_onto: Ontology,\n        annotation_property_iris: List[str],\n        class_mappings: Optional[List[ReferenceMapping]] = None,\n        auxiliary_ontos: Optional[List[Ontology]] = None,\n    ):\n        self.synonyms = []\n        self.nonsynonyms = []\n\n        # build intra-ontology corpora\n        # negative sample ratios are by default\n        self.intra_src_onto_corpus = IntraOntologyTextSemanticsCorpus(src_onto, annotation_property_iris)\n        self.add_samples_from_sub_corpus(self.intra_src_onto_corpus)\n        self.intra_tgt_onto_corpus = IntraOntologyTextSemanticsCorpus(tgt_onto, annotation_property_iris)\n        self.add_samples_from_sub_corpus(self.intra_tgt_onto_corpus)\n\n        # build cross-ontolgoy corpora\n        self.class_mappings = class_mappings\n        self.cross_onto_corpus = None\n        if self.class_mappings:\n            self.cross_onto_corpus = CrossOntologyTextSemanticsCorpus(\n                class_mappings, src_onto, tgt_onto, annotation_property_iris\n            )\n            self.add_samples_from_sub_corpus(self.cross_onto_corpus)\n\n        # build auxiliary ontology corpora (same as intra-ontology)\n        self.auxiliary_ontos = auxiliary_ontos\n        self.auxiliary_onto_corpora = []\n        if self.auxiliary_ontos:\n            for auxiliary_onto in self.auxiliary_ontos:\n                self.auxiliary_onto_corpora.append(\n                    IntraOntologyTextSemanticsCorpus(auxiliary_onto, annotation_property_iris)\n                )\n        for auxiliary_onto_corpus in self.auxiliary_onto_corpora:\n            self.add_samples_from_sub_corpus(auxiliary_onto_corpus)\n\n        # DataUtils.uniqify the samples\n        self.synonyms = DataUtils.uniqify(self.synonyms)\n        self.nonsynonyms = DataUtils.uniqify(self.nonsynonyms)\n        # remove invalid nonsynonyms\n        self.nonsynonyms = list(set(self.nonsynonyms) - set(self.synonyms))\n\n        # summary\n        self.info = {\n            type(self).__name__: {\n                \"num_synonyms\": len(self.synonyms),\n                \"num_nonsynonyms\": len(self.nonsynonyms),\n                \"intra_src_onto_corpus\": self.intra_src_onto_corpus.info[\"IntraOntologyTextSemanticsCorpus\"],\n                \"intra_tgt_onto_corpus\": self.intra_tgt_onto_corpus.info[\"IntraOntologyTextSemanticsCorpus\"],\n                \"cross_onto_corpus\": self.cross_onto_corpus.info[\"CrossOntologyTextSemanticsCorpus\"] if self.cross_onto_corpus else None,\n                \"auxiliary_onto_corpora\": [a.info[\"IntraOntologyTextSemanticsCorpus\"] for a in self.auxiliary_onto_corpora],\n            }\n        }\n\n    def __str__(self):\n        return FileUtils.print_dict(self.info)\n\n    def save(self, save_path: str):\n\"\"\"Save the overall text semantics corpora (a `.json` file for label pairs\n        and its summary) in the specified directory.\n        \"\"\"\n        FileUtils.create_path(save_path)\n        save_json = {\n            \"summary\": self.info,\n            \"synonyms\": [(pos[0], pos[1], 1) for pos in self.synonyms],\n            \"nonsynonyms\": [(neg[0], neg[1], 0) for neg in self.nonsynonyms],\n        }\n        FileUtils.save_file(save_json, os.path.join(save_path, \"text-semantics.corpora.json\"))\n\n    def add_samples_from_sub_corpus(\n        self, sub_corpus: Union[IntraOntologyTextSemanticsCorpus, CrossOntologyTextSemanticsCorpus]\n    ):\n\"\"\"Add synonyms and non-synonyms from each sub-corpus to the overall collection.\"\"\"\n        self.synonyms += sub_corpus.synonyms\n        if isinstance(sub_corpus, IntraOntologyTextSemanticsCorpus):\n            self.nonsynonyms += sub_corpus.soft_nonsynonyms + sub_corpus.hard_nonsynonyms\n        else:\n            self.nonsynonyms += sub_corpus.nonsynonyms\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.TextSemanticsCorpora.save","title":"<code>save(save_path)</code>","text":"<p>Save the overall text semantics corpora (a <code>.json</code> file for label pairs and its summary) in the specified directory.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def save(self, save_path: str):\n\"\"\"Save the overall text semantics corpora (a `.json` file for label pairs\n    and its summary) in the specified directory.\n    \"\"\"\n    FileUtils.create_path(save_path)\n    save_json = {\n        \"summary\": self.info,\n        \"synonyms\": [(pos[0], pos[1], 1) for pos in self.synonyms],\n        \"nonsynonyms\": [(neg[0], neg[1], 0) for neg in self.nonsynonyms],\n    }\n    FileUtils.save_file(save_json, os.path.join(save_path, \"text-semantics.corpora.json\"))\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.text_semantics.TextSemanticsCorpora.add_samples_from_sub_corpus","title":"<code>add_samples_from_sub_corpus(sub_corpus)</code>","text":"<p>Add synonyms and non-synonyms from each sub-corpus to the overall collection.</p> Source code in <code>deeponto/align/bertmap/text_semantics.py</code> <pre><code>def add_samples_from_sub_corpus(\n    self, sub_corpus: Union[IntraOntologyTextSemanticsCorpus, CrossOntologyTextSemanticsCorpus]\n):\n\"\"\"Add synonyms and non-synonyms from each sub-corpus to the overall collection.\"\"\"\n    self.synonyms += sub_corpus.synonyms\n    if isinstance(sub_corpus, IntraOntologyTextSemanticsCorpus):\n        self.nonsynonyms += sub_corpus.soft_nonsynonyms + sub_corpus.hard_nonsynonyms\n    else:\n        self.nonsynonyms += sub_corpus.nonsynonyms\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier","title":"<code>BERTSynonymClassifier</code>","text":"<p>Class for BERT synonym classifier.</p> <p>The main scoring module of \\(\\textsf{BERTMap}\\) consisting of a BERT model and a binary synonym classifier.</p> <p>Attributes:</p> Name Type Description <code>loaded_path</code> <code>str</code> <p>The path to the checkpoint of a pre-trained BERT model.</p> <code>output_path</code> <code>str</code> <p>The path to the output BERT model (usually fine-tuned).</p> <code>eval_mode</code> <code>bool</code> <p>Set to <code>False</code> if the model is loaded for training.</p> <code>max_length_for_input</code> <code>int</code> <p>The maximum length of an input sequence.</p> <code>num_epochs_for_training</code> <code>int</code> <p>The number of epochs for training a BERT model.</p> <code>batch_size_for_training</code> <code>int</code> <p>The batch size for training a BERT model.</p> <code>batch_size_for_prediction</code> <code>int</code> <p>The batch size for making predictions.</p> <code>training_data</code> <code>Dataset</code> <p>Data for training the model if <code>for_training</code> is set to <code>True</code>. Defaults to <code>None</code>.</p> <code>validation_data</code> <code>Dataset</code> <p>Data for validating the model if <code>for_training</code> is set to <code>True</code>. Defaults to <code>None</code>.</p> <code>training_args</code> <code>TrainingArguments</code> <p>Training arguments for training the model if <code>for_training</code> is set to <code>True</code>. Defaults to <code>None</code>.</p> <code>trainer</code> <code>Trainer</code> <p>The model trainer fed with <code>training_args</code> and data samples. Defaults to <code>None</code>.</p> <code>softmax</code> <code>torch.nn.SoftMax</code> <p>The softmax layer used for normalising synonym scores. Defaults to <code>None</code>.</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>class BERTSynonymClassifier:\nr\"\"\"Class for BERT synonym classifier.\n\n    The main scoring module of $\\textsf{BERTMap}$ consisting of a BERT model and a binary synonym classifier.\n\n    Attributes:\n        loaded_path (str): The path to the checkpoint of a pre-trained BERT model.\n        output_path (str): The path to the output BERT model (usually fine-tuned).\n        eval_mode (bool): Set to `False` if the model is loaded for training.\n        max_length_for_input (int): The maximum length of an input sequence.\n        num_epochs_for_training (int): The number of epochs for training a BERT model.\n        batch_size_for_training (int): The batch size for training a BERT model.\n        batch_size_for_prediction (int): The batch size for making predictions.\n        training_data (Dataset, optional): Data for training the model if `for_training` is set to `True`. Defaults to `None`.\n        validation_data (Dataset, optional): Data for validating the model if `for_training` is set to `True`. Defaults to `None`.\n        training_args (TrainingArguments, optional): Training arguments for training the model if `for_training` is set to `True`. Defaults to `None`.\n        trainer (Trainer, optional): The model trainer fed with `training_args` and data samples. Defaults to `None`.\n        softmax (torch.nn.SoftMax, optional): The softmax layer used for normalising synonym scores. Defaults to `None`.\n    \"\"\"\n\n    def __init__(\n        self,\n        loaded_path: str,\n        output_path: str,\n        eval_mode: bool,\n        max_length_for_input: int,\n        num_epochs_for_training: Optional[float] = None,\n        batch_size_for_training: Optional[int] = None,\n        batch_size_for_prediction: Optional[int] = None,\n        training_data: Optional[List[Tuple[str, str, int]]] = None,  # (sentence1, sentence2, label)\n        validation_data: Optional[List[Tuple[str, str, int]]] = None,\n    ):\n        # Load the pretrained BERT model from the given path\n        self.loaded_path = loaded_path\n        print(f\"Loading a BERT model from: {self.loaded_path}.\")\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            self.loaded_path, output_hidden_states=eval_mode\n        )\n        self.tokenizer = Tokenizer.from_pretrained(loaded_path)\n\n        self.output_path = output_path\n        self.eval_mode = eval_mode\n        self.max_length_for_input = max_length_for_input\n        self.num_epochs_for_training = num_epochs_for_training\n        self.batch_size_for_training = batch_size_for_training\n        self.batch_size_for_prediction = batch_size_for_prediction\n        self.training_data = None\n        self.validation_data = None\n        self.data_stat = {}\n        self.training_args = None\n        self.trainer = None\n        self.softmax = None\n\n        # load the pre-trained BERT model and set it to eval mode (static)\n        if self.eval_mode:\n            self.eval()\n        # load the pre-trained BERT model for fine-tuning\n        else:\n            if not training_data:\n                raise RuntimeError(\"Training data should be provided when `for_training` is `True`.\")\n            if not validation_data:\n                raise RuntimeError(\"Validation data should be provided when `for_training` is `True`.\")\n            # load data (max_length is used for truncation)\n            self.training_data = self.load_dataset(training_data, \"training\")\n            self.validation_data = self.load_dataset(validation_data, \"validation\")\n            self.data_stat = {\n                \"num_training\": len(self.training_data),\n                \"num_validation\": len(self.validation_data),\n            }\n\n            # generate training arguments\n            epoch_steps = len(self.training_data) // self.batch_size_for_training  # total steps of an epoch\n            if torch.cuda.device_count() &gt; 0:\n                epoch_steps = epoch_steps // torch.cuda.device_count()  # to deal with multi-gpus case\n            # keep logging steps consisitent even for small batch size\n            # report logging on every 0.02 epoch\n            logging_steps = int(epoch_steps * 0.02)\n            # eval on every 0.2 epoch\n            eval_steps = 10 * logging_steps\n            # generate the training arguments\n            self.training_args = TrainingArguments(\n                output_dir=self.output_path,\n                num_train_epochs=self.num_epochs_for_training,\n                per_device_train_batch_size=self.batch_size_for_training,\n                per_device_eval_batch_size=self.batch_size_for_training,\n                warmup_ratio=0.0,\n                weight_decay=0.01,\n                logging_steps=logging_steps,\n                logging_dir=f\"{self.output_path}/tensorboard\",\n                eval_steps=eval_steps,\n                evaluation_strategy=\"steps\",\n                do_train=True,\n                do_eval=True,\n                save_steps=eval_steps,\n                save_total_limit=2,\n                load_best_model_at_end=True,\n            )\n            # build the trainer\n            self.trainer = Trainer(\n                model=self.model,\n                args=self.training_args,\n                train_dataset=self.training_data,\n                eval_dataset=self.validation_data,\n                compute_metrics=self.compute_metrics,\n                tokenizer=self.tokenizer._tokenizer,\n            )\n\n    def train(self, resume_from_checkpoint: Optional[Union[bool, str]] = None):\n\"\"\"Start training the BERT model.\"\"\"\n        if self.eval_mode:\n            raise RuntimeError(\"Training cannot be started in `eval` mode.\")\n        self.trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n    def eval(self):\n\"\"\"To eval mode.\"\"\"\n        print(\"The BERT model is set to eval mode for making predictions.\")\n        self.model.eval()\n        # TODO: to implement multi-gpus for inference\n        self.device = self.get_device(device_num=0)\n        self.model.to(self.device)\n        self.softmax = torch.nn.Softmax(dim=1).to(self.device)\n\n    def predict(self, sent_pairs: List[Tuple[str, str]]):\nr\"\"\"Run prediction pipeline for synonym classification.\n\n        Return the `softmax` probailities of predicting pairs as synonyms (`index=1`).\n        \"\"\"\n        inputs = self.process_inputs(sent_pairs)\n        with torch.no_grad():\n            return self.softmax(self.model(**inputs).logits)[:, 1]\n\n    def load_dataset(self, data: List[Tuple[str, str, int]], split: str) -&gt; Dataset:\nr\"\"\"Load the list of `(annotation1, annotation2, label)` samples into a `datasets.Dataset`.\"\"\"\n\n        def iterate():\n            for sample in data:\n                yield {\"annotation1\": sample[0], \"annotation2\": sample[1], \"labels\": sample[2]}\n\n        dataset = Dataset.from_generator(iterate)\n        # NOTE: no padding here because the Trainer class supports dynamic padding\n        dataset = dataset.map(\n            lambda examples: self.tokenizer._tokenizer(\n                examples[\"annotation1\"], examples[\"annotation2\"], max_length=self.max_length_for_input, truncation=True\n            ),\n            batched=True,\n            desc=f\"Load {split} data with batch size 1000:\",\n        )\n        return dataset\n\n    def process_inputs(self, sent_pairs: List[Tuple[str, str]]):\nr\"\"\"Process input sentence pairs for the BERT model.\n\n        Transform the sentences into BERT input embeddings and load them into the device.\n        This function is called only when the BERT model is about to make predictions (`eval` mode).\n        \"\"\"\n        return self.tokenizer._tokenizer(\n            sent_pairs,\n            return_tensors=\"pt\",\n            max_length=self.max_length_for_input,\n            padding=True,\n            truncation=True,\n        ).to(self.device)\n\n    @staticmethod\n    def compute_metrics(pred):\n\"\"\"Add more evaluation metrics into the training log.\"\"\"\n        # TODO: currently only accuracy is added, will expect more in the future if needed\n        labels = pred.label_ids\n        preds = pred.predictions.argmax(-1)\n        acc = accuracy_score(labels, preds)\n        return {\"accuracy\": acc}\n\n    @staticmethod\n    def get_device(device_num: int = 0):\n\"\"\"Get a device (GPU or CPU) for the torch model\"\"\"\n        # If there's a GPU available...\n        if torch.cuda.is_available():\n            # Tell PyTorch to use the GPU.\n            device = torch.device(f\"cuda:{device_num}\")\n            print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n            print(\"We will use the GPU:\", torch.cuda.get_device_name(device_num))\n        # If not...\n        else:\n            print(\"No GPU available, using the CPU instead.\")\n            device = torch.device(\"cpu\")\n        return device\n\n    @staticmethod\n    def set_seed(seed_val: int = 888):\n\"\"\"Set random seed for reproducible results.\"\"\"\n        random.seed(seed_val)\n        np.random.seed(seed_val)\n        torch.manual_seed(seed_val)\n        torch.cuda.manual_seed_all(seed_val)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.train","title":"<code>train(resume_from_checkpoint=None)</code>","text":"<p>Start training the BERT model.</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>def train(self, resume_from_checkpoint: Optional[Union[bool, str]] = None):\n\"\"\"Start training the BERT model.\"\"\"\n    if self.eval_mode:\n        raise RuntimeError(\"Training cannot be started in `eval` mode.\")\n    self.trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.eval","title":"<code>eval()</code>","text":"<p>To eval mode.</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>def eval(self):\n\"\"\"To eval mode.\"\"\"\n    print(\"The BERT model is set to eval mode for making predictions.\")\n    self.model.eval()\n    # TODO: to implement multi-gpus for inference\n    self.device = self.get_device(device_num=0)\n    self.model.to(self.device)\n    self.softmax = torch.nn.Softmax(dim=1).to(self.device)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.predict","title":"<code>predict(sent_pairs)</code>","text":"<p>Run prediction pipeline for synonym classification.</p> <p>Return the <code>softmax</code> probailities of predicting pairs as synonyms (<code>index=1</code>).</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>def predict(self, sent_pairs: List[Tuple[str, str]]):\nr\"\"\"Run prediction pipeline for synonym classification.\n\n    Return the `softmax` probailities of predicting pairs as synonyms (`index=1`).\n    \"\"\"\n    inputs = self.process_inputs(sent_pairs)\n    with torch.no_grad():\n        return self.softmax(self.model(**inputs).logits)[:, 1]\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.load_dataset","title":"<code>load_dataset(data, split)</code>","text":"<p>Load the list of <code>(annotation1, annotation2, label)</code> samples into a <code>datasets.Dataset</code>.</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>def load_dataset(self, data: List[Tuple[str, str, int]], split: str) -&gt; Dataset:\nr\"\"\"Load the list of `(annotation1, annotation2, label)` samples into a `datasets.Dataset`.\"\"\"\n\n    def iterate():\n        for sample in data:\n            yield {\"annotation1\": sample[0], \"annotation2\": sample[1], \"labels\": sample[2]}\n\n    dataset = Dataset.from_generator(iterate)\n    # NOTE: no padding here because the Trainer class supports dynamic padding\n    dataset = dataset.map(\n        lambda examples: self.tokenizer._tokenizer(\n            examples[\"annotation1\"], examples[\"annotation2\"], max_length=self.max_length_for_input, truncation=True\n        ),\n        batched=True,\n        desc=f\"Load {split} data with batch size 1000:\",\n    )\n    return dataset\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.process_inputs","title":"<code>process_inputs(sent_pairs)</code>","text":"<p>Process input sentence pairs for the BERT model.</p> <p>Transform the sentences into BERT input embeddings and load them into the device. This function is called only when the BERT model is about to make predictions (<code>eval</code> mode).</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>def process_inputs(self, sent_pairs: List[Tuple[str, str]]):\nr\"\"\"Process input sentence pairs for the BERT model.\n\n    Transform the sentences into BERT input embeddings and load them into the device.\n    This function is called only when the BERT model is about to make predictions (`eval` mode).\n    \"\"\"\n    return self.tokenizer._tokenizer(\n        sent_pairs,\n        return_tensors=\"pt\",\n        max_length=self.max_length_for_input,\n        padding=True,\n        truncation=True,\n    ).to(self.device)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.compute_metrics","title":"<code>compute_metrics(pred)</code>  <code>staticmethod</code>","text":"<p>Add more evaluation metrics into the training log.</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>@staticmethod\ndef compute_metrics(pred):\n\"\"\"Add more evaluation metrics into the training log.\"\"\"\n    # TODO: currently only accuracy is added, will expect more in the future if needed\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc}\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.get_device","title":"<code>get_device(device_num=0)</code>  <code>staticmethod</code>","text":"<p>Get a device (GPU or CPU) for the torch model</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>@staticmethod\ndef get_device(device_num: int = 0):\n\"\"\"Get a device (GPU or CPU) for the torch model\"\"\"\n    # If there's a GPU available...\n    if torch.cuda.is_available():\n        # Tell PyTorch to use the GPU.\n        device = torch.device(f\"cuda:{device_num}\")\n        print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n        print(\"We will use the GPU:\", torch.cuda.get_device_name(device_num))\n    # If not...\n    else:\n        print(\"No GPU available, using the CPU instead.\")\n        device = torch.device(\"cpu\")\n    return device\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.bert_classifier.BERTSynonymClassifier.set_seed","title":"<code>set_seed(seed_val=888)</code>  <code>staticmethod</code>","text":"<p>Set random seed for reproducible results.</p> Source code in <code>deeponto/align/bertmap/bert_classifier.py</code> <pre><code>@staticmethod\ndef set_seed(seed_val: int = 888):\n\"\"\"Set random seed for reproducible results.\"\"\"\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_prediction.MappingPredictor","title":"<code>MappingPredictor</code>","text":"<p>Class for the mapping prediction module of \\(\\textsf{BERTMap}\\) and \\(\\textsf{BERTMapLt}\\) models.</p> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>Tokenizer</code> <p>The tokenizer used for constructing the inverted annotation index and candidate selection.</p> <code>src_annotation_index</code> <code>dict</code> <p>A dictionary that stores the <code>(class_iri, class_annotations)</code> pairs from <code>src_onto</code> according to <code>annotation_property_iris</code>.</p> <code>tgt_annotation_index</code> <code>dict</code> <p>A dictionary that stores the <code>(class_iri, class_annotations)</code> pairs from <code>tgt_onto</code> according to <code>annotation_property_iris</code>.</p> <code>tgt_inverted_annotation_index</code> <code>InvertedIndex</code> <p>The inverted index built from <code>tgt_annotation_index</code> used for target class candidate selection.</p> <code>bert_synonym_classifier</code> <code>BERTSynonymClassifier</code> <p>The BERT synonym classifier fine-tuned on text semantics corpora.</p> <code>num_raw_candidates</code> <code>int</code> <p>The maximum number of selected target class candidates for a source class.</p> <code>num_best_predictions</code> <code>int</code> <p>The maximum number of best scored mappings presevred for a source class.</p> <code>batch_size_for_prediction</code> <code>int</code> <p>The batch size of class annotation pairs for computing synonym scores.</p> Source code in <code>deeponto/align/bertmap/mapping_prediction.py</code> <pre><code>class MappingPredictor:\nr\"\"\"Class for the mapping prediction module of $\\textsf{BERTMap}$ and $\\textsf{BERTMapLt}$ models.\n\n    Attributes:\n        tokenizer (Tokenizer): The tokenizer used for constructing the inverted annotation index and candidate selection.\n        src_annotation_index (dict): A dictionary that stores the `(class_iri, class_annotations)` pairs from `src_onto` according to `annotation_property_iris`.\n        tgt_annotation_index (dict): A dictionary that stores the `(class_iri, class_annotations)` pairs from `tgt_onto` according to `annotation_property_iris`.\n        tgt_inverted_annotation_index (InvertedIndex): The inverted index built from `tgt_annotation_index` used for target class candidate selection.\n        bert_synonym_classifier (BERTSynonymClassifier, optional): The BERT synonym classifier fine-tuned on text semantics corpora.\n        num_raw_candidates (int): The maximum number of selected target class candidates for a source class.\n        num_best_predictions (int): The maximum number of best scored mappings presevred for a source class.\n        batch_size_for_prediction (int): The batch size of class annotation pairs for computing synonym scores.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_path: str,\n        tokenizer_path: str,\n        src_annotation_index: dict,\n        tgt_annotation_index: dict,\n        bert_synonym_classifier: Optional[BERTSynonymClassifier],\n        num_raw_candidates: Optional[int],\n        num_best_predictions: Optional[int],\n        batch_size_for_prediction: int,\n        logger: Logger,\n        enlighten_manager: enlighten.Manager,\n        enlighten_status: enlighten.StatusBar,\n    ):\n        self.logger = logger\n        self.enlighten_manager = enlighten_manager\n        self.enlighten_status = enlighten_status\n\n        self.tokenizer = Tokenizer.from_pretrained(tokenizer_path)\n\n        self.logger.info(\"Build inverted annotation index for candidate selection.\")\n        self.src_annotation_index = src_annotation_index\n        self.tgt_annotation_index = tgt_annotation_index\n        self.tgt_inverted_annotation_index = Ontology.build_inverted_annotation_index(\n            tgt_annotation_index, self.tokenizer\n        )\n        # the fundamental judgement for whether bertmap or bertmaplt is loaded\n        self.bert_synonym_classifier = bert_synonym_classifier\n        self.num_raw_candidates = num_raw_candidates\n        self.num_best_predictions = num_best_predictions\n        self.batch_size_for_prediction = batch_size_for_prediction\n        self.output_path = output_path\n\n        self.init_class_mapping = lambda head, tail, score: EntityMapping(head, tail, \"&lt;EquivalentTo&gt;\", score)\n\n    def bert_mapping_score(\n        self,\n        src_class_annotations: Set[str],\n        tgt_class_annotations: Set[str],\n    ):\nr\"\"\"$\\textsf{BERTMap}$'s main mapping score module which utilises the fine-tuned BERT synonym\n        classifier.\n\n        Compute the **synonym score** for each pair of src-tgt class annotations, and return\n        the **average** score as the mapping score. Apply string matching before applying the\n        BERT module to filter easy mappings (with scores $1.0$).\n        \"\"\"\n        # apply string matching before applying the bert module\n        prelim_score = self.edit_similarity_mapping_score(\n            src_class_annotations,\n            tgt_class_annotations,\n            string_match_only=True,\n        )\n        if prelim_score == 1.0:\n            return prelim_score\n        # apply BERT classifier and define mapping score := Average(SynonymScores)\n        class_annotation_pairs = list(itertools.product(src_class_annotations, tgt_class_annotations))\n        synonym_scores = self.bert_synonym_classifier.predict(class_annotation_pairs)\n        # only one element tensor is able to be extracted as a scalar by .item()\n        return float(torch.mean(synonym_scores).item())\n\n    @staticmethod\n    def edit_similarity_mapping_score(\n        src_class_annotations: Set[str],\n        tgt_class_annotations: Set[str],\n        string_match_only: bool = False,\n    ):\nr\"\"\"$\\textsf{BERTMap}$'s string match module and $\\textsf{BERTMapLt}$'s mapping prediction function.\n\n        Compute the **normalised edit similarity** `(1 - normalised edit distance)` for each pair\n        of src-tgt class annotations, and return the **maximum** score as the mapping score.\n        \"\"\"\n        # edge case when src and tgt classes have an exact match of annotation\n        if len(src_class_annotations.intersection(tgt_class_annotations)) &gt; 0:\n            return 1.0\n        # a shortcut to save time for $\\textsf{BERTMap}$\n        if string_match_only:\n            return 0.0\n        annotation_pairs = itertools.product(src_class_annotations, tgt_class_annotations)\n        sim_scores = [levenshtein.normalized_similarity(src, tgt) for src, tgt in annotation_pairs]\n        return max(sim_scores)\n\n    def mapping_prediction_for_src_class(self, src_class_iri: str) -&gt; List[EntityMapping]:\nr\"\"\"Predict $N$ best scored mappings for a source ontology class, where\n        $N$ is specified in `self.num_best_predictions`.\n\n        1. Apply the **string matching** module to compute \"easy\" mappings.\n        2. Return the mappings if found any, or if there is no BERT synonym classifier\n        as in $\\textsf{BERTMapLt}$.\n        3. If using the BERT synonym classifier module:\n\n            - Generate batches for class annotation pairs. Each batch contains the combinations of the\n            source class annotations and $M$ target candidate classes' annotations. $M$ is determined\n            by `batch_size_for_prediction`, i.e., stop adding annotations of a target class candidate into\n            the current batch if this operation will cause the size of current batch to exceed the limit.\n            - Compute the synonym scores for each batch and aggregate them into mapping scores; preserve\n            $N$ best scored candidates and update them in the next batch. By this dynamic process, we eventually\n            get $N$ best scored mappings for a source ontology class.\n        \"\"\"\n\n        src_class_annotations = self.src_annotation_index[src_class_iri]\n        # previously wrongly put tokenizer again !!!\n        tgt_class_candidates = self.tgt_inverted_annotation_index.idf_select(\n            list(src_class_annotations), pool_size=self.num_raw_candidates\n        )  # [(tgt_class_iri, idf_score)]\n        best_scored_mappings = []\n\n        # for string matching: save time if already found string-matched candidates\n        def string_match():\n\"\"\"Compute string-matched mappings.\"\"\"\n            string_matched_mappings = []\n            for tgt_candidate_iri, _ in tgt_class_candidates:\n                tgt_candidate_annotations = self.tgt_annotation_index[tgt_candidate_iri]\n                prelim_score = self.edit_similarity_mapping_score(\n                    src_class_annotations,\n                    tgt_candidate_annotations,\n                    string_match_only=True,\n                )\n                if prelim_score &gt; 0.0:\n                    # if src_class_annotations.intersection(tgt_candidate_annotations):\n                    string_matched_mappings.append(\n                        self.init_class_mapping(src_class_iri, tgt_candidate_iri, prelim_score)\n                    )\n\n            return string_matched_mappings\n\n        best_scored_mappings += string_match()\n        # return string-matched mappings if found or if there is no bert module (bertmaplt)\n        if best_scored_mappings or not self.bert_synonym_classifier:\n            self.logger.info(f\"The best scored class mappings for {src_class_iri} are\\n{best_scored_mappings}\")\n            return best_scored_mappings\n\n        def generate_batched_annotations(batch_size: int):\n\"\"\"Generate batches of class annotations for the input source class and its\n            target candidates.\n            \"\"\"\n            batches = []\n            # the `nums`` parameter determines how the annotations are grouped\n            current_batch = CfgNode({\"annotations\": [], \"nums\": []})\n            for i, (tgt_candidate_iri, _) in enumerate(tgt_class_candidates):\n                tgt_candidate_annotations = self.tgt_annotation_index[tgt_candidate_iri]\n                annotation_pairs = list(itertools.product(src_class_annotations, tgt_candidate_annotations))\n                current_batch.annotations += annotation_pairs\n                num_annotation_pairs = len(annotation_pairs)\n                current_batch.nums.append(num_annotation_pairs)\n                # collect when the batch is full or for the last target class candidate\n                if sum(current_batch.nums) &gt; batch_size or i == len(tgt_class_candidates) - 1:\n                    batches.append(current_batch)\n                    current_batch = CfgNode({\"annotations\": [], \"nums\": []})\n            return batches\n\n        def bert_match():\n\"\"\"Compute mappings with fine-tuned BERT synonym classifier.\"\"\"\n            bert_matched_mappings = []\n            class_annotation_batches = generate_batched_annotations(self.batch_size_for_prediction)\n            batch_base_candidate_idx = (\n                0  # after each batch, the base index will be increased by # of covered target candidates\n            )\n            device = self.bert_synonym_classifier.device\n\n            # intialize N prediction scores and N corresponding indices w.r.t `tgt_class_candidates`\n            final_best_scores = torch.tensor([-1] * self.num_best_predictions).to(device)\n            final_best_idxs = torch.tensor([-1] * self.num_best_predictions).to(device)\n\n            for annotation_batch in class_annotation_batches:\n\n                synonym_scores = self.bert_synonym_classifier.predict(annotation_batch.annotations)\n                # aggregating to mappings cores\n                grouped_synonym_scores = torch.split(\n                    synonym_scores,\n                    split_size_or_sections=annotation_batch.nums,\n                )\n                mapping_scores = torch.stack([torch.mean(chunk) for chunk in grouped_synonym_scores])\n                assert len(mapping_scores) == len(annotation_batch.nums)\n\n                # preserve N best scored mappings\n                # scale N in case there are less than N tgt candidates in this batch\n                N = min(len(mapping_scores), self.num_best_predictions)\n                batch_best_scores, batch_best_idxs = torch.topk(mapping_scores, k=N)\n                batch_best_idxs += batch_base_candidate_idx\n\n                # we do the substitution for every batch to prevent from memory overflow\n                final_best_scores, _idxs = torch.topk(\n                    torch.cat([batch_best_scores, final_best_scores]),\n                    k=self.num_best_predictions,\n                )\n                final_best_idxs = torch.cat([batch_best_idxs, final_best_idxs])[_idxs]\n\n                # update the index for target candidate classes\n                batch_base_candidate_idx += len(annotation_batch.nums)\n\n            for candidate_idx, mapping_score in zip(final_best_idxs, final_best_scores):\n                # ignore intial values (-1.0) for dummy mappings\n                # the threshold 0.9 is for mapping extension\n                if mapping_score.item() &gt;= 0.9:\n                    tgt_candidate_iri = tgt_class_candidates[candidate_idx.item()][0]\n                    bert_matched_mappings.append(\n                        self.init_class_mapping(\n                            src_class_iri,\n                            tgt_candidate_iri,\n                            mapping_score.item(),\n                        )\n                    )\n\n            assert len(bert_matched_mappings) &lt;= self.num_best_predictions\n            self.logger.info(f\"The best scored class mappings for {src_class_iri} are\\n{bert_matched_mappings}\")\n            return bert_matched_mappings\n\n        return bert_match()\n\n    def mapping_prediction(self):\nr\"\"\"Apply global matching for each class in the source ontology.\n\n        See [`mapping_prediction_for_src_class`][deeponto.align.bertmap.mapping_prediction.MappingPredictor.mapping_prediction_for_src_class].\n\n        If this process is accidentally stopped, it can be resumed from already saved predictions. The progress\n        bar keeps track of the number of source ontology classes that have been matched.\n        \"\"\"\n        self.logger.info(\"Start global matching for each class in the source ontology.\")\n\n        match_dir = os.path.join(self.output_path, \"match\")\n        try:\n            mapping_index = FileUtils.load_file(os.path.join(match_dir, \"raw_mappings.json\"))\n            self.logger.info(\"Load the existing mapping prediction file.\")\n        except:\n            mapping_index = dict()\n            FileUtils.create_path(match_dir)\n\n        progress_bar = self.enlighten_manager.counter(\n            total=len(self.src_annotation_index), desc=\"Mapping Prediction\", unit=\"per src class\"\n        )\n        self.enlighten_status.update(demo=\"Mapping Prediction\")\n\n        for i, src_class_iri in enumerate(self.src_annotation_index.keys()):\n            if src_class_iri in mapping_index.keys():\n                self.logger.info(f\"[Class {i}] Skip matching {src_class_iri} as already computed.\")\n                progress_bar.update()\n                continue\n            mappings = self.mapping_prediction_for_src_class(src_class_iri)\n            mapping_index[src_class_iri] = [m.to_tuple(with_score=True) for m in mappings]\n\n            if i % 100 == 0 or i == len(self.src_annotation_index) - 1:\n                FileUtils.save_file(mapping_index, os.path.join(match_dir, \"raw_mappings.json\"))\n                # also save a .tsv version\n                mapping_in_tuples = list(itertools.chain.from_iterable(mapping_index.values()))\n                mapping_df = pd.DataFrame(mapping_in_tuples, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n                mapping_df.to_csv(os.path.join(match_dir, \"raw_mappings.tsv\"), sep=\"\\t\", index=False)\n                self.logger.info(\"Save currently computed mappings to prevent undesirable loss.\")\n\n            progress_bar.update()\n\n        self.logger.info(\"Finished mapping prediction for each class in the source ontology.\")\n        progress_bar.close()\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_prediction.MappingPredictor.bert_mapping_score","title":"<code>bert_mapping_score(src_class_annotations, tgt_class_annotations)</code>","text":"<p>\\(\\textsf{BERTMap}\\)'s main mapping score module which utilises the fine-tuned BERT synonym classifier.</p> <p>Compute the synonym score for each pair of src-tgt class annotations, and return the average score as the mapping score. Apply string matching before applying the BERT module to filter easy mappings (with scores \\(1.0\\)).</p> Source code in <code>deeponto/align/bertmap/mapping_prediction.py</code> <pre><code>def bert_mapping_score(\n    self,\n    src_class_annotations: Set[str],\n    tgt_class_annotations: Set[str],\n):\nr\"\"\"$\\textsf{BERTMap}$'s main mapping score module which utilises the fine-tuned BERT synonym\n    classifier.\n\n    Compute the **synonym score** for each pair of src-tgt class annotations, and return\n    the **average** score as the mapping score. Apply string matching before applying the\n    BERT module to filter easy mappings (with scores $1.0$).\n    \"\"\"\n    # apply string matching before applying the bert module\n    prelim_score = self.edit_similarity_mapping_score(\n        src_class_annotations,\n        tgt_class_annotations,\n        string_match_only=True,\n    )\n    if prelim_score == 1.0:\n        return prelim_score\n    # apply BERT classifier and define mapping score := Average(SynonymScores)\n    class_annotation_pairs = list(itertools.product(src_class_annotations, tgt_class_annotations))\n    synonym_scores = self.bert_synonym_classifier.predict(class_annotation_pairs)\n    # only one element tensor is able to be extracted as a scalar by .item()\n    return float(torch.mean(synonym_scores).item())\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_prediction.MappingPredictor.edit_similarity_mapping_score","title":"<code>edit_similarity_mapping_score(src_class_annotations, tgt_class_annotations, string_match_only=False)</code>  <code>staticmethod</code>","text":"<p>\\(\\textsf{BERTMap}\\)'s string match module and \\(\\textsf{BERTMapLt}\\)'s mapping prediction function.</p> <p>Compute the normalised edit similarity <code>(1 - normalised edit distance)</code> for each pair of src-tgt class annotations, and return the maximum score as the mapping score.</p> Source code in <code>deeponto/align/bertmap/mapping_prediction.py</code> <pre><code>@staticmethod\ndef edit_similarity_mapping_score(\n    src_class_annotations: Set[str],\n    tgt_class_annotations: Set[str],\n    string_match_only: bool = False,\n):\nr\"\"\"$\\textsf{BERTMap}$'s string match module and $\\textsf{BERTMapLt}$'s mapping prediction function.\n\n    Compute the **normalised edit similarity** `(1 - normalised edit distance)` for each pair\n    of src-tgt class annotations, and return the **maximum** score as the mapping score.\n    \"\"\"\n    # edge case when src and tgt classes have an exact match of annotation\n    if len(src_class_annotations.intersection(tgt_class_annotations)) &gt; 0:\n        return 1.0\n    # a shortcut to save time for $\\textsf{BERTMap}$\n    if string_match_only:\n        return 0.0\n    annotation_pairs = itertools.product(src_class_annotations, tgt_class_annotations)\n    sim_scores = [levenshtein.normalized_similarity(src, tgt) for src, tgt in annotation_pairs]\n    return max(sim_scores)\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_prediction.MappingPredictor.mapping_prediction_for_src_class","title":"<code>mapping_prediction_for_src_class(src_class_iri)</code>","text":"<p>Predict \\(N\\) best scored mappings for a source ontology class, where \\(N\\) is specified in <code>self.num_best_predictions</code>.</p> <ol> <li>Apply the string matching module to compute \"easy\" mappings.</li> <li>Return the mappings if found any, or if there is no BERT synonym classifier as in \\(\\textsf{BERTMapLt}\\).</li> <li> <p>If using the BERT synonym classifier module:</p> <ul> <li>Generate batches for class annotation pairs. Each batch contains the combinations of the source class annotations and \\(M\\) target candidate classes' annotations. \\(M\\) is determined by <code>batch_size_for_prediction</code>, i.e., stop adding annotations of a target class candidate into the current batch if this operation will cause the size of current batch to exceed the limit.</li> <li>Compute the synonym scores for each batch and aggregate them into mapping scores; preserve \\(N\\) best scored candidates and update them in the next batch. By this dynamic process, we eventually get \\(N\\) best scored mappings for a source ontology class.</li> </ul> </li> </ol> Source code in <code>deeponto/align/bertmap/mapping_prediction.py</code> <pre><code>def mapping_prediction_for_src_class(self, src_class_iri: str) -&gt; List[EntityMapping]:\nr\"\"\"Predict $N$ best scored mappings for a source ontology class, where\n    $N$ is specified in `self.num_best_predictions`.\n\n    1. Apply the **string matching** module to compute \"easy\" mappings.\n    2. Return the mappings if found any, or if there is no BERT synonym classifier\n    as in $\\textsf{BERTMapLt}$.\n    3. If using the BERT synonym classifier module:\n\n        - Generate batches for class annotation pairs. Each batch contains the combinations of the\n        source class annotations and $M$ target candidate classes' annotations. $M$ is determined\n        by `batch_size_for_prediction`, i.e., stop adding annotations of a target class candidate into\n        the current batch if this operation will cause the size of current batch to exceed the limit.\n        - Compute the synonym scores for each batch and aggregate them into mapping scores; preserve\n        $N$ best scored candidates and update them in the next batch. By this dynamic process, we eventually\n        get $N$ best scored mappings for a source ontology class.\n    \"\"\"\n\n    src_class_annotations = self.src_annotation_index[src_class_iri]\n    # previously wrongly put tokenizer again !!!\n    tgt_class_candidates = self.tgt_inverted_annotation_index.idf_select(\n        list(src_class_annotations), pool_size=self.num_raw_candidates\n    )  # [(tgt_class_iri, idf_score)]\n    best_scored_mappings = []\n\n    # for string matching: save time if already found string-matched candidates\n    def string_match():\n\"\"\"Compute string-matched mappings.\"\"\"\n        string_matched_mappings = []\n        for tgt_candidate_iri, _ in tgt_class_candidates:\n            tgt_candidate_annotations = self.tgt_annotation_index[tgt_candidate_iri]\n            prelim_score = self.edit_similarity_mapping_score(\n                src_class_annotations,\n                tgt_candidate_annotations,\n                string_match_only=True,\n            )\n            if prelim_score &gt; 0.0:\n                # if src_class_annotations.intersection(tgt_candidate_annotations):\n                string_matched_mappings.append(\n                    self.init_class_mapping(src_class_iri, tgt_candidate_iri, prelim_score)\n                )\n\n        return string_matched_mappings\n\n    best_scored_mappings += string_match()\n    # return string-matched mappings if found or if there is no bert module (bertmaplt)\n    if best_scored_mappings or not self.bert_synonym_classifier:\n        self.logger.info(f\"The best scored class mappings for {src_class_iri} are\\n{best_scored_mappings}\")\n        return best_scored_mappings\n\n    def generate_batched_annotations(batch_size: int):\n\"\"\"Generate batches of class annotations for the input source class and its\n        target candidates.\n        \"\"\"\n        batches = []\n        # the `nums`` parameter determines how the annotations are grouped\n        current_batch = CfgNode({\"annotations\": [], \"nums\": []})\n        for i, (tgt_candidate_iri, _) in enumerate(tgt_class_candidates):\n            tgt_candidate_annotations = self.tgt_annotation_index[tgt_candidate_iri]\n            annotation_pairs = list(itertools.product(src_class_annotations, tgt_candidate_annotations))\n            current_batch.annotations += annotation_pairs\n            num_annotation_pairs = len(annotation_pairs)\n            current_batch.nums.append(num_annotation_pairs)\n            # collect when the batch is full or for the last target class candidate\n            if sum(current_batch.nums) &gt; batch_size or i == len(tgt_class_candidates) - 1:\n                batches.append(current_batch)\n                current_batch = CfgNode({\"annotations\": [], \"nums\": []})\n        return batches\n\n    def bert_match():\n\"\"\"Compute mappings with fine-tuned BERT synonym classifier.\"\"\"\n        bert_matched_mappings = []\n        class_annotation_batches = generate_batched_annotations(self.batch_size_for_prediction)\n        batch_base_candidate_idx = (\n            0  # after each batch, the base index will be increased by # of covered target candidates\n        )\n        device = self.bert_synonym_classifier.device\n\n        # intialize N prediction scores and N corresponding indices w.r.t `tgt_class_candidates`\n        final_best_scores = torch.tensor([-1] * self.num_best_predictions).to(device)\n        final_best_idxs = torch.tensor([-1] * self.num_best_predictions).to(device)\n\n        for annotation_batch in class_annotation_batches:\n\n            synonym_scores = self.bert_synonym_classifier.predict(annotation_batch.annotations)\n            # aggregating to mappings cores\n            grouped_synonym_scores = torch.split(\n                synonym_scores,\n                split_size_or_sections=annotation_batch.nums,\n            )\n            mapping_scores = torch.stack([torch.mean(chunk) for chunk in grouped_synonym_scores])\n            assert len(mapping_scores) == len(annotation_batch.nums)\n\n            # preserve N best scored mappings\n            # scale N in case there are less than N tgt candidates in this batch\n            N = min(len(mapping_scores), self.num_best_predictions)\n            batch_best_scores, batch_best_idxs = torch.topk(mapping_scores, k=N)\n            batch_best_idxs += batch_base_candidate_idx\n\n            # we do the substitution for every batch to prevent from memory overflow\n            final_best_scores, _idxs = torch.topk(\n                torch.cat([batch_best_scores, final_best_scores]),\n                k=self.num_best_predictions,\n            )\n            final_best_idxs = torch.cat([batch_best_idxs, final_best_idxs])[_idxs]\n\n            # update the index for target candidate classes\n            batch_base_candidate_idx += len(annotation_batch.nums)\n\n        for candidate_idx, mapping_score in zip(final_best_idxs, final_best_scores):\n            # ignore intial values (-1.0) for dummy mappings\n            # the threshold 0.9 is for mapping extension\n            if mapping_score.item() &gt;= 0.9:\n                tgt_candidate_iri = tgt_class_candidates[candidate_idx.item()][0]\n                bert_matched_mappings.append(\n                    self.init_class_mapping(\n                        src_class_iri,\n                        tgt_candidate_iri,\n                        mapping_score.item(),\n                    )\n                )\n\n        assert len(bert_matched_mappings) &lt;= self.num_best_predictions\n        self.logger.info(f\"The best scored class mappings for {src_class_iri} are\\n{bert_matched_mappings}\")\n        return bert_matched_mappings\n\n    return bert_match()\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_prediction.MappingPredictor.mapping_prediction","title":"<code>mapping_prediction()</code>","text":"<p>Apply global matching for each class in the source ontology.</p> <p>See <code>mapping_prediction_for_src_class</code>.</p> <p>If this process is accidentally stopped, it can be resumed from already saved predictions. The progress bar keeps track of the number of source ontology classes that have been matched.</p> Source code in <code>deeponto/align/bertmap/mapping_prediction.py</code> <pre><code>def mapping_prediction(self):\nr\"\"\"Apply global matching for each class in the source ontology.\n\n    See [`mapping_prediction_for_src_class`][deeponto.align.bertmap.mapping_prediction.MappingPredictor.mapping_prediction_for_src_class].\n\n    If this process is accidentally stopped, it can be resumed from already saved predictions. The progress\n    bar keeps track of the number of source ontology classes that have been matched.\n    \"\"\"\n    self.logger.info(\"Start global matching for each class in the source ontology.\")\n\n    match_dir = os.path.join(self.output_path, \"match\")\n    try:\n        mapping_index = FileUtils.load_file(os.path.join(match_dir, \"raw_mappings.json\"))\n        self.logger.info(\"Load the existing mapping prediction file.\")\n    except:\n        mapping_index = dict()\n        FileUtils.create_path(match_dir)\n\n    progress_bar = self.enlighten_manager.counter(\n        total=len(self.src_annotation_index), desc=\"Mapping Prediction\", unit=\"per src class\"\n    )\n    self.enlighten_status.update(demo=\"Mapping Prediction\")\n\n    for i, src_class_iri in enumerate(self.src_annotation_index.keys()):\n        if src_class_iri in mapping_index.keys():\n            self.logger.info(f\"[Class {i}] Skip matching {src_class_iri} as already computed.\")\n            progress_bar.update()\n            continue\n        mappings = self.mapping_prediction_for_src_class(src_class_iri)\n        mapping_index[src_class_iri] = [m.to_tuple(with_score=True) for m in mappings]\n\n        if i % 100 == 0 or i == len(self.src_annotation_index) - 1:\n            FileUtils.save_file(mapping_index, os.path.join(match_dir, \"raw_mappings.json\"))\n            # also save a .tsv version\n            mapping_in_tuples = list(itertools.chain.from_iterable(mapping_index.values()))\n            mapping_df = pd.DataFrame(mapping_in_tuples, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n            mapping_df.to_csv(os.path.join(match_dir, \"raw_mappings.tsv\"), sep=\"\\t\", index=False)\n            self.logger.info(\"Save currently computed mappings to prevent undesirable loss.\")\n\n        progress_bar.update()\n\n    self.logger.info(\"Finished mapping prediction for each class in the source ontology.\")\n    progress_bar.close()\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_refinement.MappingRefiner","title":"<code>MappingRefiner</code>","text":"<p>Class for the mapping refinement module of \\(\\textsf{BERTMap}\\).</p> <p>\\(\\textsf{BERTMapLt}\\) does not go through mapping refinement for its being \"light\". All the attributes of this class are supposed to be passed from <code>BERTMapPipeline</code>.</p> <p>Attributes:</p> Name Type Description <code>src_onto</code> <code>Ontology</code> <p>The source ontology to be matched.</p> <code>tgt_onto</code> <code>Ontology</code> <p>The target ontology to be matched.</p> <code>mapping_predictor</code> <code>MappingPredictor</code> <p>The mapping prediction module of BERTMap.</p> <code>mapping_extension_threshold</code> <code>float</code> <p>Mappings with scores \\(\\geq\\) this value will be considered in the iterative mapping extension process.</p> <code>raw_mappings</code> <code>List[EntityMapping]</code> <p>List of raw class mappings predicted in the global matching phase.</p> <code>mapping_score_dict</code> <code>dict</code> <p>A dynamic dictionary that keeps track of mappings (with scores) that have already been computed.</p> <code>mapping_filter_threshold</code> <code>float</code> <p>Mappings with scores \\(\\geq\\) this value will be preserved for the final mapping repairing.</p> Source code in <code>deeponto/align/bertmap/mapping_refinement.py</code> <pre><code>class MappingRefiner:\nr\"\"\"Class for the mapping refinement module of $\\textsf{BERTMap}$.\n\n    $\\textsf{BERTMapLt}$ does not go through mapping refinement for its being \"light\".\n    All the attributes of this class are supposed to be passed from `BERTMapPipeline`.\n\n    Attributes:\n        src_onto (Ontology): The source ontology to be matched.\n        tgt_onto (Ontology): The target ontology to be matched.\n        mapping_predictor (MappingPredictor): The mapping prediction module of BERTMap.\n        mapping_extension_threshold (float): Mappings with scores $\\geq$ this value will be considered in the iterative mapping extension process.\n        raw_mappings (List[EntityMapping]): List of **raw class mappings** predicted in the **global matching** phase.\n        mapping_score_dict (dict): A dynamic dictionary that keeps track of mappings (with scores) that have already been computed.\n        mapping_filter_threshold (float): Mappings with scores $\\geq$ this value will be preserved for the final mapping repairing. \n    \"\"\"\n    def __init__(\n        self,\n        output_path: str,\n        src_onto: Ontology,\n        tgt_onto: Ontology,\n        mapping_predictor: MappingPredictor,\n        mapping_extension_threshold: float,\n        mapping_filtered_threshold: float,\n        logger: Logger,\n        enlighten_manager: enlighten.Manager,\n        enlighten_status: enlighten.StatusBar\n    ):\n        self.output_path = output_path\n        self.logger = logger\n        self.enlighten_manager = enlighten_manager\n        self.enlighten_status = enlighten_status\n\n        self.src_onto = src_onto\n        self.tgt_onto = tgt_onto\n\n        # iterative mapping extension\n        self.mapping_predictor = mapping_predictor\n        self.mapping_extension_threshold = mapping_extension_threshold  # \\kappa\n        self.raw_mappings = EntityMapping.read_table_mappings(\n            os.path.join(self.output_path, \"match\", \"raw_mappings.tsv\"),\n            threshold=self.mapping_extension_threshold,\n            relation=\"&lt;EquivalentTo&gt;\",\n        )\n        # keep track of already scored mappings to prevent duplicated predictions\n        self.mapping_score_dict = dict()\n        for m in self.raw_mappings:\n            src_class_iri, tgt_class_iri, score = m.to_tuple(with_score=True)\n            self.mapping_score_dict[(src_class_iri, tgt_class_iri)] = score\n\n        # the threshold for final filtering the extended mappings\n        self.mapping_filtered_threshold = mapping_filtered_threshold  # \\lambda\n\n        # logmap mapping repair folder\n        self.logmap_repair_path = os.path.join(self.output_path, \"match\", \"logmap-repair\")\n\n        # paths for mapping extension and repair\n        self.extended_mapping_path = os.path.join(self.output_path, \"match\", \"extended_mappings.tsv\")\n        self.filtered_mapping_path = os.path.join(self.output_path, \"match\", \"filtered_mappings.tsv\")\n        self.repaired_mapping_path = os.path.join(self.output_path, \"match\", \"repaired_mappings.tsv\")\n\n    def mapping_extension(self, max_iter: int = 10):\nr\"\"\"Iterative mapping extension based on the locality principle.\n\n        For each class pair $(c, c')$ (scored in the global matching phase) with score \n        $\\geq \\kappa$, search for plausible mappings between the parents of $c$ and $c'$,\n        and between the children of $c$ and $c'$. This is an iterative process as the set \n        newly discovered mappings can act renew the frontier for searching. Terminate if\n        no new mappings with score $\\geq \\kappa$ can be found or the limit `max_iter` has \n        been reached. Note that $\\kappa$ is set to $0.9$ by default (can be altered\n        in the configuration file). The mapping extension progress bar keeps track of the \n        total number of extended mappings (including the previously predicted ones).\n\n        A further filtering will be performed by only preserving mappings with score $\\geq \\lambda$,\n        in the original BERTMap paper, $\\lambda$ is determined by the validation mappings, but\n        in practice $\\lambda$ is not a sensitive hyperparameter and validation mappings are often\n        not available. Therefore, we manually set $\\lambda$ to $0.9995$ by default (can be altered\n        in the configuration file). The mapping filtering progress bar keeps track of the \n        total number of filtered mappings (this bar is purely for logging purpose).\n\n        Args:\n            max_iter (int, optional): The maximum number of mapping extension iterations. Defaults to `10`.\n        \"\"\"\n\n        num_iter = 0\n        self.enlighten_status.update(demo=\"Mapping Extension\")\n        extension_progress_bar = self.enlighten_manager.counter(\n            desc=f\"Mapping Extension [Iteration #{num_iter}]\", unit=\"mapping\"\n        )\n        filtering_progress_bar = self.enlighten_manager.counter(\n            desc=f\"Mapping Filtering\", unit=\"mapping\"\n        )\n\n        if os.path.exists(self.extended_mapping_path) and os.path.exists(self.filtered_mapping_path):\n            self.logger.info(\n                f\"Found extended and filtered mapping files at {self.extended_mapping_path}\"\n                + f\" and {self.filtered_mapping_path}.\\nPlease check file integrity; if incomplete, \"\n                + \"delete them and re-run the program.\"\n            )\n\n            # for animation purposes\n            extension_progress_bar.desc = f\"Mapping Extension\"\n            for _ in EntityMapping.read_table_mappings(self.extended_mapping_path):\n                extension_progress_bar.update()\n\n            self.enlighten_status.update(demo=\"Mapping Filtering\")\n            for _ in EntityMapping.read_table_mappings(self.filtered_mapping_path):\n                filtering_progress_bar.update()\n\n            extension_progress_bar.close()\n            filtering_progress_bar.close()\n\n            return\n        # intialise the frontier, explored, final expansion sets with the raw mappings\n        # NOTE be careful of address pointers\n        frontier = [m.to_tuple() for m in self.raw_mappings]\n        expansion = [m.to_tuple(with_score=True) for m in self.raw_mappings]\n        # for animation purposes\n        for _ in range(len(expansion)):\n            extension_progress_bar.update()\n\n        self.logger.info(\n            f\"Start mapping extension for each class pair with score &gt;= {self.mapping_extension_threshold}.\"\n        )\n        while frontier and num_iter &lt; max_iter:\n            new_mappings = []\n            for src_class_iri, tgt_class_iri in frontier:\n                # one hop extension makes sure new mappings are really \"new\"\n                cur_new_mappings = self.one_hop_extend(src_class_iri, tgt_class_iri)\n                extension_progress_bar.update(len(cur_new_mappings))\n                new_mappings += cur_new_mappings\n            # add new mappings to the expansion set\n            expansion += new_mappings\n            # renew frontier with the newly discovered mappings\n            frontier = [(x, y) for x, y, _ in new_mappings]\n\n            self.logger.info(f\"Add {len(new_mappings)} mappings at iteration #{num_iter}.\")\n            num_iter += 1\n            extension_progress_bar.desc = f\"Mapping Extension [Iteration #{num_iter}]\"\n\n        num_extended = len(expansion) - len(self.raw_mappings)\n        self.logger.info(\n            f\"Finished iterative mapping extension with {num_extended} new mappings and in total {len(expansion)} extended mappings.\"\n        )\n\n        extended_mapping_df = pd.DataFrame(expansion, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n        extended_mapping_df.to_csv(self.extended_mapping_path, sep=\"\\t\", index=False)\n\n        self.enlighten_status.update(demo=\"Mapping Filtering\")\n\n        filtered_expansion = [\n            (src, tgt, score) for src, tgt, score in expansion if score &gt;= self.mapping_filtered_threshold\n        ]\n        self.logger.info(\n            f\"Filtered the extended mappings by a threshold of {self.mapping_filtered_threshold}.\"\n            + f\"There are {len(filtered_expansion)} mappings left for mapping repair.\"\n        )\n\n        for _ in range(len(filtered_expansion)):\n            filtering_progress_bar.update()\n\n        filtered_mapping_df = pd.DataFrame(filtered_expansion, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n        filtered_mapping_df.to_csv(self.filtered_mapping_path, sep=\"\\t\", index=False)\n\n        extension_progress_bar.close()\n        filtering_progress_bar.close()\n        return filtered_expansion\n\n    def one_hop_extend(self, src_class_iri: str, tgt_class_iri: str, pool_size: int = 200):\nr\"\"\"Extend mappings from a scored class pair $(c, c')$ by\n        searching from one-hop neighbors.\n\n        Search for plausible mappings between the parents of $c$ and $c'$,\n        and between the children of $c$ and $c'$. Mappings that are not\n        already computed (recorded in `self.mapping_score_dict`) and have\n        a score $\\geq `self.mapping_extension_threshold` will be returned as\n        **new** mappings.\n\n        Args:\n            src_class_iri (str): The IRI of the source ontology class $c$.\n            tgt_class_iri (str): The IRI of the target ontology class $c'$.\n            pool_size (int, optional): The maximum number of plausible mappings to be extended. Defaults to 200.\n\n        Returns:\n            (List[EntityMapping]): A list of one-hop extended mappings.\n        \"\"\"\n\n        src_class = self.src_onto.get_owl_object_from_iri(src_class_iri)\n        src_class_parent_iris = self.src_onto.reasoner.super_entities_of(src_class, direct=True)\n        src_class_children_iris = self.src_onto.reasoner.sub_entities_of(src_class, direct=True)\n\n        tgt_class = self.tgt_onto.get_owl_object_from_iri(tgt_class_iri)\n        tgt_class_parent_iris = self.tgt_onto.reasoner.super_entities_of(tgt_class, direct=True)\n        tgt_class_children_iris = self.tgt_onto.reasoner.sub_entities_of(tgt_class, direct=True)\n\n        # pair up parents and children, respectively; NOTE set() might not be necessary\n        parent_pairs = list(set(itertools.product(src_class_parent_iris, tgt_class_parent_iris)))\n        children_pairs = list(set(itertools.product(src_class_children_iris, tgt_class_children_iris)))\n\n        candidate_pairs = parent_pairs + children_pairs\n        # downsample if the number of candidates is too large\n        if len(candidate_pairs) &gt; pool_size:\n            candidate_pairs = random.sample(candidate_pairs, pool_size)\n\n        extended_mappings = []\n        for src_candidate_iri, tgt_candidate_iri in parent_pairs + children_pairs:\n\n            # if already computed meaning that it is not a new mapping\n            if (src_candidate_iri, tgt_candidate_iri) in self.mapping_score_dict:\n                continue\n\n            src_candidate_annotations = self.mapping_predictor.src_annotation_index[src_candidate_iri]\n            tgt_candidate_annotations = self.mapping_predictor.tgt_annotation_index[tgt_candidate_iri]\n            score = self.mapping_predictor.bert_mapping_score(src_candidate_annotations, tgt_candidate_annotations)\n            # add to already scored collection\n            self.mapping_score_dict[(src_candidate_iri, tgt_candidate_iri)] = score\n\n            # skip mappings with low scores\n            if score &lt; self.mapping_extension_threshold:\n                continue\n\n            extended_mappings.append((src_candidate_iri, tgt_candidate_iri, score))\n\n        self.logger.info(\n            f\"New mappings (in tuples) extended from {(src_class_iri, tgt_class_iri)} are:\\n\" + f\"{extended_mappings}\"\n        )\n\n        return extended_mappings\n\n    def mapping_repair(self):\n\"\"\"Repair the filtered mappings with LogMap's debugger.\n\n        !!! note\n\n            A sub-folder under `match` named `logmap-repair` contains LogMap-related intermediate files.\n        \"\"\"\n\n        # progress bar for animation purposes\n        self.enlighten_status.update(demo=\"Mapping Repairing\")\n        repair_progress_bar = self.enlighten_manager.counter(\n            desc=f\"Mapping Repairing\", unit=\"mapping\"\n        )\n\n        # skip repairing if already found the file\n        if os.path.exists(self.repaired_mapping_path):\n            self.logger.info(\n                f\"Found the repaired mapping file at {self.repaired_mapping_path}.\"\n                + \"\\nPlease check file integrity; if incomplete, \"\n                + \"delete it and re-run the program.\"\n            )\n            # update progress bar for animation purposes\n            for _ in EntityMapping.read_table_mappings(self.repaired_mapping_path):\n                repair_progress_bar.update()\n            repair_progress_bar.close()\n            return \n\n        # start mapping repair\n        self.logger.info(\"Repair the filtered mappings with LogMap debugger.\")\n        # formatting the filtered mappings\n        self.logmap_repair_formatting()\n\n        # run the LogMap repair module on the extended mappings\n        run_logmap_repair(\n            self.src_onto.owl_path,\n            self.tgt_onto.owl_path,\n            os.path.join(self.logmap_repair_path, f\"filtered_mappings_for_LogMap_repair.txt\"),\n            self.logmap_repair_path,\n        )\n\n        # create table mappings from LogMap repair outputs\n        with open(os.path.join(self.logmap_repair_path, \"mappings_repaired_with_LogMap.tsv\"), \"r\") as f:\n            lines = f.readlines()\n        with open(os.path.join(self.output_path, \"match\", \"repaired_mappings.tsv\"), \"w+\") as f:\n            f.write(\"SrcEntity\\tTgtEntity\\tScore\\n\")\n            for line in lines:\n                src_ent_iri, tgt_ent_iri, score = line.split(\"\\t\")\n                f.write(f\"{src_ent_iri}\\t{tgt_ent_iri}\\t{score}\")\n                repair_progress_bar.update()\n\n        self.logger.info(\"Mapping repair finished.\")\n        repair_progress_bar.close()\n\n    def logmap_repair_formatting(self):\n\"\"\"Transform the filtered mapping file into the LogMap format.\n\n        An auxiliary function of the mapping repair module which requires mappings\n        to be formatted as LogMap's input format.\n        \"\"\"\n        # read the filtered mapping file and convert to tuples\n        filtered_mappings = EntityMapping.read_table_mappings(self.filtered_mapping_path)\n        filtered_mappings_in_tuples = [m.to_tuple(with_score=True) for m in filtered_mappings]\n\n        # write the mappings into logmap format\n        lines = []\n        for src_class_iri, tgt_class_iri, score in filtered_mappings_in_tuples:\n            lines.append(f\"{src_class_iri}|{tgt_class_iri}|=|{score}|CLS\\n\")\n\n        # create a path to prevent error\n        FileUtils.create_path(self.logmap_repair_path)\n        formatted_file = os.path.join(self.logmap_repair_path, f\"filtered_mappings_for_LogMap_repair.txt\")\n        with open(formatted_file, \"w\") as f:\n            f.writelines(lines)\n\n        return lines\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_refinement.MappingRefiner.mapping_extension","title":"<code>mapping_extension(max_iter=10)</code>","text":"<p>Iterative mapping extension based on the locality principle.</p> <p>For each class pair \\((c, c')\\) (scored in the global matching phase) with score  \\(\\geq \\kappa\\), search for plausible mappings between the parents of \\(c\\) and \\(c'\\), and between the children of \\(c\\) and \\(c'\\). This is an iterative process as the set  newly discovered mappings can act renew the frontier for searching. Terminate if no new mappings with score \\(\\geq \\kappa\\) can be found or the limit <code>max_iter</code> has  been reached. Note that \\(\\kappa\\) is set to \\(0.9\\) by default (can be altered in the configuration file). The mapping extension progress bar keeps track of the  total number of extended mappings (including the previously predicted ones).</p> <p>A further filtering will be performed by only preserving mappings with score \\(\\geq \\lambda\\), in the original BERTMap paper, \\(\\lambda\\) is determined by the validation mappings, but in practice \\(\\lambda\\) is not a sensitive hyperparameter and validation mappings are often not available. Therefore, we manually set \\(\\lambda\\) to \\(0.9995\\) by default (can be altered in the configuration file). The mapping filtering progress bar keeps track of the  total number of filtered mappings (this bar is purely for logging purpose).</p> <p>Parameters:</p> Name Type Description Default <code>max_iter</code> <code>int</code> <p>The maximum number of mapping extension iterations. Defaults to <code>10</code>.</p> <code>10</code> Source code in <code>deeponto/align/bertmap/mapping_refinement.py</code> <pre><code>def mapping_extension(self, max_iter: int = 10):\nr\"\"\"Iterative mapping extension based on the locality principle.\n\n    For each class pair $(c, c')$ (scored in the global matching phase) with score \n    $\\geq \\kappa$, search for plausible mappings between the parents of $c$ and $c'$,\n    and between the children of $c$ and $c'$. This is an iterative process as the set \n    newly discovered mappings can act renew the frontier for searching. Terminate if\n    no new mappings with score $\\geq \\kappa$ can be found or the limit `max_iter` has \n    been reached. Note that $\\kappa$ is set to $0.9$ by default (can be altered\n    in the configuration file). The mapping extension progress bar keeps track of the \n    total number of extended mappings (including the previously predicted ones).\n\n    A further filtering will be performed by only preserving mappings with score $\\geq \\lambda$,\n    in the original BERTMap paper, $\\lambda$ is determined by the validation mappings, but\n    in practice $\\lambda$ is not a sensitive hyperparameter and validation mappings are often\n    not available. Therefore, we manually set $\\lambda$ to $0.9995$ by default (can be altered\n    in the configuration file). The mapping filtering progress bar keeps track of the \n    total number of filtered mappings (this bar is purely for logging purpose).\n\n    Args:\n        max_iter (int, optional): The maximum number of mapping extension iterations. Defaults to `10`.\n    \"\"\"\n\n    num_iter = 0\n    self.enlighten_status.update(demo=\"Mapping Extension\")\n    extension_progress_bar = self.enlighten_manager.counter(\n        desc=f\"Mapping Extension [Iteration #{num_iter}]\", unit=\"mapping\"\n    )\n    filtering_progress_bar = self.enlighten_manager.counter(\n        desc=f\"Mapping Filtering\", unit=\"mapping\"\n    )\n\n    if os.path.exists(self.extended_mapping_path) and os.path.exists(self.filtered_mapping_path):\n        self.logger.info(\n            f\"Found extended and filtered mapping files at {self.extended_mapping_path}\"\n            + f\" and {self.filtered_mapping_path}.\\nPlease check file integrity; if incomplete, \"\n            + \"delete them and re-run the program.\"\n        )\n\n        # for animation purposes\n        extension_progress_bar.desc = f\"Mapping Extension\"\n        for _ in EntityMapping.read_table_mappings(self.extended_mapping_path):\n            extension_progress_bar.update()\n\n        self.enlighten_status.update(demo=\"Mapping Filtering\")\n        for _ in EntityMapping.read_table_mappings(self.filtered_mapping_path):\n            filtering_progress_bar.update()\n\n        extension_progress_bar.close()\n        filtering_progress_bar.close()\n\n        return\n    # intialise the frontier, explored, final expansion sets with the raw mappings\n    # NOTE be careful of address pointers\n    frontier = [m.to_tuple() for m in self.raw_mappings]\n    expansion = [m.to_tuple(with_score=True) for m in self.raw_mappings]\n    # for animation purposes\n    for _ in range(len(expansion)):\n        extension_progress_bar.update()\n\n    self.logger.info(\n        f\"Start mapping extension for each class pair with score &gt;= {self.mapping_extension_threshold}.\"\n    )\n    while frontier and num_iter &lt; max_iter:\n        new_mappings = []\n        for src_class_iri, tgt_class_iri in frontier:\n            # one hop extension makes sure new mappings are really \"new\"\n            cur_new_mappings = self.one_hop_extend(src_class_iri, tgt_class_iri)\n            extension_progress_bar.update(len(cur_new_mappings))\n            new_mappings += cur_new_mappings\n        # add new mappings to the expansion set\n        expansion += new_mappings\n        # renew frontier with the newly discovered mappings\n        frontier = [(x, y) for x, y, _ in new_mappings]\n\n        self.logger.info(f\"Add {len(new_mappings)} mappings at iteration #{num_iter}.\")\n        num_iter += 1\n        extension_progress_bar.desc = f\"Mapping Extension [Iteration #{num_iter}]\"\n\n    num_extended = len(expansion) - len(self.raw_mappings)\n    self.logger.info(\n        f\"Finished iterative mapping extension with {num_extended} new mappings and in total {len(expansion)} extended mappings.\"\n    )\n\n    extended_mapping_df = pd.DataFrame(expansion, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n    extended_mapping_df.to_csv(self.extended_mapping_path, sep=\"\\t\", index=False)\n\n    self.enlighten_status.update(demo=\"Mapping Filtering\")\n\n    filtered_expansion = [\n        (src, tgt, score) for src, tgt, score in expansion if score &gt;= self.mapping_filtered_threshold\n    ]\n    self.logger.info(\n        f\"Filtered the extended mappings by a threshold of {self.mapping_filtered_threshold}.\"\n        + f\"There are {len(filtered_expansion)} mappings left for mapping repair.\"\n    )\n\n    for _ in range(len(filtered_expansion)):\n        filtering_progress_bar.update()\n\n    filtered_mapping_df = pd.DataFrame(filtered_expansion, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n    filtered_mapping_df.to_csv(self.filtered_mapping_path, sep=\"\\t\", index=False)\n\n    extension_progress_bar.close()\n    filtering_progress_bar.close()\n    return filtered_expansion\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_refinement.MappingRefiner.one_hop_extend","title":"<code>one_hop_extend(src_class_iri, tgt_class_iri, pool_size=200)</code>","text":"<p>Extend mappings from a scored class pair \\((c, c')\\) by searching from one-hop neighbors.</p> <p>Search for plausible mappings between the parents of \\(c\\) and \\(c'\\), and between the children of \\(c\\) and \\(c'\\). Mappings that are not already computed (recorded in <code>self.mapping_score_dict</code>) and have a score $\\geq <code>self.mapping_extension_threshold</code> will be returned as new mappings.</p> <p>Parameters:</p> Name Type Description Default <code>src_class_iri</code> <code>str</code> <p>The IRI of the source ontology class \\(c\\).</p> required <code>tgt_class_iri</code> <code>str</code> <p>The IRI of the target ontology class \\(c'\\).</p> required <code>pool_size</code> <code>int</code> <p>The maximum number of plausible mappings to be extended. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <code>List[EntityMapping]</code> <p>A list of one-hop extended mappings.</p> Source code in <code>deeponto/align/bertmap/mapping_refinement.py</code> <pre><code>def one_hop_extend(self, src_class_iri: str, tgt_class_iri: str, pool_size: int = 200):\nr\"\"\"Extend mappings from a scored class pair $(c, c')$ by\n    searching from one-hop neighbors.\n\n    Search for plausible mappings between the parents of $c$ and $c'$,\n    and between the children of $c$ and $c'$. Mappings that are not\n    already computed (recorded in `self.mapping_score_dict`) and have\n    a score $\\geq `self.mapping_extension_threshold` will be returned as\n    **new** mappings.\n\n    Args:\n        src_class_iri (str): The IRI of the source ontology class $c$.\n        tgt_class_iri (str): The IRI of the target ontology class $c'$.\n        pool_size (int, optional): The maximum number of plausible mappings to be extended. Defaults to 200.\n\n    Returns:\n        (List[EntityMapping]): A list of one-hop extended mappings.\n    \"\"\"\n\n    src_class = self.src_onto.get_owl_object_from_iri(src_class_iri)\n    src_class_parent_iris = self.src_onto.reasoner.super_entities_of(src_class, direct=True)\n    src_class_children_iris = self.src_onto.reasoner.sub_entities_of(src_class, direct=True)\n\n    tgt_class = self.tgt_onto.get_owl_object_from_iri(tgt_class_iri)\n    tgt_class_parent_iris = self.tgt_onto.reasoner.super_entities_of(tgt_class, direct=True)\n    tgt_class_children_iris = self.tgt_onto.reasoner.sub_entities_of(tgt_class, direct=True)\n\n    # pair up parents and children, respectively; NOTE set() might not be necessary\n    parent_pairs = list(set(itertools.product(src_class_parent_iris, tgt_class_parent_iris)))\n    children_pairs = list(set(itertools.product(src_class_children_iris, tgt_class_children_iris)))\n\n    candidate_pairs = parent_pairs + children_pairs\n    # downsample if the number of candidates is too large\n    if len(candidate_pairs) &gt; pool_size:\n        candidate_pairs = random.sample(candidate_pairs, pool_size)\n\n    extended_mappings = []\n    for src_candidate_iri, tgt_candidate_iri in parent_pairs + children_pairs:\n\n        # if already computed meaning that it is not a new mapping\n        if (src_candidate_iri, tgt_candidate_iri) in self.mapping_score_dict:\n            continue\n\n        src_candidate_annotations = self.mapping_predictor.src_annotation_index[src_candidate_iri]\n        tgt_candidate_annotations = self.mapping_predictor.tgt_annotation_index[tgt_candidate_iri]\n        score = self.mapping_predictor.bert_mapping_score(src_candidate_annotations, tgt_candidate_annotations)\n        # add to already scored collection\n        self.mapping_score_dict[(src_candidate_iri, tgt_candidate_iri)] = score\n\n        # skip mappings with low scores\n        if score &lt; self.mapping_extension_threshold:\n            continue\n\n        extended_mappings.append((src_candidate_iri, tgt_candidate_iri, score))\n\n    self.logger.info(\n        f\"New mappings (in tuples) extended from {(src_class_iri, tgt_class_iri)} are:\\n\" + f\"{extended_mappings}\"\n    )\n\n    return extended_mappings\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_refinement.MappingRefiner.mapping_repair","title":"<code>mapping_repair()</code>","text":"<p>Repair the filtered mappings with LogMap's debugger.</p> <p>Note</p> <p>A sub-folder under <code>match</code> named <code>logmap-repair</code> contains LogMap-related intermediate files.</p> Source code in <code>deeponto/align/bertmap/mapping_refinement.py</code> <pre><code>def mapping_repair(self):\n\"\"\"Repair the filtered mappings with LogMap's debugger.\n\n    !!! note\n\n        A sub-folder under `match` named `logmap-repair` contains LogMap-related intermediate files.\n    \"\"\"\n\n    # progress bar for animation purposes\n    self.enlighten_status.update(demo=\"Mapping Repairing\")\n    repair_progress_bar = self.enlighten_manager.counter(\n        desc=f\"Mapping Repairing\", unit=\"mapping\"\n    )\n\n    # skip repairing if already found the file\n    if os.path.exists(self.repaired_mapping_path):\n        self.logger.info(\n            f\"Found the repaired mapping file at {self.repaired_mapping_path}.\"\n            + \"\\nPlease check file integrity; if incomplete, \"\n            + \"delete it and re-run the program.\"\n        )\n        # update progress bar for animation purposes\n        for _ in EntityMapping.read_table_mappings(self.repaired_mapping_path):\n            repair_progress_bar.update()\n        repair_progress_bar.close()\n        return \n\n    # start mapping repair\n    self.logger.info(\"Repair the filtered mappings with LogMap debugger.\")\n    # formatting the filtered mappings\n    self.logmap_repair_formatting()\n\n    # run the LogMap repair module on the extended mappings\n    run_logmap_repair(\n        self.src_onto.owl_path,\n        self.tgt_onto.owl_path,\n        os.path.join(self.logmap_repair_path, f\"filtered_mappings_for_LogMap_repair.txt\"),\n        self.logmap_repair_path,\n    )\n\n    # create table mappings from LogMap repair outputs\n    with open(os.path.join(self.logmap_repair_path, \"mappings_repaired_with_LogMap.tsv\"), \"r\") as f:\n        lines = f.readlines()\n    with open(os.path.join(self.output_path, \"match\", \"repaired_mappings.tsv\"), \"w+\") as f:\n        f.write(\"SrcEntity\\tTgtEntity\\tScore\\n\")\n        for line in lines:\n            src_ent_iri, tgt_ent_iri, score = line.split(\"\\t\")\n            f.write(f\"{src_ent_iri}\\t{tgt_ent_iri}\\t{score}\")\n            repair_progress_bar.update()\n\n    self.logger.info(\"Mapping repair finished.\")\n    repair_progress_bar.close()\n</code></pre>"},{"location":"deeponto/align/bertmap/#deeponto.align.bertmap.mapping_refinement.MappingRefiner.logmap_repair_formatting","title":"<code>logmap_repair_formatting()</code>","text":"<p>Transform the filtered mapping file into the LogMap format.</p> <p>An auxiliary function of the mapping repair module which requires mappings to be formatted as LogMap's input format.</p> Source code in <code>deeponto/align/bertmap/mapping_refinement.py</code> <pre><code>def logmap_repair_formatting(self):\n\"\"\"Transform the filtered mapping file into the LogMap format.\n\n    An auxiliary function of the mapping repair module which requires mappings\n    to be formatted as LogMap's input format.\n    \"\"\"\n    # read the filtered mapping file and convert to tuples\n    filtered_mappings = EntityMapping.read_table_mappings(self.filtered_mapping_path)\n    filtered_mappings_in_tuples = [m.to_tuple(with_score=True) for m in filtered_mappings]\n\n    # write the mappings into logmap format\n    lines = []\n    for src_class_iri, tgt_class_iri, score in filtered_mappings_in_tuples:\n        lines.append(f\"{src_class_iri}|{tgt_class_iri}|=|{score}|CLS\\n\")\n\n    # create a path to prevent error\n    FileUtils.create_path(self.logmap_repair_path)\n    formatted_file = os.path.join(self.logmap_repair_path, f\"filtered_mappings_for_LogMap_repair.txt\")\n    with open(formatted_file, \"w\") as f:\n        f.writelines(lines)\n\n    return lines\n</code></pre>"},{"location":"deeponto/align/logmap/","title":"LogMap","text":"<p>Run LogMap matcher 4.0 in a <code>jar</code> command.</p> <p>Credit</p> <p>See LogMap repository at: https://github.com/ernestojimenezruiz/logmap-matcher.</p>"},{"location":"deeponto/align/logmap/#deeponto.align.logmap.run_logmap_repair","title":"<code>run_logmap_repair(src_onto_path, tgt_onto_path, mapping_file_path, output_path)</code>","text":"<p>Run the repair module of LogMap with <code>java -jar</code>.</p> Source code in <code>deeponto/align/logmap/__init__.py</code> <pre><code>def run_logmap_repair(\n    src_onto_path: str, tgt_onto_path: str, mapping_file_path: str, output_path: str\n):\n\"\"\"Run the repair module of LogMap with `java -jar`.\"\"\"\n\n    # find logmap directory\n    logmap_path = os.path.dirname(__file__)\n\n    # obtain absolute paths\n    src_onto_path = os.path.abspath(src_onto_path)\n    tgt_onto_path = os.path.abspath(tgt_onto_path)\n    mapping_file_path = os.path.abspath(mapping_file_path)\n    output_path = os.path.abspath(output_path)\n\n    # run jar command\n    print(f\"Run the repair module of LogMap from {logmap_path}.\")\n    repair_command = (\n        f\"java -jar {logmap_path}/logmap-matcher-4.0.jar DEBUGGER \"\n        + f\"file:{src_onto_path} file:{tgt_onto_path} TXT {mapping_file_path}\"\n        + f\" {output_path} false true\"\n    )\n    print(f\"The jar command is:\\n{repair_command}.\")\n    FileUtils.run_jar(repair_command)\n</code></pre>"},{"location":"deeponto/onto/","title":"Ontology","text":""},{"location":"deeponto/onto/#overview","title":"Overview","text":"<p>Python classes in this page are strongly dependent on the OWLAPI library.  The base class <code>Ontology</code> extends several features including convenient access to specially defined entities (e.g., <code>owl:Thing</code> and <code>owl:Nothing</code>), indexing of entities in the signature with their IRIs as keys, and some other customised functions for specific ontology engineering purposes. <code>Ontology</code> also has a  <code>OntologyReasoner</code> attribute which provides reasoning facilities such as classifying entities, checking entailment, and so on. Users who are familiar with the OWLAPI should feel relatively easy to extend the Python classes here.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology","title":"<code>Ontology</code>","text":"<p>Ontology class that extends from the Java library OWLAPI.</p> <p>Note</p> <p>Types with <code>OWL</code> prefix are mostly imported from the OWLAPI library by, for example,  <code>from org.semanticweb.owlapi.model import OWLObject</code>.</p> <p>Attributes:</p> Name Type Description <code>owl_path</code> <code>str</code> <p>The path to the OWL ontology file.</p> <code>owl_manager</code> <code>OWLOntologyManager</code> <p>A ontology manager for creating <code>OWLOntology</code>.</p> <code>owl_onto</code> <code>OWLOntology</code> <p>An <code>OWLOntology</code> created by <code>owl_manger</code> from <code>owl_path</code>.</p> <code>owl_iri</code> <code>str</code> <p>The IRI of the <code>owl_onto</code>.</p> <code>owl_classes</code> <code>Dict[str, OWLClass]</code> <p>A dictionary that stores the <code>(iri, ontology_class)</code> pairs.</p> <code>owl_object_properties</code> <code>Dict[str, OWLObjectProperty]</code> <p>A dictionary that stores the <code>(iri, ontology_object_property)</code> pairs.</p> <code>owl_data_properties</code> <code>Dict[str, OWLDataProperty]</code> <p>A dictionary that stores the <code>(iri, ontology_data_property)</code> pairs.</p> <code>owl_data_factory</code> <code>OWLDataFactory</code> <p>A data factory for manipulating axioms.</p> <code>owl_annotation_properties</code> <code>Dict[str, OWLAnnotationProperty]</code> <p>A dictionary that stores the <code>(iri, ontology_annotation_property)</code> pairs.</p> <code>reasoner</code> <code>OntologyReasoner</code> <p>A reasoner for ontology inference.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>class Ontology:\n\"\"\"Ontology class that extends from the Java library OWLAPI.\n\n    !!! note\n\n        Types with `OWL` prefix are mostly imported from the OWLAPI library by, for example, \n        `from org.semanticweb.owlapi.model import OWLObject`.\n\n    Attributes:\n        owl_path (str): The path to the OWL ontology file.\n        owl_manager (OWLOntologyManager): A ontology manager for creating `OWLOntology`.\n        owl_onto (OWLOntology): An `OWLOntology` created by `owl_manger` from `owl_path`.\n        owl_iri (str): The IRI of the `owl_onto`.\n        owl_classes (Dict[str, OWLClass]): A dictionary that stores the `(iri, ontology_class)` pairs.\n        owl_object_properties (Dict[str, OWLObjectProperty]): A dictionary that stores the `(iri, ontology_object_property)` pairs.\n        owl_data_properties (Dict[str, OWLDataProperty]): A dictionary that stores the `(iri, ontology_data_property)` pairs.\n        owl_data_factory (OWLDataFactory): A data factory for manipulating axioms.\n        owl_annotation_properties (Dict[str, OWLAnnotationProperty]): A dictionary that stores the `(iri, ontology_annotation_property)` pairs.\n        reasoner (OntologyReasoner): A reasoner for ontology inference.\n    \"\"\"\n\n    def __init__(self, owl_path: str):\n\"\"\"Initialise a new ontology.\n\n        Args:\n            owl_path (str): The path to the OWL ontology file.\n        \"\"\"\n        self.owl_path = os.path.abspath(owl_path)\n        self.owl_manager = OWLManager.createOWLOntologyManager()\n        self.owl_onto = self.owl_manager.loadOntologyFromOntologyDocument(\n            IRI.create(\"file:///\" + self.owl_path)\n        )\n        self.owl_iri = str(self.owl_onto.getOntologyID().getOntologyIRI().get())\n        self.owl_classes = self.get_owl_objects(\"Classes\")\n        self.owl_object_properties = self.get_owl_objects(\"ObjectProperties\")\n        self.owl_data_properties = self.get_owl_objects(\"DataProperties\")\n        self.owl_data_factory = self.owl_manager.getOWLDataFactory()\n        self.owl_annotation_properties = self.get_owl_objects(\"AnnotationProperties\")\n\n        # reasoning\n        self.reasoner = OntologyReasoner(self)\n\n        # hidden attributes\n        self._multi_children_classes = None\n        self._sibling_class_groups = None\n        self._equiv_axioms = None\n\n        # summary\n        self.info = {\n            type(self).__name__: {\n                \"loaded_from\": os.path.basename(self.owl_path),\n                \"num_classes\": len(self.owl_classes),\n                \"num_object_properties\": len(self.owl_object_properties),\n                \"num_data_properties\": len(self.owl_data_properties),\n                \"num_annotation_properties\": len(self.owl_annotation_properties),\n            }\n        }\n\n    @property\n    def name(self):\n\"\"\"Return the name of the ontology file.\n        \"\"\"\n        return os.path.normpath(self.owl_path).split(os.path.sep)[-1]\n\n    @property\n    def OWLThing(self):\n\"\"\"Return `OWLThing`.\n        \"\"\"\n        return self.owl_data_factory.getOWLThing()\n\n    @property\n    def OWLNothing(self):\n\"\"\"Return `OWLNoThing`.\n        \"\"\"\n        return self.owl_data_factory.getOWLNothing()\n\n    @property\n    def OWLTopObjectProperty(self):\n\"\"\"Return `OWLTopObjectProperty`.\n        \"\"\"\n        return self.owl_data_factory.getOWLTopObjectProperty()\n\n    @property\n    def OWLBottomObjectProperty(self):\n\"\"\"Return `OWLBottomObjectProperty`.\n        \"\"\"\n        return self.owl_data_factory.getOWLBottomObjectProperty()\n\n    @property\n    def OWLTopDataProperty(self):\n\"\"\"Return `OWLTopDataProperty`.\n        \"\"\"\n        return self.owl_data_factory.getOWLTopDataProperty()\n\n    @property\n    def OWLBottomDataProperty(self):\n\"\"\"Return `OWLBottomDataProperty`.\n        \"\"\"\n        return self.owl_data_factory.getOWLBottomDataProperty()\n\n    @staticmethod\n    def get_entity_type(entity: OWLObject, is_singular: bool = False):\n\"\"\"A handy method to get the `type` of an `OWLObject` entity.\n        \"\"\"\n        if isinstance(entity, OWLClassExpression):\n            return \"Classes\" if not is_singular else \"Class\"\n        elif isinstance(entity, OWLObjectPropertyExpression):\n            return \"ObjectProperties\" if not is_singular else \"ObjectProperty\"\n        elif isinstance(entity, OWLDataPropertyExpression):\n            return \"DataProperties\" if not is_singular else \"DataProperty\"\n        else:\n            # NOTE: add further options in future\n            pass\n\n    def __str__(self) -&gt; str:\n        return FileUtils.print_dict(self.info)\n\n    def get_owl_objects(self, entity_type: str):\n\"\"\"Get an index of `OWLObject` of certain type from the ontology.\n\n        Args:\n            entity_type (str): Options are `\"Classes\"`, `\"ObjectProperties\"`, `\"DatasetProperties\"`, etc.\n\n        Returns:\n            (dict): A dictionary that stores the `(iri, owl_object)` pairs\n        \"\"\"\n        owl_objects = dict()\n        source = getattr(self.owl_onto, f\"get{entity_type}InSignature\")\n        for cl in source():\n            owl_objects[str(cl.getIRI())] = cl\n        return owl_objects\n\n    def get_owl_object_from_iri(self, iri: str):\n\"\"\"Get an `OWLObject` given its IRI.\n        \"\"\"\n        if iri in self.owl_classes.keys():\n            return self.owl_classes[iri]\n        elif iri in self.owl_object_properties.keys():\n            return self.owl_object_properties[iri]\n        elif iri in self.owl_data_properties.keys():\n            return self.owl_data_properties[iri]\n        elif iri in self.owl_annotation_properties.keys():\n            return self.owl_annotation_properties[iri]\n        else:\n            raise KeyError(f\"Cannot retrieve unknown IRI: {iri}.\")\n\n    def get_owl_object_annotations(\n        self,\n        owl_object: Union[OWLObject, str],\n        annotation_property_iri: Optional[str] = None,\n        annotation_language_tag: Optional[str] = None,\n        apply_lowercasing: bool = True,\n    ):\n\"\"\"Get the annotations of the given `OWLObject`.\n\n        Args:\n            owl_object (Union[OWLObject, str]): An `OWLObject` or its IRI.\n            annotation_property_iri (Optional[str], optional): \n                Any particular annotation property IRI of interest. Defaults to `None`.\n            annotation_language_tag (Optional[str], optional): \n                Any particular annotation language tag of interest; NOTE that not every \n                annotation has a language tag, in this case assume it is in English.\n                Defaults to `None`. Options are `\"en\"`, `\"ge\"` etc.\n            apply_lowercasing (bool): Whether or not to apply lowercasing to annotation literals. \n                Defaults to `True`.\n        Returns:\n            (Set[str]): A set of annotation literals of the given `OWLObject`.\n        \"\"\"\n        if isinstance(owl_object, str):\n            owl_object = self.get_owl_object_from_iri(owl_object)\n\n        annotation_property = None\n        if annotation_property_iri:\n            # return an empty list if `annotation_property_iri` does not exist in this OWLOntology`\n            annotation_property = self.get_owl_object_from_iri(annotation_property_iri)\n\n        annotations = []\n        for annotation in EntitySearcher.getAnnotations(\n            owl_object, self.owl_onto, annotation_property\n        ):\n\n            annotation = annotation.getValue()\n            # boolean that indicates whether the annotation's language is of interest\n            fit_language = False\n            if not annotation_language_tag:\n                # it is set to `True` if `annotation_langauge` is not specified\n                fit_language = True\n            else:\n                # restrict the annotations to a language if specified\n                try:\n                    # NOTE: not every annotation has a language attribute\n                    fit_language = annotation.getLang() == annotation_language_tag\n                except:\n                    # in the case when this annotation has no language tag\n                    # we assume it is in English\n                    if annotation_language_tag == \"en\":\n                        fit_language = True\n\n            if fit_language:\n                # only get annotations that have a literal value\n                if annotation.isLiteral():\n                    annotations.append(\n                        TextUtils.process_annotation_literal(\n                            str(annotation.getLiteral()), apply_lowercasing\n                        )\n                    )\n\n        return set(annotations)\n\n    @property\n    def sibling_class_groups(self) -&gt; List[List[str]]:\n\"\"\"Return grouped sibling classes (with a common *direct* parent);\n\n            NOTE that only groups with size &gt; 1 will be considered\n        \"\"\"\n        if not self._sibling_class_groups:\n\n            self._multi_children_classes = dict()\n            self._sibling_class_groups = []\n            all_class_iris = list(self.owl_classes.keys()) + [OWL_THING]  # including the root node\n\n            for cl_iri in all_class_iris:\n\n                if cl_iri == OWL_THING:\n                    cl = self.OWLThing\n                else:\n                    cl = self.get_owl_object_from_iri(cl_iri)\n\n                children_iris = self.reasoner.sub_entities_of(cl, direct=True)\n                self._multi_children_classes[cl_iri] = children_iris\n\n                if len(children_iris) &gt; 1:\n                    # classes that have siblings form a sibling group\n                    if children_iris not in self._sibling_class_groups:\n                        # it is possible that some groups appear more than once be they have mutltiple\n                        # common parents\n                        self._sibling_class_groups.append(children_iris)\n\n        return self._sibling_class_groups\n\n    @property\n    def equivalence_axioms(self):\n\"\"\"Return all the equivalence axioms in the `OWLOntology`.\n\n            NOTE: (checked with protege)\n        \"\"\"\n        if not self._equiv_axioms:\n            self._equiv_axioms = []\n            for cl in self.owl_classes.values():\n                self._equiv_axioms += self.owl_onto.getEquivalentClassesAxioms(cl)\n            self._equiv_axioms = list(set(self._equiv_axioms))\n        return self._equiv_axioms\n\n    def save_onto(self, save_path: str):\n\"\"\"Save the ontology file to the given path.\n        \"\"\"\n        self.owl_onto.saveOntology(IRI.create(File(save_path).toURI()))\n\n    def build_annotation_index(\n        self,\n        annotation_property_iris: List[str] = [RDFS_LABEL],\n        entity_type: str = \"Classes\",\n        apply_lowercasing: bool = True,\n    ):\n\"\"\"Build an annotation index for a given type of entities.\n\n        Args:\n            annotation_property_iris (List[str]): A list of annotation property IRIs (it is possible\n                that not every annotation property IRI is in use); if not provided, the built-in \n                `rdfs:label` is considered. Defaults to `[RDFS_LABEL]`.\n            entity_type (str, optional): The entity type to be considered. Defaults to `\"Classes\"`. \n                Options are `\"Classes\"`, `\"ObjectProperties\"`, `\"DatasetProperties\"`, etc.\n            apply_lowercasing (bool): Whether or not to apply lowercasing to annotation literals. \n                Defaults to `True`.\n\n        Returns:\n            (Tuple[dict, List[str]]): The built annotation index, and the list of annotation property IRIs that are in use.\n        \"\"\"\n\n        annotation_index = defaultdict(set)\n        # example: Classes =&gt; owl_classes; ObjectProperties =&gt; owl_object_properties\n        entity_type = (\n            \"owl_\" + TextUtils.split_java_identifier(entity_type).replace(\" \", \"_\").lower()\n        )\n        entity_index = getattr(self, entity_type)\n\n        # preserve available annotation properties\n        annotation_property_iris = [\n            airi\n            for airi in annotation_property_iris\n            if airi in self.owl_annotation_properties.keys()\n        ]\n\n        # build the annotation index without duplicated literals\n        for airi in annotation_property_iris:\n            for iri, entity in entity_index.items():\n                annotation_index[iri].update(\n                    self.get_owl_object_annotations(\n                        owl_object=entity,\n                        annotation_property_iri=airi,\n                        annotation_language_tag=None,\n                        apply_lowercasing=apply_lowercasing,\n                    )\n                )\n\n        return annotation_index, annotation_property_iris\n\n    @staticmethod\n    def build_inverted_annotation_index(annotation_index: dict, tokenizer: Tokenizer):\n\"\"\"Build an inverted annotation index given an annotation index and a tokenizer.\n        \"\"\"\n        return InvertedIndex(annotation_index, tokenizer)\n\n    def add_axiom(self, owl_axiom: OWLAxiom, return_undo: bool = True):\n\"\"\"Add an axiom into the current ontology.\n\n        Args:\n            owl_axiom (OWLAxiom): An axiom to be added.\n            return_undo (bool, optional): Returning the undo operation or not. Defaults to True.\n        \"\"\"\n        change = AddAxiom(self.owl_onto, owl_axiom)\n        result = self.owl_onto.applyChange(change)\n        print(f\"[{str(result)}] Adding the axiom {str(owl_axiom)} into the ontology.\")\n        if return_undo:\n            return change.reverseChange()\n\n    def replace_entity(self, owl_object: OWLObject, entity_iri: str, replacement_iri: str):\n\"\"\"Replace an entity in a class expression with another entity.\n\n        Args:\n            owl_object (OWLObject): An `OWLObject` entity to be manipulated.\n            entity_iri (str): IRI of the entity to be replaced.\n            replacement_iri (str): IRI of the entity to replace.\n\n        Returns:\n            (OWLObject): The changed `OWLObject` entity.\n        \"\"\"\n        iri_dict = {IRI.create(entity_iri): IRI.create(replacement_iri)}\n        replacer = OWLObjectDuplicator(self.owlDataFactory, iri_dict)\n        return replacer.duplicateObject(owl_object)\n\n    @paper(\n        \"Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022)\",\n        \"https://link.springer.com/chapter/10.1007/978-3-031-19433-7_33\",\n    )\n    def apply_pruning(self, class_iris_to_be_removed: List[str]):\nr\"\"\"Run pruning given a list of classes that will be pruned.    \n\n        !!! credit \"paper\"\n\n            This refers to the ontology pruning algorithm introduced in the paper: \n            [Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022)](https://link.springer.com/chapter/10.1007/978-3-031-19433-7_33).\n\n        For each class $c$ to be pruned, subsumption axioms will be created between $c$'s parents and children so as to preserve the\n        relevant hierarchy.\n\n        Args:\n            class_iris_to_be_removed (List[str]): Classes with IRIs in this list will be pruned and the relevant hierarchy will be repaired.\n        \"\"\"\n\n        # create the subsumption axioms first\n        for cl_iri in class_iris_to_be_removed:\n            cl = self.get_owl_object_from_iri(cl_iri)\n            cl_parents = self.reasoner.super_entities_of(cl, direct=True)\n            cl_children = self.reasoner.sub_entities_of(cl, direct=True)\n            for parent, child in itertools.product(cl_parents, cl_children):\n                parent = self.get_owl_object_from_iri(parent)\n                child = self.get_owl_object_from_iri(child)\n                sub_axiom = self.owl_data_factory.getOWLSubClassOfAxiom(\n                    child, parent\n                )\n                self.add_axiom(sub_axiom)\n\n        # apply pruning\n        class_remover = OWLEntityRemover(Collections.singleton(self.owl_onto))\n        for cl_iri in class_iris_to_be_removed:\n            cl = self.get_owl_object_from_iri(cl_iri)\n            cl.accept(class_remover)\n        self.owl_manager.applyChanges(class_remover.getChanges())\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the name of the ontology file.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.OWLThing","title":"<code>OWLThing</code>  <code>property</code>","text":"<p>Return <code>OWLThing</code>.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.OWLNothing","title":"<code>OWLNothing</code>  <code>property</code>","text":"<p>Return <code>OWLNoThing</code>.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.OWLTopObjectProperty","title":"<code>OWLTopObjectProperty</code>  <code>property</code>","text":"<p>Return <code>OWLTopObjectProperty</code>.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.OWLBottomObjectProperty","title":"<code>OWLBottomObjectProperty</code>  <code>property</code>","text":"<p>Return <code>OWLBottomObjectProperty</code>.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.OWLTopDataProperty","title":"<code>OWLTopDataProperty</code>  <code>property</code>","text":"<p>Return <code>OWLTopDataProperty</code>.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.OWLBottomDataProperty","title":"<code>OWLBottomDataProperty</code>  <code>property</code>","text":"<p>Return <code>OWLBottomDataProperty</code>.</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.sibling_class_groups","title":"<code>sibling_class_groups: List[List[str]]</code>  <code>property</code>","text":"<p>Return grouped sibling classes (with a common direct parent);</p> <p>NOTE that only groups with size &gt; 1 will be considered</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.equivalence_axioms","title":"<code>equivalence_axioms</code>  <code>property</code>","text":"<p>Return all the equivalence axioms in the <code>OWLOntology</code>.</p> <p>NOTE: (checked with protege)</p>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.__init__","title":"<code>__init__(owl_path)</code>","text":"<p>Initialise a new ontology.</p> <p>Parameters:</p> Name Type Description Default <code>owl_path</code> <code>str</code> <p>The path to the OWL ontology file.</p> required Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def __init__(self, owl_path: str):\n\"\"\"Initialise a new ontology.\n\n    Args:\n        owl_path (str): The path to the OWL ontology file.\n    \"\"\"\n    self.owl_path = os.path.abspath(owl_path)\n    self.owl_manager = OWLManager.createOWLOntologyManager()\n    self.owl_onto = self.owl_manager.loadOntologyFromOntologyDocument(\n        IRI.create(\"file:///\" + self.owl_path)\n    )\n    self.owl_iri = str(self.owl_onto.getOntologyID().getOntologyIRI().get())\n    self.owl_classes = self.get_owl_objects(\"Classes\")\n    self.owl_object_properties = self.get_owl_objects(\"ObjectProperties\")\n    self.owl_data_properties = self.get_owl_objects(\"DataProperties\")\n    self.owl_data_factory = self.owl_manager.getOWLDataFactory()\n    self.owl_annotation_properties = self.get_owl_objects(\"AnnotationProperties\")\n\n    # reasoning\n    self.reasoner = OntologyReasoner(self)\n\n    # hidden attributes\n    self._multi_children_classes = None\n    self._sibling_class_groups = None\n    self._equiv_axioms = None\n\n    # summary\n    self.info = {\n        type(self).__name__: {\n            \"loaded_from\": os.path.basename(self.owl_path),\n            \"num_classes\": len(self.owl_classes),\n            \"num_object_properties\": len(self.owl_object_properties),\n            \"num_data_properties\": len(self.owl_data_properties),\n            \"num_annotation_properties\": len(self.owl_annotation_properties),\n        }\n    }\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.get_entity_type","title":"<code>get_entity_type(entity, is_singular=False)</code>  <code>staticmethod</code>","text":"<p>A handy method to get the <code>type</code> of an <code>OWLObject</code> entity.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>@staticmethod\ndef get_entity_type(entity: OWLObject, is_singular: bool = False):\n\"\"\"A handy method to get the `type` of an `OWLObject` entity.\n    \"\"\"\n    if isinstance(entity, OWLClassExpression):\n        return \"Classes\" if not is_singular else \"Class\"\n    elif isinstance(entity, OWLObjectPropertyExpression):\n        return \"ObjectProperties\" if not is_singular else \"ObjectProperty\"\n    elif isinstance(entity, OWLDataPropertyExpression):\n        return \"DataProperties\" if not is_singular else \"DataProperty\"\n    else:\n        # NOTE: add further options in future\n        pass\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.get_owl_objects","title":"<code>get_owl_objects(entity_type)</code>","text":"<p>Get an index of <code>OWLObject</code> of certain type from the ontology.</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>Options are <code>\"Classes\"</code>, <code>\"ObjectProperties\"</code>, <code>\"DatasetProperties\"</code>, etc.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary that stores the <code>(iri, owl_object)</code> pairs</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def get_owl_objects(self, entity_type: str):\n\"\"\"Get an index of `OWLObject` of certain type from the ontology.\n\n    Args:\n        entity_type (str): Options are `\"Classes\"`, `\"ObjectProperties\"`, `\"DatasetProperties\"`, etc.\n\n    Returns:\n        (dict): A dictionary that stores the `(iri, owl_object)` pairs\n    \"\"\"\n    owl_objects = dict()\n    source = getattr(self.owl_onto, f\"get{entity_type}InSignature\")\n    for cl in source():\n        owl_objects[str(cl.getIRI())] = cl\n    return owl_objects\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.get_owl_object_from_iri","title":"<code>get_owl_object_from_iri(iri)</code>","text":"<p>Get an <code>OWLObject</code> given its IRI.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def get_owl_object_from_iri(self, iri: str):\n\"\"\"Get an `OWLObject` given its IRI.\n    \"\"\"\n    if iri in self.owl_classes.keys():\n        return self.owl_classes[iri]\n    elif iri in self.owl_object_properties.keys():\n        return self.owl_object_properties[iri]\n    elif iri in self.owl_data_properties.keys():\n        return self.owl_data_properties[iri]\n    elif iri in self.owl_annotation_properties.keys():\n        return self.owl_annotation_properties[iri]\n    else:\n        raise KeyError(f\"Cannot retrieve unknown IRI: {iri}.\")\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.get_owl_object_annotations","title":"<code>get_owl_object_annotations(owl_object, annotation_property_iri=None, annotation_language_tag=None, apply_lowercasing=True)</code>","text":"<p>Get the annotations of the given <code>OWLObject</code>.</p> <p>Parameters:</p> Name Type Description Default <code>owl_object</code> <code>Union[OWLObject, str]</code> <p>An <code>OWLObject</code> or its IRI.</p> required <code>annotation_property_iri</code> <code>Optional[str]</code> <p>Any particular annotation property IRI of interest. Defaults to <code>None</code>.</p> <code>None</code> <code>annotation_language_tag</code> <code>Optional[str]</code> <p>Any particular annotation language tag of interest; NOTE that not every  annotation has a language tag, in this case assume it is in English. Defaults to <code>None</code>. Options are <code>\"en\"</code>, <code>\"ge\"</code> etc.</p> <code>None</code> <code>apply_lowercasing</code> <code>bool</code> <p>Whether or not to apply lowercasing to annotation literals.  Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Set[str]</code> <p>A set of annotation literals of the given <code>OWLObject</code>.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def get_owl_object_annotations(\n    self,\n    owl_object: Union[OWLObject, str],\n    annotation_property_iri: Optional[str] = None,\n    annotation_language_tag: Optional[str] = None,\n    apply_lowercasing: bool = True,\n):\n\"\"\"Get the annotations of the given `OWLObject`.\n\n    Args:\n        owl_object (Union[OWLObject, str]): An `OWLObject` or its IRI.\n        annotation_property_iri (Optional[str], optional): \n            Any particular annotation property IRI of interest. Defaults to `None`.\n        annotation_language_tag (Optional[str], optional): \n            Any particular annotation language tag of interest; NOTE that not every \n            annotation has a language tag, in this case assume it is in English.\n            Defaults to `None`. Options are `\"en\"`, `\"ge\"` etc.\n        apply_lowercasing (bool): Whether or not to apply lowercasing to annotation literals. \n            Defaults to `True`.\n    Returns:\n        (Set[str]): A set of annotation literals of the given `OWLObject`.\n    \"\"\"\n    if isinstance(owl_object, str):\n        owl_object = self.get_owl_object_from_iri(owl_object)\n\n    annotation_property = None\n    if annotation_property_iri:\n        # return an empty list if `annotation_property_iri` does not exist in this OWLOntology`\n        annotation_property = self.get_owl_object_from_iri(annotation_property_iri)\n\n    annotations = []\n    for annotation in EntitySearcher.getAnnotations(\n        owl_object, self.owl_onto, annotation_property\n    ):\n\n        annotation = annotation.getValue()\n        # boolean that indicates whether the annotation's language is of interest\n        fit_language = False\n        if not annotation_language_tag:\n            # it is set to `True` if `annotation_langauge` is not specified\n            fit_language = True\n        else:\n            # restrict the annotations to a language if specified\n            try:\n                # NOTE: not every annotation has a language attribute\n                fit_language = annotation.getLang() == annotation_language_tag\n            except:\n                # in the case when this annotation has no language tag\n                # we assume it is in English\n                if annotation_language_tag == \"en\":\n                    fit_language = True\n\n        if fit_language:\n            # only get annotations that have a literal value\n            if annotation.isLiteral():\n                annotations.append(\n                    TextUtils.process_annotation_literal(\n                        str(annotation.getLiteral()), apply_lowercasing\n                    )\n                )\n\n    return set(annotations)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.save_onto","title":"<code>save_onto(save_path)</code>","text":"<p>Save the ontology file to the given path.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def save_onto(self, save_path: str):\n\"\"\"Save the ontology file to the given path.\n    \"\"\"\n    self.owl_onto.saveOntology(IRI.create(File(save_path).toURI()))\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.build_annotation_index","title":"<code>build_annotation_index(annotation_property_iris=[RDFS_LABEL], entity_type='Classes', apply_lowercasing=True)</code>","text":"<p>Build an annotation index for a given type of entities.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_property_iris</code> <code>List[str]</code> <p>A list of annotation property IRIs (it is possible that not every annotation property IRI is in use); if not provided, the built-in  <code>rdfs:label</code> is considered. Defaults to <code>[RDFS_LABEL]</code>.</p> <code>[RDFS_LABEL]</code> <code>entity_type</code> <code>str</code> <p>The entity type to be considered. Defaults to <code>\"Classes\"</code>.  Options are <code>\"Classes\"</code>, <code>\"ObjectProperties\"</code>, <code>\"DatasetProperties\"</code>, etc.</p> <code>'Classes'</code> <code>apply_lowercasing</code> <code>bool</code> <p>Whether or not to apply lowercasing to annotation literals.  Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[dict, List[str]]</code> <p>The built annotation index, and the list of annotation property IRIs that are in use.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def build_annotation_index(\n    self,\n    annotation_property_iris: List[str] = [RDFS_LABEL],\n    entity_type: str = \"Classes\",\n    apply_lowercasing: bool = True,\n):\n\"\"\"Build an annotation index for a given type of entities.\n\n    Args:\n        annotation_property_iris (List[str]): A list of annotation property IRIs (it is possible\n            that not every annotation property IRI is in use); if not provided, the built-in \n            `rdfs:label` is considered. Defaults to `[RDFS_LABEL]`.\n        entity_type (str, optional): The entity type to be considered. Defaults to `\"Classes\"`. \n            Options are `\"Classes\"`, `\"ObjectProperties\"`, `\"DatasetProperties\"`, etc.\n        apply_lowercasing (bool): Whether or not to apply lowercasing to annotation literals. \n            Defaults to `True`.\n\n    Returns:\n        (Tuple[dict, List[str]]): The built annotation index, and the list of annotation property IRIs that are in use.\n    \"\"\"\n\n    annotation_index = defaultdict(set)\n    # example: Classes =&gt; owl_classes; ObjectProperties =&gt; owl_object_properties\n    entity_type = (\n        \"owl_\" + TextUtils.split_java_identifier(entity_type).replace(\" \", \"_\").lower()\n    )\n    entity_index = getattr(self, entity_type)\n\n    # preserve available annotation properties\n    annotation_property_iris = [\n        airi\n        for airi in annotation_property_iris\n        if airi in self.owl_annotation_properties.keys()\n    ]\n\n    # build the annotation index without duplicated literals\n    for airi in annotation_property_iris:\n        for iri, entity in entity_index.items():\n            annotation_index[iri].update(\n                self.get_owl_object_annotations(\n                    owl_object=entity,\n                    annotation_property_iri=airi,\n                    annotation_language_tag=None,\n                    apply_lowercasing=apply_lowercasing,\n                )\n            )\n\n    return annotation_index, annotation_property_iris\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.build_inverted_annotation_index","title":"<code>build_inverted_annotation_index(annotation_index, tokenizer)</code>  <code>staticmethod</code>","text":"<p>Build an inverted annotation index given an annotation index and a tokenizer.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>@staticmethod\ndef build_inverted_annotation_index(annotation_index: dict, tokenizer: Tokenizer):\n\"\"\"Build an inverted annotation index given an annotation index and a tokenizer.\n    \"\"\"\n    return InvertedIndex(annotation_index, tokenizer)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.add_axiom","title":"<code>add_axiom(owl_axiom, return_undo=True)</code>","text":"<p>Add an axiom into the current ontology.</p> <p>Parameters:</p> Name Type Description Default <code>owl_axiom</code> <code>OWLAxiom</code> <p>An axiom to be added.</p> required <code>return_undo</code> <code>bool</code> <p>Returning the undo operation or not. Defaults to True.</p> <code>True</code> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def add_axiom(self, owl_axiom: OWLAxiom, return_undo: bool = True):\n\"\"\"Add an axiom into the current ontology.\n\n    Args:\n        owl_axiom (OWLAxiom): An axiom to be added.\n        return_undo (bool, optional): Returning the undo operation or not. Defaults to True.\n    \"\"\"\n    change = AddAxiom(self.owl_onto, owl_axiom)\n    result = self.owl_onto.applyChange(change)\n    print(f\"[{str(result)}] Adding the axiom {str(owl_axiom)} into the ontology.\")\n    if return_undo:\n        return change.reverseChange()\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.replace_entity","title":"<code>replace_entity(owl_object, entity_iri, replacement_iri)</code>","text":"<p>Replace an entity in a class expression with another entity.</p> <p>Parameters:</p> Name Type Description Default <code>owl_object</code> <code>OWLObject</code> <p>An <code>OWLObject</code> entity to be manipulated.</p> required <code>entity_iri</code> <code>str</code> <p>IRI of the entity to be replaced.</p> required <code>replacement_iri</code> <code>str</code> <p>IRI of the entity to replace.</p> required <p>Returns:</p> Type Description <code>OWLObject</code> <p>The changed <code>OWLObject</code> entity.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def replace_entity(self, owl_object: OWLObject, entity_iri: str, replacement_iri: str):\n\"\"\"Replace an entity in a class expression with another entity.\n\n    Args:\n        owl_object (OWLObject): An `OWLObject` entity to be manipulated.\n        entity_iri (str): IRI of the entity to be replaced.\n        replacement_iri (str): IRI of the entity to replace.\n\n    Returns:\n        (OWLObject): The changed `OWLObject` entity.\n    \"\"\"\n    iri_dict = {IRI.create(entity_iri): IRI.create(replacement_iri)}\n    replacer = OWLObjectDuplicator(self.owlDataFactory, iri_dict)\n    return replacer.duplicateObject(owl_object)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.Ontology.apply_pruning","title":"<code>apply_pruning(class_iris_to_be_removed)</code>","text":"<p>Run pruning given a list of classes that will be pruned.    </p> <p>paper</p> <p>This refers to the ontology pruning algorithm introduced in the paper:  Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022).</p> <p>For each class \\(c\\) to be pruned, subsumption axioms will be created between \\(c\\)'s parents and children so as to preserve the relevant hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>class_iris_to_be_removed</code> <code>List[str]</code> <p>Classes with IRIs in this list will be pruned and the relevant hierarchy will be repaired.</p> required Source code in <code>deeponto/onto/ontology.py</code> <pre><code>@paper(\n    \"Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022)\",\n    \"https://link.springer.com/chapter/10.1007/978-3-031-19433-7_33\",\n)\ndef apply_pruning(self, class_iris_to_be_removed: List[str]):\nr\"\"\"Run pruning given a list of classes that will be pruned.    \n\n    !!! credit \"paper\"\n\n        This refers to the ontology pruning algorithm introduced in the paper: \n        [Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022)](https://link.springer.com/chapter/10.1007/978-3-031-19433-7_33).\n\n    For each class $c$ to be pruned, subsumption axioms will be created between $c$'s parents and children so as to preserve the\n    relevant hierarchy.\n\n    Args:\n        class_iris_to_be_removed (List[str]): Classes with IRIs in this list will be pruned and the relevant hierarchy will be repaired.\n    \"\"\"\n\n    # create the subsumption axioms first\n    for cl_iri in class_iris_to_be_removed:\n        cl = self.get_owl_object_from_iri(cl_iri)\n        cl_parents = self.reasoner.super_entities_of(cl, direct=True)\n        cl_children = self.reasoner.sub_entities_of(cl, direct=True)\n        for parent, child in itertools.product(cl_parents, cl_children):\n            parent = self.get_owl_object_from_iri(parent)\n            child = self.get_owl_object_from_iri(child)\n            sub_axiom = self.owl_data_factory.getOWLSubClassOfAxiom(\n                child, parent\n            )\n            self.add_axiom(sub_axiom)\n\n    # apply pruning\n    class_remover = OWLEntityRemover(Collections.singleton(self.owl_onto))\n    for cl_iri in class_iris_to_be_removed:\n        cl = self.get_owl_object_from_iri(cl_iri)\n        cl.accept(class_remover)\n    self.owl_manager.applyChanges(class_remover.getChanges())\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner","title":"<code>OntologyReasoner</code>","text":"<p>Ontology reasoner class that extends from the Java library OWLAPI.</p> <p>Attributes:</p> Name Type Description <code>onto</code> <code>Ontology</code> <p>The input <code>deeponto</code> ontology.</p> <code>owl_reasoner_factory</code> <code>OWLReasonerFactory</code> <p>A reasoner factory for creating a reasoner.</p> <code>owl_reasoner</code> <code>OWLReasoner</code> <p>The created reasoner.</p> <code>owl_data_factory</code> <code>OWLDataFactory</code> <p>A data factory (inherited from <code>onto</code>) for manipulating axioms.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>class OntologyReasoner:\n\"\"\"Ontology reasoner class that extends from the Java library OWLAPI.\n\n    Attributes:\n        onto (Ontology): The input `deeponto` ontology.\n        owl_reasoner_factory (OWLReasonerFactory): A reasoner factory for creating a reasoner.\n        owl_reasoner (OWLReasoner): The created reasoner.\n        owl_data_factory (OWLDataFactory): A data factory (inherited from `onto`) for manipulating axioms.\n    \"\"\"\n\n    def __init__(self, onto: Ontology):\n\"\"\"Initialise an ontology reasoner.\n\n        Args:\n            onto (Ontology): The input ontology to conduct reasoning on.\n        \"\"\"\n        self.onto = onto\n        self.owl_reasoner_factory = ReasonerFactory()\n        self.owl_reasoner = self.owl_reasoner_factory.createReasoner(self.onto.owl_onto)\n        self.owl_data_factory = self.onto.owl_data_factory\n\n    def reload_reasoner(self):\n\"\"\"Reload the reasoner for the current ontology (possibly changed).\n        \"\"\"\n        # release the memory\n        self.owl_reasoner.dispose()\n        # conduct reasoning on the possibly changed ontology\n        self.owl_reasoner = self.owl_reasoner_factory.createReasoner(self.onto.owl_onto)\n\n    @staticmethod\n    def get_entity_type(entity: OWLObject, is_singular: bool = False):\n\"\"\"A handy method to get the type of an entity (`OWLObject`).\n\n        NOTE: This method is inherited from the Ontology Class.\n        \"\"\"\n        return Ontology.get_entity_type(entity, is_singular)\n\n    @staticmethod\n    def has_iri(entity: OWLObject):\n\"\"\"Check if an entity has an IRI.\n        \"\"\"\n        try:\n            entity.getIRI()\n            return True\n        except:\n            return False\n\n    def super_entities_of(self, entity: OWLObject, direct: bool = False):\nr\"\"\"Return the IRIs of super-entities of a given `OWLObject` according to the reasoner.\n\n        A mixture of `getSuperClasses`, `getSuperObjectProperties`, `getSuperDataProperties`\n        functions imported from the OWLAPI reasoner. The type of input entity will be \n        automatically determined. The top entity such as `owl:Thing` is ignored.\n\n\n        Args:\n            entity (OWLObject): An `OWLObject` entity of interest.\n            direct (bool, optional): Return parents (`direct=True`) or \n                ancestors (`direct=False`). Defaults to `False`.\n\n        Returns:\n            (List[str]): A list of IRIs of the super-entities of the given `OWLObject` entity.\n        \"\"\"\n        entity_type = self.get_entity_type(entity)\n        get_super = f\"getSuper{entity_type}\"\n        TOP = TOP_BOTTOMS[entity_type].TOP  # get the corresponding TOP entity\n        super_entities = getattr(self.owl_reasoner, get_super)(entity, direct).getFlattened()\n        super_entity_iris = [str(s.getIRI()) for s in super_entities]\n        # the root node is owl#Thing\n        if TOP in super_entity_iris:\n            super_entity_iris.remove(TOP)\n        return super_entity_iris\n\n    def sub_entities_of(self, entity: OWLObject, direct: bool = False):\n\"\"\"Return the IRIs of sub-entities of a given `OWLObject` according to the reasoner.\n\n        A mixture of `getSubClasses`, `getSubObjectProperties`, `getSubDataProperties`\n        functions imported from the OWLAPI reasoner. The type of input entity will be \n        automatically determined. The bottom entity such as `owl:Nothing` is ignored.\n\n        Args:\n            entity (OWLObject): An `OWLObject` entity of interest.\n            direct (bool, optional): Return parents (`direct=True`) or \n                ancestors (`direct=False`). Defaults to `False`.\n\n        Returns:\n            (List[str]): A list of IRIs of the sub-entities of the given `OWLObject` entity.\n        \"\"\"\n        entity_type = self.get_entity_type(entity)\n        get_sub = f\"getSub{entity_type}\"\n        BOTTOM = TOP_BOTTOMS[entity_type].BOTTOM\n        sub_entities = getattr(self.owl_reasoner, get_sub)(entity, direct).getFlattened()\n        sub_entity_iris = [str(s.getIRI()) for s in sub_entities]\n        # the root node is owl#Thing\n        if BOTTOM in sub_entity_iris:\n            sub_entity_iris.remove(BOTTOM)\n        return sub_entity_iris\n\n    def check_subsumption(self, sub_entity: OWLObject, super_entity: OWLObject):\n\"\"\"Check if the first entity is subsumed by the second entity according to the reasoner.\n        \"\"\"\n        entity_type = self.get_entity_type(sub_entity, is_singular=True)\n        assert entity_type == self.get_entity_type(super_entity, is_singular=True)\n\n        sub_axiom = getattr(self.owl_data_factory, f\"getOWLSub{entity_type}OfAxiom\")(\n            sub_entity, super_entity\n        )\n\n        return self.owl_reasoner.isEntailed(sub_axiom)\n\n    def check_disjoint(self, entity1: OWLObject, entity2: OWLObject):\n\"\"\"Check if two OWL class expressions are disjoint according to the reasoner.\n        \"\"\"\n        entity_type = self.get_entity_type(entity1)\n        assert entity_type == self.get_entity_type(entity2)\n\n        disjoint_axiom = getattr(self.owl_data_factory, f\"getOWLDisjoint{entity_type}Axiom\")(\n            [entity1, entity2]\n        )\n\n        return self.owl_reasoner.isEntailed(disjoint_axiom)\n\n    def check_common_descendants(self, entity1: OWLObject, entity2: OWLObject):\n\"\"\"Check if two entities have a common decendant.\n\n        Entities can be **OWL class or property expressions**, and can be either **atomic\n        or complex**. It takes longer computation time for the complex ones. Complex \n        entities do not have an IRI. This method is optimised in the way that if \n        there exists an atomic entity `A`, we compute descendants for `A` and\n        compare them against the other entity which could be complex.\n        \"\"\"\n        entity_type = self.get_entity_type(entity1)\n        assert entity_type == self.get_entity_type(entity2)\n\n        if not self.has_iri(entity1) and not self.has_iri(entity2):\n            warnings.warn(\"Computing descendants for two complex entities is not efficient.\")\n\n        # `computed` is the one we compute the descendants\n        # `compared` is the one we compare `computed`'s descendant one-by-one\n        # we set the atomic entity as `computed` for efficiency if there is one\n        computed, compared = entity1, entity2\n        if not self.has_iri(entity1) and self.has_iri(entity2):\n            computed, compared = entity2, entity1\n\n        # for every inferred child of `computed`, check if it is subsumed by `compared``\n        for descendant_iri in self.sub_entities_of(computed):\n            # print(\"check a subsumption\")\n            if self.check_subsumption(self.onto.get_owl_object_from_iri(descendant_iri), compared):\n                return True\n        return False\n\n    def instances_of(self, owl_class: OWLClassExpression):\n\"\"\"Return the IRIs of sub-entities of a given OWL class expression.\n\n        Args:\n            owl_class (OWLClassExpression): An ontology class of interest.\n\n        Returns:\n            (List[OWLNamedIndividual]): A list of named individuals that are entailed instances of `owl_class`.\n        \"\"\"\n        return list(self.owl_reasoner.getInstances(owl_class).getFlattened())\n\n    def check_instance(self, owl_instance: OWLNamedIndividual, owl_class: OWLClassExpression):\n\"\"\"Check if a named individual is an instance of an OWL class.\n        \"\"\"\n        assertion_axiom = self.owl_data_factory.getOWLClassAssertionAxiom(owl_class, owl_instance)\n        return self.owl_reasoner.isEntailed(assertion_axiom)\n\n    def check_common_instances(\n        self, owl_class1: OWLClassExpression, owl_class2: OWLClassExpression\n    ):\n\"\"\"Check if two OWL class expressions have a common instance.\n\n        Class expressions can be **atomic or complex**, and it takes longer computation time\n        for the complex ones. Complex classes do not have an IRI. This method is optimised\n        in the way that if there exists an atomic class `A`, we compute instances for `A` and\n        compare them against the other class which could be complex. \n\n        NOTE that compared to [`check_common_descendants`][deeponto.onto.OntologyReasoner.check_common_descendants]\n        function, the inputs of this function is restricted to OWL class expressions. This is because\n        `descendant` is related to hierarchy and both class and property expressions have a hierarchy,\n        but `instance` is restricted to classes.\n        \"\"\"\n\n        if not self.has_iri(owl_class1) and not self.has_iri(owl_class2):\n            warnings.warn(\"Computing instances for two complex classes is not efficient.\")\n\n        # `computed` is the one we compute the instances\n        # `compared` is the one we compare `computed`'s descendant one-by-one\n        # we set the atomic entity as `computed` for efficiency if there is one\n        computed, compared = owl_class1, owl_class2\n        if not self.has_iri(owl_class1) and self.has_iri(owl_class2):\n            computed, compared = owl_class2, owl_class2\n\n        # for every inferred instance of `computed`, check if it is subsumed by `compared``\n        for instance in self.instances_of(computed):\n            if self.check_instance(instance, compared):\n                return True\n        return False\n\n\n    def check_assumed_disjoint(\n        self, owl_class1: OWLClassExpression, owl_class2: OWLClassExpression\n    ):\nr\"\"\"Check if two OWL class expressions satisfy the Assumed Disjointness.\n\n        !!! credit \"Paper\"\n\n            The definition of **Assumed Disjointness** comes from the paper:\n            *[still-under-review](link)*.\n\n        !!! note\n\n            Two class expressions C and D are assumed to be disjoint if they meet the followings:\n\n            1. By adding the disjointness axiom of them into the ontology, C and D are **still satisfiable**.\n            2. C and D **do not have a common descendant** (otherwise C and D can be satisfiable but their\n            common descendants become the bottom $\\bot$.) \n\n        Note that the special case where C and D are already disjoint is covered by the first check.\n        The paper also proposed a practical alternative to decide Assumed Disjointness. \n        See [`check_assumed_disjoint_alternative`][deeponto.onto.OntologyReasoner.check_assumed_disjoint_alternative].\n\n        Examples:\n            Suppose pre-load an ontology `onto` from the disease ontology file `doid.owl`.\n\n            ```python\n            &gt;&gt;&gt; c1 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_4058\")\n            &gt;&gt;&gt; c2 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_0001816\")\n            &gt;&gt;&gt; onto.reasoner.check_assumed_disjoint(c1, c2)\n            [SUCCESSFULLY] Adding the axiom DisjointClasses(&lt;http://purl.obolibrary.org/obo/DOID_0001816&gt; &lt;http://purl.obolibrary.org/obo/DOID_4058&gt;) into the ontology.\n            [CHECK1 True] input classes are still satisfiable;\n            [SUCCESSFULLY] Removing the axiom from the ontology.\n            [CHECK2 False] input classes have NO common descendant.\n            [PASSED False] assumed disjointness check done.\n            False   \n            ```\n        \"\"\"\n        # banner_message(\"Check Asssumed Disjointness\")\n\n        entity_type = self.get_entity_type(owl_class1)\n        assert entity_type == self.get_entity_type(owl_class2)\n\n        # adding the disjointness axiom of `class1`` and `class2``\n        disjoint_axiom = getattr(self.owl_data_factory, f\"getOWLDisjoint{entity_type}Axiom\")(\n            [owl_class1, owl_class2]\n        )\n        undo_change = self.onto.add_axiom(disjoint_axiom, return_undo=True)\n        self.reload_reasoner()\n\n        # check if they are still satisfiable\n        still_satisfiable = self.owl_reasoner.isSatisfiable(owl_class1)\n        still_satisfiable = still_satisfiable and self.owl_reasoner.isSatisfiable(owl_class2)\n        print(f\"[CHECK1 {still_satisfiable}] input classes are still satisfiable;\")\n\n        # remove the axiom and re-construct the reasoner\n        undo_change_result = self.onto.owl_onto.applyChange(undo_change)\n        print(f\"[{str(undo_change_result)}] Removing the axiom from the ontology.\")\n        self.reload_reasoner()\n\n        # failing first check, there is no need to do the second.\n        if not still_satisfiable:\n            print(\"Failed `satisfiability check`, skip the `common descendant` check.\")\n            print(f\"[PASSED {still_satisfiable}] assumed disjointness check done.\")\n            return False\n\n        # otherwise, the classes are still satisfiable and we should conduct the second check\n        has_common_descendants = self.check_common_descendants(owl_class1, owl_class2)\n        print(f\"[CHECK2 {not has_common_descendants}] input classes have NO common descendant.\")\n        print(f\"[PASSED {not has_common_descendants}] assumed disjointness check done.\")\n        return not has_common_descendants\n\n    def check_assumed_disjoint_alternative(\n        self, owl_class1: OWLClassExpression, owl_class2: OWLClassExpression\n    ):\nr\"\"\"Check if two OWL class expressions satisfy the Assumed Disjointness.\n\n        !!! credit \"Paper\"\n\n            The definition of **Assumed Disjointness** comes from the paper:\n            *[still-under-review](link)*.\n\n        The practical alternative version of [`check_assumed_disjoint`][deeponto.onto.OntologyReasoner.check_assumed_disjoint]\n        with following conditions:\n\n\n        !!! note\n\n            Two class expressions C and D are assumed to be disjoint if they\n\n            1. **do not** have a **subsumption relationship** between them, \n            2. **do not** have a **common descendant** (in TBox), \n            3. **do not** have a **common instance** (in ABox).\n\n        If either of the conditions have been met, then we assume `class1` and `class2` as disjoint.\n\n        Examples:\n            Suppose pre-load an ontology `onto` from the disease ontology file `doid.owl`.\n\n            ```python\n            &gt;&gt;&gt; c1 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_4058\")\n            &gt;&gt;&gt; c2 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_0001816\")\n            &gt;&gt;&gt; onto.reasoner.check_assumed_disjoint(c1, c2)\n            [CHECK1 True] input classes have NO subsumption relationship;\n            [CHECK2 False] input classes have NO common descendant;\n            Failed the `common descendant check`, skip the `common instance` check.\n            [PASSED False] assumed disjointness check done.\n            False   \n            ```\n            In this alternative implementation, we do no need to add and remove axioms which will then\n            be time-saving.\n        \"\"\"\n        # banner_message(\"Check Asssumed Disjointness (Alternative)\")\n\n        # # Check for entailed disjointness (short-cut)\n        # if self.check_disjoint(owl_class1, owl_class2):\n        #     print(f\"Input classes are already entailed as disjoint.\")\n        #     return True\n\n        # Check for entailed subsumption,\n        # common descendants and common instances\n\n        has_subsumption = self.check_subsumption(owl_class1, owl_class2)\n        has_subsumption = has_subsumption or self.check_subsumption(owl_class2, owl_class1)\n        print(f\"[CHECK1 {not has_subsumption}] input classes have NO subsumption relationship;\")\n        if has_subsumption:\n            print(\"Failed the `subsumption check`, skip the `common descendant` check.\")\n            print(f\"[PASSED {not has_subsumption}] assumed disjointness check done.\")\n            return False\n\n        has_common_descendants = self.check_common_descendants(owl_class1, owl_class2)\n        print(f\"[CHECK2 {not has_common_descendants}] input classes have NO common descendant;\")\n        if has_common_descendants:\n            print(\"Failed the `common descendant check`, skip the `common instance` check.\")\n            print(f\"[PASSED {not has_common_descendants}] assumed disjointness check done.\")\n            return False\n\n        # TODO: `check_common_instances` is still experimental because we have not tested it with ontologies of rich ABox.\n        has_common_instances = self.check_common_instances(owl_class1, owl_class2)\n        print(f\"[CHECK3 {not has_common_instances}] input classes have NO common instance;\")\n        print(f\"[PASSED {not has_common_instances}] assumed disjointness check done.\")\n        return not has_common_instances\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.__init__","title":"<code>__init__(onto)</code>","text":"<p>Initialise an ontology reasoner.</p> <p>Parameters:</p> Name Type Description Default <code>onto</code> <code>Ontology</code> <p>The input ontology to conduct reasoning on.</p> required Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def __init__(self, onto: Ontology):\n\"\"\"Initialise an ontology reasoner.\n\n    Args:\n        onto (Ontology): The input ontology to conduct reasoning on.\n    \"\"\"\n    self.onto = onto\n    self.owl_reasoner_factory = ReasonerFactory()\n    self.owl_reasoner = self.owl_reasoner_factory.createReasoner(self.onto.owl_onto)\n    self.owl_data_factory = self.onto.owl_data_factory\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.reload_reasoner","title":"<code>reload_reasoner()</code>","text":"<p>Reload the reasoner for the current ontology (possibly changed).</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def reload_reasoner(self):\n\"\"\"Reload the reasoner for the current ontology (possibly changed).\n    \"\"\"\n    # release the memory\n    self.owl_reasoner.dispose()\n    # conduct reasoning on the possibly changed ontology\n    self.owl_reasoner = self.owl_reasoner_factory.createReasoner(self.onto.owl_onto)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.get_entity_type","title":"<code>get_entity_type(entity, is_singular=False)</code>  <code>staticmethod</code>","text":"<p>A handy method to get the type of an entity (<code>OWLObject</code>).</p> <p>NOTE: This method is inherited from the Ontology Class.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>@staticmethod\ndef get_entity_type(entity: OWLObject, is_singular: bool = False):\n\"\"\"A handy method to get the type of an entity (`OWLObject`).\n\n    NOTE: This method is inherited from the Ontology Class.\n    \"\"\"\n    return Ontology.get_entity_type(entity, is_singular)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.has_iri","title":"<code>has_iri(entity)</code>  <code>staticmethod</code>","text":"<p>Check if an entity has an IRI.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>@staticmethod\ndef has_iri(entity: OWLObject):\n\"\"\"Check if an entity has an IRI.\n    \"\"\"\n    try:\n        entity.getIRI()\n        return True\n    except:\n        return False\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.super_entities_of","title":"<code>super_entities_of(entity, direct=False)</code>","text":"<p>Return the IRIs of super-entities of a given <code>OWLObject</code> according to the reasoner.</p> <p>A mixture of <code>getSuperClasses</code>, <code>getSuperObjectProperties</code>, <code>getSuperDataProperties</code> functions imported from the OWLAPI reasoner. The type of input entity will be  automatically determined. The top entity such as <code>owl:Thing</code> is ignored.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>OWLObject</code> <p>An <code>OWLObject</code> entity of interest.</p> required <code>direct</code> <code>bool</code> <p>Return parents (<code>direct=True</code>) or  ancestors (<code>direct=False</code>). Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of IRIs of the super-entities of the given <code>OWLObject</code> entity.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def super_entities_of(self, entity: OWLObject, direct: bool = False):\nr\"\"\"Return the IRIs of super-entities of a given `OWLObject` according to the reasoner.\n\n    A mixture of `getSuperClasses`, `getSuperObjectProperties`, `getSuperDataProperties`\n    functions imported from the OWLAPI reasoner. The type of input entity will be \n    automatically determined. The top entity such as `owl:Thing` is ignored.\n\n\n    Args:\n        entity (OWLObject): An `OWLObject` entity of interest.\n        direct (bool, optional): Return parents (`direct=True`) or \n            ancestors (`direct=False`). Defaults to `False`.\n\n    Returns:\n        (List[str]): A list of IRIs of the super-entities of the given `OWLObject` entity.\n    \"\"\"\n    entity_type = self.get_entity_type(entity)\n    get_super = f\"getSuper{entity_type}\"\n    TOP = TOP_BOTTOMS[entity_type].TOP  # get the corresponding TOP entity\n    super_entities = getattr(self.owl_reasoner, get_super)(entity, direct).getFlattened()\n    super_entity_iris = [str(s.getIRI()) for s in super_entities]\n    # the root node is owl#Thing\n    if TOP in super_entity_iris:\n        super_entity_iris.remove(TOP)\n    return super_entity_iris\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.sub_entities_of","title":"<code>sub_entities_of(entity, direct=False)</code>","text":"<p>Return the IRIs of sub-entities of a given <code>OWLObject</code> according to the reasoner.</p> <p>A mixture of <code>getSubClasses</code>, <code>getSubObjectProperties</code>, <code>getSubDataProperties</code> functions imported from the OWLAPI reasoner. The type of input entity will be  automatically determined. The bottom entity such as <code>owl:Nothing</code> is ignored.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>OWLObject</code> <p>An <code>OWLObject</code> entity of interest.</p> required <code>direct</code> <code>bool</code> <p>Return parents (<code>direct=True</code>) or  ancestors (<code>direct=False</code>). Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of IRIs of the sub-entities of the given <code>OWLObject</code> entity.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def sub_entities_of(self, entity: OWLObject, direct: bool = False):\n\"\"\"Return the IRIs of sub-entities of a given `OWLObject` according to the reasoner.\n\n    A mixture of `getSubClasses`, `getSubObjectProperties`, `getSubDataProperties`\n    functions imported from the OWLAPI reasoner. The type of input entity will be \n    automatically determined. The bottom entity such as `owl:Nothing` is ignored.\n\n    Args:\n        entity (OWLObject): An `OWLObject` entity of interest.\n        direct (bool, optional): Return parents (`direct=True`) or \n            ancestors (`direct=False`). Defaults to `False`.\n\n    Returns:\n        (List[str]): A list of IRIs of the sub-entities of the given `OWLObject` entity.\n    \"\"\"\n    entity_type = self.get_entity_type(entity)\n    get_sub = f\"getSub{entity_type}\"\n    BOTTOM = TOP_BOTTOMS[entity_type].BOTTOM\n    sub_entities = getattr(self.owl_reasoner, get_sub)(entity, direct).getFlattened()\n    sub_entity_iris = [str(s.getIRI()) for s in sub_entities]\n    # the root node is owl#Thing\n    if BOTTOM in sub_entity_iris:\n        sub_entity_iris.remove(BOTTOM)\n    return sub_entity_iris\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.check_subsumption","title":"<code>check_subsumption(sub_entity, super_entity)</code>","text":"<p>Check if the first entity is subsumed by the second entity according to the reasoner.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def check_subsumption(self, sub_entity: OWLObject, super_entity: OWLObject):\n\"\"\"Check if the first entity is subsumed by the second entity according to the reasoner.\n    \"\"\"\n    entity_type = self.get_entity_type(sub_entity, is_singular=True)\n    assert entity_type == self.get_entity_type(super_entity, is_singular=True)\n\n    sub_axiom = getattr(self.owl_data_factory, f\"getOWLSub{entity_type}OfAxiom\")(\n        sub_entity, super_entity\n    )\n\n    return self.owl_reasoner.isEntailed(sub_axiom)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.check_disjoint","title":"<code>check_disjoint(entity1, entity2)</code>","text":"<p>Check if two OWL class expressions are disjoint according to the reasoner.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def check_disjoint(self, entity1: OWLObject, entity2: OWLObject):\n\"\"\"Check if two OWL class expressions are disjoint according to the reasoner.\n    \"\"\"\n    entity_type = self.get_entity_type(entity1)\n    assert entity_type == self.get_entity_type(entity2)\n\n    disjoint_axiom = getattr(self.owl_data_factory, f\"getOWLDisjoint{entity_type}Axiom\")(\n        [entity1, entity2]\n    )\n\n    return self.owl_reasoner.isEntailed(disjoint_axiom)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.check_common_descendants","title":"<code>check_common_descendants(entity1, entity2)</code>","text":"<p>Check if two entities have a common decendant.</p> <p>Entities can be OWL class or property expressions, and can be either atomic or complex. It takes longer computation time for the complex ones. Complex  entities do not have an IRI. This method is optimised in the way that if  there exists an atomic entity <code>A</code>, we compute descendants for <code>A</code> and compare them against the other entity which could be complex.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def check_common_descendants(self, entity1: OWLObject, entity2: OWLObject):\n\"\"\"Check if two entities have a common decendant.\n\n    Entities can be **OWL class or property expressions**, and can be either **atomic\n    or complex**. It takes longer computation time for the complex ones. Complex \n    entities do not have an IRI. This method is optimised in the way that if \n    there exists an atomic entity `A`, we compute descendants for `A` and\n    compare them against the other entity which could be complex.\n    \"\"\"\n    entity_type = self.get_entity_type(entity1)\n    assert entity_type == self.get_entity_type(entity2)\n\n    if not self.has_iri(entity1) and not self.has_iri(entity2):\n        warnings.warn(\"Computing descendants for two complex entities is not efficient.\")\n\n    # `computed` is the one we compute the descendants\n    # `compared` is the one we compare `computed`'s descendant one-by-one\n    # we set the atomic entity as `computed` for efficiency if there is one\n    computed, compared = entity1, entity2\n    if not self.has_iri(entity1) and self.has_iri(entity2):\n        computed, compared = entity2, entity1\n\n    # for every inferred child of `computed`, check if it is subsumed by `compared``\n    for descendant_iri in self.sub_entities_of(computed):\n        # print(\"check a subsumption\")\n        if self.check_subsumption(self.onto.get_owl_object_from_iri(descendant_iri), compared):\n            return True\n    return False\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.instances_of","title":"<code>instances_of(owl_class)</code>","text":"<p>Return the IRIs of sub-entities of a given OWL class expression.</p> <p>Parameters:</p> Name Type Description Default <code>owl_class</code> <code>OWLClassExpression</code> <p>An ontology class of interest.</p> required <p>Returns:</p> Type Description <code>List[OWLNamedIndividual]</code> <p>A list of named individuals that are entailed instances of <code>owl_class</code>.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def instances_of(self, owl_class: OWLClassExpression):\n\"\"\"Return the IRIs of sub-entities of a given OWL class expression.\n\n    Args:\n        owl_class (OWLClassExpression): An ontology class of interest.\n\n    Returns:\n        (List[OWLNamedIndividual]): A list of named individuals that are entailed instances of `owl_class`.\n    \"\"\"\n    return list(self.owl_reasoner.getInstances(owl_class).getFlattened())\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.check_instance","title":"<code>check_instance(owl_instance, owl_class)</code>","text":"<p>Check if a named individual is an instance of an OWL class.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def check_instance(self, owl_instance: OWLNamedIndividual, owl_class: OWLClassExpression):\n\"\"\"Check if a named individual is an instance of an OWL class.\n    \"\"\"\n    assertion_axiom = self.owl_data_factory.getOWLClassAssertionAxiom(owl_class, owl_instance)\n    return self.owl_reasoner.isEntailed(assertion_axiom)\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.check_common_instances","title":"<code>check_common_instances(owl_class1, owl_class2)</code>","text":"<p>Check if two OWL class expressions have a common instance.</p> <p>Class expressions can be atomic or complex, and it takes longer computation time for the complex ones. Complex classes do not have an IRI. This method is optimised in the way that if there exists an atomic class <code>A</code>, we compute instances for <code>A</code> and compare them against the other class which could be complex. </p> <p>NOTE that compared to <code>check_common_descendants</code> function, the inputs of this function is restricted to OWL class expressions. This is because <code>descendant</code> is related to hierarchy and both class and property expressions have a hierarchy, but <code>instance</code> is restricted to classes.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def check_common_instances(\n    self, owl_class1: OWLClassExpression, owl_class2: OWLClassExpression\n):\n\"\"\"Check if two OWL class expressions have a common instance.\n\n    Class expressions can be **atomic or complex**, and it takes longer computation time\n    for the complex ones. Complex classes do not have an IRI. This method is optimised\n    in the way that if there exists an atomic class `A`, we compute instances for `A` and\n    compare them against the other class which could be complex. \n\n    NOTE that compared to [`check_common_descendants`][deeponto.onto.OntologyReasoner.check_common_descendants]\n    function, the inputs of this function is restricted to OWL class expressions. This is because\n    `descendant` is related to hierarchy and both class and property expressions have a hierarchy,\n    but `instance` is restricted to classes.\n    \"\"\"\n\n    if not self.has_iri(owl_class1) and not self.has_iri(owl_class2):\n        warnings.warn(\"Computing instances for two complex classes is not efficient.\")\n\n    # `computed` is the one we compute the instances\n    # `compared` is the one we compare `computed`'s descendant one-by-one\n    # we set the atomic entity as `computed` for efficiency if there is one\n    computed, compared = owl_class1, owl_class2\n    if not self.has_iri(owl_class1) and self.has_iri(owl_class2):\n        computed, compared = owl_class2, owl_class2\n\n    # for every inferred instance of `computed`, check if it is subsumed by `compared``\n    for instance in self.instances_of(computed):\n        if self.check_instance(instance, compared):\n            return True\n    return False\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.check_assumed_disjoint","title":"<code>check_assumed_disjoint(owl_class1, owl_class2)</code>","text":"<p>Check if two OWL class expressions satisfy the Assumed Disjointness.</p> <p>Paper</p> <p>The definition of Assumed Disjointness comes from the paper: still-under-review.</p> <p>Note</p> <p>Two class expressions C and D are assumed to be disjoint if they meet the followings:</p> <ol> <li>By adding the disjointness axiom of them into the ontology, C and D are still satisfiable.</li> <li>C and D do not have a common descendant (otherwise C and D can be satisfiable but their common descendants become the bottom \\(\\bot\\).) </li> </ol> <p>Note that the special case where C and D are already disjoint is covered by the first check. The paper also proposed a practical alternative to decide Assumed Disjointness.  See <code>check_assumed_disjoint_alternative</code>.</p> <p>Examples:</p> <p>Suppose pre-load an ontology <code>onto</code> from the disease ontology file <code>doid.owl</code>.</p> <pre><code>&gt;&gt;&gt; c1 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_4058\")\n&gt;&gt;&gt; c2 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_0001816\")\n&gt;&gt;&gt; onto.reasoner.check_assumed_disjoint(c1, c2)\n[SUCCESSFULLY] Adding the axiom DisjointClasses(&lt;http://purl.obolibrary.org/obo/DOID_0001816&gt; &lt;http://purl.obolibrary.org/obo/DOID_4058&gt;) into the ontology.\n[CHECK1 True] input classes are still satisfiable;\n[SUCCESSFULLY] Removing the axiom from the ontology.\n[CHECK2 False] input classes have NO common descendant.\n[PASSED False] assumed disjointness check done.\nFalse   \n</code></pre> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def check_assumed_disjoint(\n    self, owl_class1: OWLClassExpression, owl_class2: OWLClassExpression\n):\nr\"\"\"Check if two OWL class expressions satisfy the Assumed Disjointness.\n\n    !!! credit \"Paper\"\n\n        The definition of **Assumed Disjointness** comes from the paper:\n        *[still-under-review](link)*.\n\n    !!! note\n\n        Two class expressions C and D are assumed to be disjoint if they meet the followings:\n\n        1. By adding the disjointness axiom of them into the ontology, C and D are **still satisfiable**.\n        2. C and D **do not have a common descendant** (otherwise C and D can be satisfiable but their\n        common descendants become the bottom $\\bot$.) \n\n    Note that the special case where C and D are already disjoint is covered by the first check.\n    The paper also proposed a practical alternative to decide Assumed Disjointness. \n    See [`check_assumed_disjoint_alternative`][deeponto.onto.OntologyReasoner.check_assumed_disjoint_alternative].\n\n    Examples:\n        Suppose pre-load an ontology `onto` from the disease ontology file `doid.owl`.\n\n        ```python\n        &gt;&gt;&gt; c1 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_4058\")\n        &gt;&gt;&gt; c2 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_0001816\")\n        &gt;&gt;&gt; onto.reasoner.check_assumed_disjoint(c1, c2)\n        [SUCCESSFULLY] Adding the axiom DisjointClasses(&lt;http://purl.obolibrary.org/obo/DOID_0001816&gt; &lt;http://purl.obolibrary.org/obo/DOID_4058&gt;) into the ontology.\n        [CHECK1 True] input classes are still satisfiable;\n        [SUCCESSFULLY] Removing the axiom from the ontology.\n        [CHECK2 False] input classes have NO common descendant.\n        [PASSED False] assumed disjointness check done.\n        False   \n        ```\n    \"\"\"\n    # banner_message(\"Check Asssumed Disjointness\")\n\n    entity_type = self.get_entity_type(owl_class1)\n    assert entity_type == self.get_entity_type(owl_class2)\n\n    # adding the disjointness axiom of `class1`` and `class2``\n    disjoint_axiom = getattr(self.owl_data_factory, f\"getOWLDisjoint{entity_type}Axiom\")(\n        [owl_class1, owl_class2]\n    )\n    undo_change = self.onto.add_axiom(disjoint_axiom, return_undo=True)\n    self.reload_reasoner()\n\n    # check if they are still satisfiable\n    still_satisfiable = self.owl_reasoner.isSatisfiable(owl_class1)\n    still_satisfiable = still_satisfiable and self.owl_reasoner.isSatisfiable(owl_class2)\n    print(f\"[CHECK1 {still_satisfiable}] input classes are still satisfiable;\")\n\n    # remove the axiom and re-construct the reasoner\n    undo_change_result = self.onto.owl_onto.applyChange(undo_change)\n    print(f\"[{str(undo_change_result)}] Removing the axiom from the ontology.\")\n    self.reload_reasoner()\n\n    # failing first check, there is no need to do the second.\n    if not still_satisfiable:\n        print(\"Failed `satisfiability check`, skip the `common descendant` check.\")\n        print(f\"[PASSED {still_satisfiable}] assumed disjointness check done.\")\n        return False\n\n    # otherwise, the classes are still satisfiable and we should conduct the second check\n    has_common_descendants = self.check_common_descendants(owl_class1, owl_class2)\n    print(f\"[CHECK2 {not has_common_descendants}] input classes have NO common descendant.\")\n    print(f\"[PASSED {not has_common_descendants}] assumed disjointness check done.\")\n    return not has_common_descendants\n</code></pre>"},{"location":"deeponto/onto/#deeponto.onto.ontology.OntologyReasoner.check_assumed_disjoint_alternative","title":"<code>check_assumed_disjoint_alternative(owl_class1, owl_class2)</code>","text":"<p>Check if two OWL class expressions satisfy the Assumed Disjointness.</p> <p>Paper</p> <p>The definition of Assumed Disjointness comes from the paper: still-under-review.</p> <p>The practical alternative version of <code>check_assumed_disjoint</code> with following conditions:</p> <p>Note</p> <p>Two class expressions C and D are assumed to be disjoint if they</p> <ol> <li>do not have a subsumption relationship between them, </li> <li>do not have a common descendant (in TBox), </li> <li>do not have a common instance (in ABox).</li> </ol> <p>If either of the conditions have been met, then we assume <code>class1</code> and <code>class2</code> as disjoint.</p> <p>Examples:</p> <p>Suppose pre-load an ontology <code>onto</code> from the disease ontology file <code>doid.owl</code>.</p> <p><pre><code>&gt;&gt;&gt; c1 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_4058\")\n&gt;&gt;&gt; c2 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_0001816\")\n&gt;&gt;&gt; onto.reasoner.check_assumed_disjoint(c1, c2)\n[CHECK1 True] input classes have NO subsumption relationship;\n[CHECK2 False] input classes have NO common descendant;\nFailed the `common descendant check`, skip the `common instance` check.\n[PASSED False] assumed disjointness check done.\nFalse   \n</code></pre> In this alternative implementation, we do no need to add and remove axioms which will then be time-saving.</p> Source code in <code>deeponto/onto/ontology.py</code> <pre><code>def check_assumed_disjoint_alternative(\n    self, owl_class1: OWLClassExpression, owl_class2: OWLClassExpression\n):\nr\"\"\"Check if two OWL class expressions satisfy the Assumed Disjointness.\n\n    !!! credit \"Paper\"\n\n        The definition of **Assumed Disjointness** comes from the paper:\n        *[still-under-review](link)*.\n\n    The practical alternative version of [`check_assumed_disjoint`][deeponto.onto.OntologyReasoner.check_assumed_disjoint]\n    with following conditions:\n\n\n    !!! note\n\n        Two class expressions C and D are assumed to be disjoint if they\n\n        1. **do not** have a **subsumption relationship** between them, \n        2. **do not** have a **common descendant** (in TBox), \n        3. **do not** have a **common instance** (in ABox).\n\n    If either of the conditions have been met, then we assume `class1` and `class2` as disjoint.\n\n    Examples:\n        Suppose pre-load an ontology `onto` from the disease ontology file `doid.owl`.\n\n        ```python\n        &gt;&gt;&gt; c1 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_4058\")\n        &gt;&gt;&gt; c2 = onto.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_0001816\")\n        &gt;&gt;&gt; onto.reasoner.check_assumed_disjoint(c1, c2)\n        [CHECK1 True] input classes have NO subsumption relationship;\n        [CHECK2 False] input classes have NO common descendant;\n        Failed the `common descendant check`, skip the `common instance` check.\n        [PASSED False] assumed disjointness check done.\n        False   \n        ```\n        In this alternative implementation, we do no need to add and remove axioms which will then\n        be time-saving.\n    \"\"\"\n    # banner_message(\"Check Asssumed Disjointness (Alternative)\")\n\n    # # Check for entailed disjointness (short-cut)\n    # if self.check_disjoint(owl_class1, owl_class2):\n    #     print(f\"Input classes are already entailed as disjoint.\")\n    #     return True\n\n    # Check for entailed subsumption,\n    # common descendants and common instances\n\n    has_subsumption = self.check_subsumption(owl_class1, owl_class2)\n    has_subsumption = has_subsumption or self.check_subsumption(owl_class2, owl_class1)\n    print(f\"[CHECK1 {not has_subsumption}] input classes have NO subsumption relationship;\")\n    if has_subsumption:\n        print(\"Failed the `subsumption check`, skip the `common descendant` check.\")\n        print(f\"[PASSED {not has_subsumption}] assumed disjointness check done.\")\n        return False\n\n    has_common_descendants = self.check_common_descendants(owl_class1, owl_class2)\n    print(f\"[CHECK2 {not has_common_descendants}] input classes have NO common descendant;\")\n    if has_common_descendants:\n        print(\"Failed the `common descendant check`, skip the `common instance` check.\")\n        print(f\"[PASSED {not has_common_descendants}] assumed disjointness check done.\")\n        return False\n\n    # TODO: `check_common_instances` is still experimental because we have not tested it with ontologies of rich ABox.\n    has_common_instances = self.check_common_instances(owl_class1, owl_class2)\n    print(f\"[CHECK3 {not has_common_instances}] input classes have NO common instance;\")\n    print(f\"[PASSED {not has_common_instances}] assumed disjointness check done.\")\n    return not has_common_instances\n</code></pre>"},{"location":"deeponto/utils/data_utils/","title":"Data Utilities","text":""},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.DataUtils","title":"<code>DataUtils</code>","text":"Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>class DataUtils:\n\n    @staticmethod\n    def uniqify(ls):\n\"\"\"Return a list of unique elements without messing around the order\"\"\"\n        non_empty_ls = list(filter(lambda x: x != \"\", ls))\n        return list(dict.fromkeys(non_empty_ls))\n\n    @staticmethod\n    def sort_dict_by_values(dic: dict, desc: bool = True, k: Optional[int] = None):\n\"\"\"Return a sorted dict by values with first k reserved if provided.\"\"\"\n        sorted_items = list(sorted(dic.items(), key=lambda item: item[1], reverse=desc))\n        return dict(sorted_items[:k])\n</code></pre>"},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.DataUtils.uniqify","title":"<code>uniqify(ls)</code>  <code>staticmethod</code>","text":"<p>Return a list of unique elements without messing around the order</p> Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>@staticmethod\ndef uniqify(ls):\n\"\"\"Return a list of unique elements without messing around the order\"\"\"\n    non_empty_ls = list(filter(lambda x: x != \"\", ls))\n    return list(dict.fromkeys(non_empty_ls))\n</code></pre>"},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.DataUtils.sort_dict_by_values","title":"<code>sort_dict_by_values(dic, desc=True, k=None)</code>  <code>staticmethod</code>","text":"<p>Return a sorted dict by values with first k reserved if provided.</p> Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>@staticmethod\ndef sort_dict_by_values(dic: dict, desc: bool = True, k: Optional[int] = None):\n\"\"\"Return a sorted dict by values with first k reserved if provided.\"\"\"\n    sorted_items = list(sorted(dic.items(), key=lambda item: item[1], reverse=desc))\n    return dict(sorted_items[:k])\n</code></pre>"},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.RangeNode","title":"<code>RangeNode</code>","text":"<p>         Bases: <code>NodeMixin</code></p> <p>A tree implementation for ranges (without partial overlap).</p> <ul> <li>parent node's range fully covers child node's range, e.g., <code>[1, 10]</code> is a parent of <code>[2, 5]</code>.</li> <li>partial overlap between ranges not allowed, e.g., <code>[2, 4]</code> and <code>[3, 5]</code> cannot appear in the same <code>RangeNodeTree</code>.</li> <li>non-overlap ranges are on different branches.</li> <li>child nodes are ordered according to their relative positions.</li> </ul> Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>class RangeNode(NodeMixin):\n\"\"\"A tree implementation for ranges (without partial overlap).\n\n        - parent node's range fully covers child node's range, e.g., `[1, 10]` is a parent of `[2, 5]`.\n        - partial overlap between ranges not allowed, e.g., `[2, 4]` and `[3, 5]` cannot appear in the same `RangeNodeTree`.\n        - non-overlap ranges are on different branches.\n        - child nodes are ordered according to their relative positions.\n    \"\"\"\n\n    def __init__(self, start, end, **kwargs):\n        if start &gt;= end:\n            raise RuntimeError(\"invalid start and end positions ...\")\n        self.start = start\n        self.end = end\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n        super().__init__()\n\n    # def __eq__(self, other: RangeNode):\n    #     return self.start == other.start and self.end == other.end\n\n    def __gt__(self, other: RangeNode):\nr\"\"\"Modified compare function for a range.\n\n        !!! note\n\n            There are three kinds of comparisons:\n\n            - $R_1 \\leq R_2$: if range $R_1$ is completely contained in range $R_2$.\n            - $R_1 \\gt R_2$: if range $R_2$ is completely contained in range $R_1$.\n            - `\"irrelevant\"`: if range $R_1$ and range $R_2$ have no overlap.\n\n        NOTE that partial overlap is not allowed.\n        \"\"\"\n        if other.start &lt;= self.start and self.end &lt;= other.end:\n            return False\n        elif self.start &lt;= other.start and other.end &lt;= self.end:\n            return True\n        elif other.end &lt; self.start or self.end &lt; other.start:\n            # print(\"compared ranges are irrelevant ...\")\n            return \"irrelevant\"\n        else:\n            raise RuntimeError(\"compared ranges have partial overlap ...\")\n\n    @staticmethod\n    def sort_by_start(nodes: List[RangeNode]):\n\"\"\"A sorting function that sorts the nodes by their starting positions.\"\"\"\n        temp = {sib: sib.start for sib in nodes}\n        return list(dict(sorted(temp.items(), key=lambda item: item[1])).keys())\n\n    def insert_child(self, node: RangeNode):\nr\"\"\"Inserting a child `RangeNode`.\n\n        Child nodes have a smaller (inclusive) range, e.g., `[2, 5]` is a child of `[1, 6]`.\n        \"\"\"\n        if node &gt; self:\n            raise RuntimeError(\"invalid child node\")\n        if node.start == self.start and node.end == self.end:\n            # duplicated node\n            return\n        # print(self.children)\n        if self.children:\n            inserted = False\n            for ch in self.children:\n                if (node &lt; ch) is True:\n                    # print(\"further down\")\n                    ch.insert_child(node)\n                    inserted = True\n                    break\n                elif (node &gt; ch) is True:\n                    # print(\"insert in between\")\n                    ch.parent = node\n                    # NOTE: should not break here as it could be parent of multiple children !\n                    # break\n            if not inserted:\n                self.children = list(self.children) + [node]\n                self.children = self.sort_by_start(self.children)\n        else:\n            node.parent = self\n            self.children = [node]\n\n    def __repr__(self):\n        # only present downwards (down, left, right)\n        printed = f\"[{self.start}, {self.end}]\"\n        if self.children:\n            printed = f\"[{self.start}, {str(list(self.children))[1:-1]}, {self.end}]\"\n        return printed\n\n    def print_tree(self):\n\"\"\"Pretty printing in the tree structure.\"\"\"\n        print(RenderTree(self))\n</code></pre>"},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.RangeNode.__gt__","title":"<code>__gt__(other)</code>","text":"<p>Modified compare function for a range.</p> <p>Note</p> <p>There are three kinds of comparisons:</p> <ul> <li>\\(R_1 \\leq R_2\\): if range \\(R_1\\) is completely contained in range \\(R_2\\).</li> <li>\\(R_1 \\gt R_2\\): if range \\(R_2\\) is completely contained in range \\(R_1\\).</li> <li><code>\"irrelevant\"</code>: if range \\(R_1\\) and range \\(R_2\\) have no overlap.</li> </ul> <p>NOTE that partial overlap is not allowed.</p> Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>def __gt__(self, other: RangeNode):\nr\"\"\"Modified compare function for a range.\n\n    !!! note\n\n        There are three kinds of comparisons:\n\n        - $R_1 \\leq R_2$: if range $R_1$ is completely contained in range $R_2$.\n        - $R_1 \\gt R_2$: if range $R_2$ is completely contained in range $R_1$.\n        - `\"irrelevant\"`: if range $R_1$ and range $R_2$ have no overlap.\n\n    NOTE that partial overlap is not allowed.\n    \"\"\"\n    if other.start &lt;= self.start and self.end &lt;= other.end:\n        return False\n    elif self.start &lt;= other.start and other.end &lt;= self.end:\n        return True\n    elif other.end &lt; self.start or self.end &lt; other.start:\n        # print(\"compared ranges are irrelevant ...\")\n        return \"irrelevant\"\n    else:\n        raise RuntimeError(\"compared ranges have partial overlap ...\")\n</code></pre>"},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.RangeNode.sort_by_start","title":"<code>sort_by_start(nodes)</code>  <code>staticmethod</code>","text":"<p>A sorting function that sorts the nodes by their starting positions.</p> Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>@staticmethod\ndef sort_by_start(nodes: List[RangeNode]):\n\"\"\"A sorting function that sorts the nodes by their starting positions.\"\"\"\n    temp = {sib: sib.start for sib in nodes}\n    return list(dict(sorted(temp.items(), key=lambda item: item[1])).keys())\n</code></pre>"},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.RangeNode.insert_child","title":"<code>insert_child(node)</code>","text":"<p>Inserting a child <code>RangeNode</code>.</p> <p>Child nodes have a smaller (inclusive) range, e.g., <code>[2, 5]</code> is a child of <code>[1, 6]</code>.</p> Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>def insert_child(self, node: RangeNode):\nr\"\"\"Inserting a child `RangeNode`.\n\n    Child nodes have a smaller (inclusive) range, e.g., `[2, 5]` is a child of `[1, 6]`.\n    \"\"\"\n    if node &gt; self:\n        raise RuntimeError(\"invalid child node\")\n    if node.start == self.start and node.end == self.end:\n        # duplicated node\n        return\n    # print(self.children)\n    if self.children:\n        inserted = False\n        for ch in self.children:\n            if (node &lt; ch) is True:\n                # print(\"further down\")\n                ch.insert_child(node)\n                inserted = True\n                break\n            elif (node &gt; ch) is True:\n                # print(\"insert in between\")\n                ch.parent = node\n                # NOTE: should not break here as it could be parent of multiple children !\n                # break\n        if not inserted:\n            self.children = list(self.children) + [node]\n            self.children = self.sort_by_start(self.children)\n    else:\n        node.parent = self\n        self.children = [node]\n</code></pre>"},{"location":"deeponto/utils/data_utils/#deeponto.utils.data_utils.RangeNode.print_tree","title":"<code>print_tree()</code>","text":"<p>Pretty printing in the tree structure.</p> Source code in <code>deeponto/utils/data_utils.py</code> <pre><code>def print_tree(self):\n\"\"\"Pretty printing in the tree structure.\"\"\"\n    print(RenderTree(self))\n</code></pre>"},{"location":"deeponto/utils/decorators/","title":"Decorators","text":"<p>Utility functions for general purposes.</p>"},{"location":"deeponto/utils/decorators/#deeponto.utils.decorators.timer","title":"<code>timer(function)</code>","text":"<p>Print the runtime of the decorated function.</p> Source code in <code>deeponto/utils/decorators.py</code> <pre><code>def timer(function):\n\"\"\"Print the runtime of the decorated function.\"\"\"\n\n    @wraps(function)\n    def wrapper_timer(*args, **kwargs):\n        start_time = time.perf_counter()  # 1\n        value = function(*args, **kwargs)\n        end_time = time.perf_counter()  # 2\n        run_time = end_time - start_time  # 3\n        print(f\"Finished {function.__name__!r} in {run_time:.4f} secs.\")\n        return value\n\n    return wrapper_timer\n</code></pre>"},{"location":"deeponto/utils/decorators/#deeponto.utils.decorators.debug","title":"<code>debug(function)</code>","text":"<p>Print the function signature and return value.</p> Source code in <code>deeponto/utils/decorators.py</code> <pre><code>def debug(function):\n\"\"\"Print the function signature and return value.\"\"\"\n\n    @wraps(function)\n    def wrapper_debug(*args, **kwargs):\n        args_repr = [repr(a) for a in args]\n        kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]\n        signature = \", \".join(args_repr + kwargs_repr)\n        print(f\"Calling {function.__name__}({signature})\")\n        value = function(*args, **kwargs)\n        print(f\"{function.__name__!r} returned {value!r}.\")\n        return value\n\n    return wrapper_debug\n</code></pre>"},{"location":"deeponto/utils/decorators/#deeponto.utils.decorators.paper","title":"<code>paper(title, link)</code>","text":"<p>Add paper tagger for methods.</p> Source code in <code>deeponto/utils/decorators.py</code> <pre><code>def paper(title: str, link: str):\n\"\"\"Add paper tagger for methods.\"\"\"\n    # Define a new decorator, named \"decorator\", to return\n    def decorator(func):\n        # Ensure the decorated function keeps its metadata\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Call the function being decorated and return the result\n            return func(*args, **kwargs)\n\n        wrapper.paper_title = f'This method is associated with tha paper of title: \"{title}\".'\n        wrapper.paper_link = f\"This method is associated with the paper with link: {link}.\"\n        return wrapper\n\n    # Return the new decorator\n    return decorator\n</code></pre>"},{"location":"deeponto/utils/file_utils/","title":"File Utilities","text":""},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils","title":"<code>FileUtils</code>","text":"<p>Provides file processing utilities.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>class FileUtils:\n\"\"\"Provides file processing utilities.\"\"\"\n\n    @staticmethod\n    def create_path(path: str):\n\"\"\"Create a path recursively.\"\"\"\n        Path(path).mkdir(parents=True, exist_ok=True)\n\n    @staticmethod\n    def save_file(obj, save_path: str, sort_keys: bool = False):\n\"\"\"Save an object to a certain format.\"\"\"\n        if save_path.endswith(\".json\"):\n            with open(save_path, \"w\") as output:\n                json.dump(obj, output, indent=4, separators=(\",\", \": \"), sort_keys=sort_keys)\n        elif save_path.endswith(\".pkl\"):\n            with open(save_path, \"wb\") as output:\n                pickle.dump(obj, output, -1)\n        elif save_path.endswith(\".yaml\"):\n            with open(save_path, \"w\") as output:\n                yaml.dump(obj, output, default_flow_style=False, allow_unicode=True)\n        else:\n            raise RuntimeError(f\"Unsupported saving format: {save_path}\")\n\n    @staticmethod\n    def load_file(save_path: str):\n\"\"\"Load an object of a certain format.\"\"\"\n        if save_path.endswith(\".json\"):\n            with open(save_path, \"r\") as input:\n                return json.load(input)\n        elif save_path.endswith(\".pkl\"):\n            with open(save_path, \"rb\") as input:\n                return pickle.load(input)\n        elif save_path.endswith(\".yaml\"):\n            with open(save_path, \"r\") as input:\n                return yaml.safe_load(input)\n        else:\n            raise RuntimeError(f\"Unsupported loading format: {save_path}\")\n\n    @staticmethod\n    def print_dict(dic: dict):\n\"\"\"Pretty print a dictionary.\"\"\"\n        pretty_print = json.dumps(dic, indent=4, separators=(\",\", \": \"))\n        # print(pretty_print)\n        return pretty_print\n\n    @staticmethod\n    def copy2(source: str, destination: str):\n\"\"\"Copy a file from source to destination.\"\"\"\n        try:\n            shutil.copy2(source, destination)\n            print(f\"copied successfully FROM {source} TO {destination}\")\n        except shutil.SameFileError:\n            print(f\"same file exists at {destination}\")\n\n    @staticmethod\n    def read_table(table_file_path: str):\nr\"\"\"Read `csv` or `tsv` file as pandas dataframe without treating `\"NULL\"`, `\"null\"`, and `\"n/a\"` as an empty string.\"\"\"\n        # TODO: this might change with the version of pandas\n        na_vals = pd.io.parsers.readers.STR_NA_VALUES.difference({\"NULL\", \"null\", \"n/a\"})\n        sep = \"\\t\" if table_file_path.endswith(\".tsv\") else \",\"\n        return pd.read_csv(table_file_path, sep=sep, na_values=na_vals, keep_default_na=False)\n\n    @staticmethod\n    def read_jsonl(file_path: str):\n\"\"\"Read `.jsonl` file (list of json) introduced in the BLINK project.\"\"\"\n        results = []\n        key_set = []\n        with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n            lines = f.readlines()\n            for line in lines:\n                record = json.loads(line)\n                results.append(record)\n                key_set += list(record.keys())\n        print(f\"all available keys: {set(key_set)}\")\n        return results\n\n    @staticmethod\n    def read_oaei_mappings(rdf_file: str):\n\"\"\"To read mapping files in the OAEI rdf format.\"\"\"\n        xml_root = ET.parse(rdf_file).getroot()\n        ref_mappings = []  # where relation is \"=\"\n        ignored_mappings = []  # where relation is \"?\"\n\n        for elem in xml_root.iter():\n            # every Cell contains a mapping of en1 -rel(some value)-&gt; en2\n            if \"Cell\" in elem.tag:\n                en1, en2, rel, measure = None, None, None, None\n                for sub_elem in elem:\n                    if \"entity1\" in sub_elem.tag:\n                        en1 = list(sub_elem.attrib.values())[0]\n                    elif \"entity2\" in sub_elem.tag:\n                        en2 = list(sub_elem.attrib.values())[0]\n                    elif \"relation\" in sub_elem.tag:\n                        rel = sub_elem.text\n                    elif \"measure\" in sub_elem.tag:\n                        measure = sub_elem.text\n                row = (en1, en2, measure)\n                # =: equivalent; &gt; superset of; &lt; subset of.\n                if rel == \"=\" or rel == \"&gt;\" or rel == \"&lt;\":\n                    # rel.replace(\"&amp;gt;\", \"&gt;\").replace(\"&amp;lt;\", \"&lt;\")\n                    ref_mappings.append(row)\n                elif rel == \"?\":\n                    ignored_mappings.append(row)\n                else:\n                    print(\"Unknown Relation Warning: \", rel)\n\n        print('#Maps (\"=\"):', len(ref_mappings))\n        print('#Maps (\"?\"):', len(ignored_mappings))\n\n        return ref_mappings, ignored_mappings\n\n    @staticmethod\n    def run_jar(jar_command: str):\n\"\"\"Run jar command using subprocess.\"\"\"\n        proc = subprocess.Popen(jar_command.split(\" \"))\n        try:\n            _, _ = proc.communicate(timeout=600)\n        except subprocess.TimeoutExpired:\n            proc.kill()\n            _, _ = proc.communicate()\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.create_path","title":"<code>create_path(path)</code>  <code>staticmethod</code>","text":"<p>Create a path recursively.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef create_path(path: str):\n\"\"\"Create a path recursively.\"\"\"\n    Path(path).mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.save_file","title":"<code>save_file(obj, save_path, sort_keys=False)</code>  <code>staticmethod</code>","text":"<p>Save an object to a certain format.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef save_file(obj, save_path: str, sort_keys: bool = False):\n\"\"\"Save an object to a certain format.\"\"\"\n    if save_path.endswith(\".json\"):\n        with open(save_path, \"w\") as output:\n            json.dump(obj, output, indent=4, separators=(\",\", \": \"), sort_keys=sort_keys)\n    elif save_path.endswith(\".pkl\"):\n        with open(save_path, \"wb\") as output:\n            pickle.dump(obj, output, -1)\n    elif save_path.endswith(\".yaml\"):\n        with open(save_path, \"w\") as output:\n            yaml.dump(obj, output, default_flow_style=False, allow_unicode=True)\n    else:\n        raise RuntimeError(f\"Unsupported saving format: {save_path}\")\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.load_file","title":"<code>load_file(save_path)</code>  <code>staticmethod</code>","text":"<p>Load an object of a certain format.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef load_file(save_path: str):\n\"\"\"Load an object of a certain format.\"\"\"\n    if save_path.endswith(\".json\"):\n        with open(save_path, \"r\") as input:\n            return json.load(input)\n    elif save_path.endswith(\".pkl\"):\n        with open(save_path, \"rb\") as input:\n            return pickle.load(input)\n    elif save_path.endswith(\".yaml\"):\n        with open(save_path, \"r\") as input:\n            return yaml.safe_load(input)\n    else:\n        raise RuntimeError(f\"Unsupported loading format: {save_path}\")\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.print_dict","title":"<code>print_dict(dic)</code>  <code>staticmethod</code>","text":"<p>Pretty print a dictionary.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef print_dict(dic: dict):\n\"\"\"Pretty print a dictionary.\"\"\"\n    pretty_print = json.dumps(dic, indent=4, separators=(\",\", \": \"))\n    # print(pretty_print)\n    return pretty_print\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.copy2","title":"<code>copy2(source, destination)</code>  <code>staticmethod</code>","text":"<p>Copy a file from source to destination.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef copy2(source: str, destination: str):\n\"\"\"Copy a file from source to destination.\"\"\"\n    try:\n        shutil.copy2(source, destination)\n        print(f\"copied successfully FROM {source} TO {destination}\")\n    except shutil.SameFileError:\n        print(f\"same file exists at {destination}\")\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.read_table","title":"<code>read_table(table_file_path)</code>  <code>staticmethod</code>","text":"<p>Read <code>csv</code> or <code>tsv</code> file as pandas dataframe without treating <code>\"NULL\"</code>, <code>\"null\"</code>, and <code>\"n/a\"</code> as an empty string.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef read_table(table_file_path: str):\nr\"\"\"Read `csv` or `tsv` file as pandas dataframe without treating `\"NULL\"`, `\"null\"`, and `\"n/a\"` as an empty string.\"\"\"\n    # TODO: this might change with the version of pandas\n    na_vals = pd.io.parsers.readers.STR_NA_VALUES.difference({\"NULL\", \"null\", \"n/a\"})\n    sep = \"\\t\" if table_file_path.endswith(\".tsv\") else \",\"\n    return pd.read_csv(table_file_path, sep=sep, na_values=na_vals, keep_default_na=False)\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.read_jsonl","title":"<code>read_jsonl(file_path)</code>  <code>staticmethod</code>","text":"<p>Read <code>.jsonl</code> file (list of json) introduced in the BLINK project.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef read_jsonl(file_path: str):\n\"\"\"Read `.jsonl` file (list of json) introduced in the BLINK project.\"\"\"\n    results = []\n    key_set = []\n    with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n        lines = f.readlines()\n        for line in lines:\n            record = json.loads(line)\n            results.append(record)\n            key_set += list(record.keys())\n    print(f\"all available keys: {set(key_set)}\")\n    return results\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.read_oaei_mappings","title":"<code>read_oaei_mappings(rdf_file)</code>  <code>staticmethod</code>","text":"<p>To read mapping files in the OAEI rdf format.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef read_oaei_mappings(rdf_file: str):\n\"\"\"To read mapping files in the OAEI rdf format.\"\"\"\n    xml_root = ET.parse(rdf_file).getroot()\n    ref_mappings = []  # where relation is \"=\"\n    ignored_mappings = []  # where relation is \"?\"\n\n    for elem in xml_root.iter():\n        # every Cell contains a mapping of en1 -rel(some value)-&gt; en2\n        if \"Cell\" in elem.tag:\n            en1, en2, rel, measure = None, None, None, None\n            for sub_elem in elem:\n                if \"entity1\" in sub_elem.tag:\n                    en1 = list(sub_elem.attrib.values())[0]\n                elif \"entity2\" in sub_elem.tag:\n                    en2 = list(sub_elem.attrib.values())[0]\n                elif \"relation\" in sub_elem.tag:\n                    rel = sub_elem.text\n                elif \"measure\" in sub_elem.tag:\n                    measure = sub_elem.text\n            row = (en1, en2, measure)\n            # =: equivalent; &gt; superset of; &lt; subset of.\n            if rel == \"=\" or rel == \"&gt;\" or rel == \"&lt;\":\n                # rel.replace(\"&amp;gt;\", \"&gt;\").replace(\"&amp;lt;\", \"&lt;\")\n                ref_mappings.append(row)\n            elif rel == \"?\":\n                ignored_mappings.append(row)\n            else:\n                print(\"Unknown Relation Warning: \", rel)\n\n    print('#Maps (\"=\"):', len(ref_mappings))\n    print('#Maps (\"?\"):', len(ignored_mappings))\n\n    return ref_mappings, ignored_mappings\n</code></pre>"},{"location":"deeponto/utils/file_utils/#deeponto.utils.file_utils.FileUtils.run_jar","title":"<code>run_jar(jar_command)</code>  <code>staticmethod</code>","text":"<p>Run jar command using subprocess.</p> Source code in <code>deeponto/utils/file_utils.py</code> <pre><code>@staticmethod\ndef run_jar(jar_command: str):\n\"\"\"Run jar command using subprocess.\"\"\"\n    proc = subprocess.Popen(jar_command.split(\" \"))\n    try:\n        _, _ = proc.communicate(timeout=600)\n    except subprocess.TimeoutExpired:\n        proc.kill()\n        _, _ = proc.communicate()\n</code></pre>"},{"location":"deeponto/utils/logging/","title":"Logging","text":""},{"location":"deeponto/utils/logging/#deeponto.utils.logging.RuntimeFormatter","title":"<code>RuntimeFormatter</code>","text":"<p>         Bases: <code>logging.Formatter</code></p> <p>Auxiliary class for runtime formatting in the logger.</p> Source code in <code>deeponto/utils/logging.py</code> <pre><code>class RuntimeFormatter(logging.Formatter):\n\"\"\"Auxiliary class for runtime formatting in the logger.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_time = time.time()\n\n    def formatTime(self, record, datefmt=None):\n\"\"\"Record relative runtime in hr:min:sec format\u3002\"\"\"\n        duration = datetime.datetime.utcfromtimestamp(record.created - self.start_time)\n        elapsed = duration.strftime(\"%H:%M:%S\")\n        return \"{}\".format(elapsed)\n</code></pre>"},{"location":"deeponto/utils/logging/#deeponto.utils.logging.RuntimeFormatter.formatTime","title":"<code>formatTime(record, datefmt=None)</code>","text":"<p>Record relative runtime in hr:min:sec format\u3002</p> Source code in <code>deeponto/utils/logging.py</code> <pre><code>def formatTime(self, record, datefmt=None):\n\"\"\"Record relative runtime in hr:min:sec format\u3002\"\"\"\n    duration = datetime.datetime.utcfromtimestamp(record.created - self.start_time)\n    elapsed = duration.strftime(\"%H:%M:%S\")\n    return \"{}\".format(elapsed)\n</code></pre>"},{"location":"deeponto/utils/logging/#deeponto.utils.logging.create_logger","title":"<code>create_logger(model_name, saved_path)</code>","text":"<p>Create logger for both console info and saved info.</p> <p>The pre-existed log file will be cleared before writing into new messages.</p> Source code in <code>deeponto/utils/logging.py</code> <pre><code>def create_logger(model_name: str, saved_path: str):\n\"\"\"Create logger for both console info and saved info.\n\n    The pre-existed log file will be cleared before writing into new messages.\n    \"\"\"\n    logger = logging.getLogger(model_name)\n    logger.setLevel(logging.DEBUG)\n    # create file handler which logs even debug messages\n    fh = logging.FileHandler(f\"{saved_path}/{model_name}.log\", mode=\"w\")  # \"w\" means clear the log file before writing\n    fh.setLevel(logging.DEBUG)\n    # create console handler with a higher log level\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n    # create formatter and add it to the handlers\n    formatter = RuntimeFormatter(\"[Time: %(asctime)s] - [PID: %(process)d] - [Model: %(name)s] \\n%(message)s\")\n    fh.setFormatter(formatter)\n    ch.setFormatter(formatter)\n    # add the handlers to the logger\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n    return logger\n</code></pre>"},{"location":"deeponto/utils/logging/#deeponto.utils.logging.banner_message","title":"<code>banner_message(message, sym='^')</code>","text":"<p>Print a banner message surrounded by special symbols.</p> Source code in <code>deeponto/utils/logging.py</code> <pre><code>def banner_message(message: str, sym=\"^\"):\n\"\"\"Print a banner message surrounded by special symbols.\"\"\"\n    print()\n    message = message.upper()\n    banner_len = len(message) + 4\n    message = \" \" * ((banner_len - len(message)) // 2) + message\n    message = message + \" \" * (banner_len - len(message))\n    print(message)\n    print(sym * banner_len)\n    print()\n</code></pre>"},{"location":"deeponto/utils/text_utils/","title":"Text Utilities","text":""},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.TextUtils","title":"<code>TextUtils</code>","text":"<p>Provides text processing utilities.</p> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>class TextUtils:\n\"\"\"Provides text processing utilities.\"\"\"\n\n    @staticmethod\n    def process_annotation_literal(annotation_literal: str, apply_lowercasing: bool = True):\n\"\"\"Pre-process an annotation literal string.\n\n        Args:\n            annotation_literal (str): A literal string of an entity's annotation.\n            apply_lowercasing (bool, optional): A boolean that determines lowercasing or not. Defaults to `True`.\n\n        Returns:\n            (str): the processed annotation literal string.\n        \"\"\"\n\n        # replace the underscores with spaces\n        annotation_literal = annotation_literal.replace(\"_\", \" \")\n\n        # if the annotation literal is a valid identifier with first letter capitalised\n        # we suspect that it could be a Java style identifier that needs to be split\n        if annotation_literal[0].isupper() and annotation_literal.isidentifier():\n            annotation_literal = TextUtils.split_java_identifier(annotation_literal)\n\n        # lowercase the annotation literal if specfied\n        if apply_lowercasing:\n            annotation_literal = annotation_literal.lower()\n\n        return annotation_literal\n\n    @staticmethod\n    def split_java_identifier(java_style_identifier: str):\nr\"\"\"Split words in java's identifier style into natural language phrase.\n\n        Examples:\n            - `\"SuperNaturalPower\"` $\\rightarrow$ `\"Super Natural Power\"`\n            - `\"APIReference\"` $\\rightarrow$ `\"API Reference\"`\n            - `\"Covid19\"` $\\rightarrow$ `\"Covid 19\"`\n        \"\"\"\n        # split at every capital letter or number (numbers are treated as capital letters)\n        raw_words = re.findall(\"([0-9A-Z][a-z]*)\", java_style_identifier)\n        words = []\n        capitalized_word = \"\"\n        for i, w in enumerate(raw_words):\n\n            # the above regex pattern will split at capitals\n            # so the capitalized words are split into characters\n            # i.e., (len(w) == 1)\n            if len(w) == 1:\n                capitalized_word += w\n                # edge case for the last word\n                if i == len(raw_words) - 1:\n                    words.append(capitalized_word)\n\n            # if the the current w is a full word, save the previous\n            # cached capitalized_word and also save current full word\n            elif capitalized_word:\n                words.append(capitalized_word)\n                words.append(w)\n                capitalized_word = \"\"\n\n            # just save the current full word otherwise\n            else:\n                words.append(w)\n\n        return \" \".join(words)\n</code></pre>"},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.TextUtils.process_annotation_literal","title":"<code>process_annotation_literal(annotation_literal, apply_lowercasing=True)</code>  <code>staticmethod</code>","text":"<p>Pre-process an annotation literal string.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_literal</code> <code>str</code> <p>A literal string of an entity's annotation.</p> required <code>apply_lowercasing</code> <code>bool</code> <p>A boolean that determines lowercasing or not. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>the processed annotation literal string.</p> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>@staticmethod\ndef process_annotation_literal(annotation_literal: str, apply_lowercasing: bool = True):\n\"\"\"Pre-process an annotation literal string.\n\n    Args:\n        annotation_literal (str): A literal string of an entity's annotation.\n        apply_lowercasing (bool, optional): A boolean that determines lowercasing or not. Defaults to `True`.\n\n    Returns:\n        (str): the processed annotation literal string.\n    \"\"\"\n\n    # replace the underscores with spaces\n    annotation_literal = annotation_literal.replace(\"_\", \" \")\n\n    # if the annotation literal is a valid identifier with first letter capitalised\n    # we suspect that it could be a Java style identifier that needs to be split\n    if annotation_literal[0].isupper() and annotation_literal.isidentifier():\n        annotation_literal = TextUtils.split_java_identifier(annotation_literal)\n\n    # lowercase the annotation literal if specfied\n    if apply_lowercasing:\n        annotation_literal = annotation_literal.lower()\n\n    return annotation_literal\n</code></pre>"},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.TextUtils.split_java_identifier","title":"<code>split_java_identifier(java_style_identifier)</code>  <code>staticmethod</code>","text":"<p>Split words in java's identifier style into natural language phrase.</p> <p>Examples:</p> <ul> <li><code>\"SuperNaturalPower\"</code> \\(\\rightarrow\\) <code>\"Super Natural Power\"</code></li> <li><code>\"APIReference\"</code> \\(\\rightarrow\\) <code>\"API Reference\"</code></li> <li><code>\"Covid19\"</code> \\(\\rightarrow\\) <code>\"Covid 19\"</code></li> </ul> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>@staticmethod\ndef split_java_identifier(java_style_identifier: str):\nr\"\"\"Split words in java's identifier style into natural language phrase.\n\n    Examples:\n        - `\"SuperNaturalPower\"` $\\rightarrow$ `\"Super Natural Power\"`\n        - `\"APIReference\"` $\\rightarrow$ `\"API Reference\"`\n        - `\"Covid19\"` $\\rightarrow$ `\"Covid 19\"`\n    \"\"\"\n    # split at every capital letter or number (numbers are treated as capital letters)\n    raw_words = re.findall(\"([0-9A-Z][a-z]*)\", java_style_identifier)\n    words = []\n    capitalized_word = \"\"\n    for i, w in enumerate(raw_words):\n\n        # the above regex pattern will split at capitals\n        # so the capitalized words are split into characters\n        # i.e., (len(w) == 1)\n        if len(w) == 1:\n            capitalized_word += w\n            # edge case for the last word\n            if i == len(raw_words) - 1:\n                words.append(capitalized_word)\n\n        # if the the current w is a full word, save the previous\n        # cached capitalized_word and also save current full word\n        elif capitalized_word:\n            words.append(capitalized_word)\n            words.append(w)\n            capitalized_word = \"\"\n\n        # just save the current full word otherwise\n        else:\n            words.append(w)\n\n    return \" \".join(words)\n</code></pre>"},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>A Tokenizer class for both sub-word (pre-trained) and word (rule-based) level tokenization.</p> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>class Tokenizer:\n\"\"\"A Tokenizer class for both sub-word (pre-trained) and word (rule-based) level tokenization.\"\"\"\n\n    def __init__(self, tokenizer_type: str):\n        self.type = tokenizer_type\n        self._tokenizer = None  # hidden tokenizer\n        self.tokenize = None  # the tokenization method\n\n    def __call__(self, texts: Union[str, List[str]]):\n        if isinstance(texts, str):\n            return self.tokenize(texts)\n        else:\n            return list(chain.from_iterable(self.tokenize(t) for t in texts))\n\n    @classmethod\n    def from_pretrained(cls, pretrained_path: str = \"bert-base-uncased\"):\n\"\"\"(Based on **transformers**) Load a sub-word level tokenizer from pre-trained model.\"\"\"\n        instance = cls(\"pre-trained\")\n        instance._tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n        instance.tokenize = instance._tokenizer.tokenize\n        return instance\n\n    @classmethod\n    def from_rule_based(cls):\n\"\"\"(Based on **spacy**) Load a word-level (rule-based) tokenizer.\"\"\"\n        spacy.prefer_gpu()\n        instance = cls(\"rule-based\")\n        instance._tokenizer = English()\n        instance.tokenize = lambda texts: [word.text for word in instance._tokenizer(texts).doc]\n        return instance\n</code></pre>"},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.Tokenizer.from_pretrained","title":"<code>from_pretrained(pretrained_path='bert-base-uncased')</code>  <code>classmethod</code>","text":"<p>(Based on transformers) Load a sub-word level tokenizer from pre-trained model.</p> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, pretrained_path: str = \"bert-base-uncased\"):\n\"\"\"(Based on **transformers**) Load a sub-word level tokenizer from pre-trained model.\"\"\"\n    instance = cls(\"pre-trained\")\n    instance._tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n    instance.tokenize = instance._tokenizer.tokenize\n    return instance\n</code></pre>"},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.Tokenizer.from_rule_based","title":"<code>from_rule_based()</code>  <code>classmethod</code>","text":"<p>(Based on spacy) Load a word-level (rule-based) tokenizer.</p> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>@classmethod\ndef from_rule_based(cls):\n\"\"\"(Based on **spacy**) Load a word-level (rule-based) tokenizer.\"\"\"\n    spacy.prefer_gpu()\n    instance = cls(\"rule-based\")\n    instance._tokenizer = English()\n    instance.tokenize = lambda texts: [word.text for word in instance._tokenizer(texts).doc]\n    return instance\n</code></pre>"},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.InvertedIndex","title":"<code>InvertedIndex</code>","text":"<p>Inverted index built from a text index.</p> <p>Attributes:</p> Name Type Description <code>tokenizer</code> <code>Tokenizer</code> <p>A tokenizer instance to be used.</p> <code>original_index</code> <code>defaultdict</code> <p>A dictionary where the values are text strings to be tokenized.</p> <code>constructed_index</code> <code>defaultdict</code> <p>A dictionary that acts as the inverted index of <code>original_index</code>.</p> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>class InvertedIndex:\nr\"\"\"Inverted index built from a text index.\n\n    Attributes:\n        tokenizer (Tokenizer): A tokenizer instance to be used.\n        original_index (defaultdict): A dictionary where the values are text strings to be tokenized.\n        constructed_index (defaultdict): A dictionary that acts as the inverted index of `original_index`.\n    \"\"\"\n\n    def __init__(self, index: defaultdict, tokenizer: Tokenizer):\n        self.tokenizer = tokenizer\n        self.original_index = index\n        self.constructed_index = defaultdict(list)\n        for k, v in self.original_index.items():\n            # value is a list of strings\n            for token in self.tokenizer(v):\n                self.constructed_index[token].append(k)\n\n    def idf_select(self, texts: Union[str, List[str]], pool_size: int = 200):\n\"\"\"Given a list of tokens, select a set candidates based on the inverted document frequency (idf) scores.\n\n        We use `idf` instead of  `tf` because labels have different lengths and thus tf is not a fair measure.\n        \"\"\"\n        candidate_pool = defaultdict(lambda: 0)\n        # D := number of \"documents\", i.e., number of \"keys\" in the original index\n        D = len(self.original_index)\n        for token in self.tokenizer(texts):\n            # each token is associated with some classes\n            potential_candidates = self.constructed_index[token]\n            if not potential_candidates:\n                continue\n            # We use idf instead of tf because the text for each class is of different length, tf is not a fair measure\n            # inverse document frequency: with more classes to have the current token tk, the score decreases\n            idf = math.log10(D / len(potential_candidates))\n            for candidate in potential_candidates:\n                # each candidate class is scored by sum(idf)\n                candidate_pool[candidate] += idf\n        candidate_pool = list(sorted(candidate_pool.items(), key=lambda item: item[1], reverse=True))\n        # print(f\"Select {min(len(candidate_pool), pool_size)} candidates.\")\n        # select the first K ranked\n        return candidate_pool[:pool_size]\n</code></pre>"},{"location":"deeponto/utils/text_utils/#deeponto.utils.text_utils.InvertedIndex.idf_select","title":"<code>idf_select(texts, pool_size=200)</code>","text":"<p>Given a list of tokens, select a set candidates based on the inverted document frequency (idf) scores.</p> <p>We use <code>idf</code> instead of  <code>tf</code> because labels have different lengths and thus tf is not a fair measure.</p> Source code in <code>deeponto/utils/text_utils.py</code> <pre><code>def idf_select(self, texts: Union[str, List[str]], pool_size: int = 200):\n\"\"\"Given a list of tokens, select a set candidates based on the inverted document frequency (idf) scores.\n\n    We use `idf` instead of  `tf` because labels have different lengths and thus tf is not a fair measure.\n    \"\"\"\n    candidate_pool = defaultdict(lambda: 0)\n    # D := number of \"documents\", i.e., number of \"keys\" in the original index\n    D = len(self.original_index)\n    for token in self.tokenizer(texts):\n        # each token is associated with some classes\n        potential_candidates = self.constructed_index[token]\n        if not potential_candidates:\n            continue\n        # We use idf instead of tf because the text for each class is of different length, tf is not a fair measure\n        # inverse document frequency: with more classes to have the current token tk, the score decreases\n        idf = math.log10(D / len(potential_candidates))\n        for candidate in potential_candidates:\n            # each candidate class is scored by sum(idf)\n            candidate_pool[candidate] += idf\n    candidate_pool = list(sorted(candidate_pool.items(), key=lambda item: item[1], reverse=True))\n    # print(f\"Select {min(len(candidate_pool), pool_size)} candidates.\")\n    # select the first K ranked\n    return candidate_pool[:pool_size]\n</code></pre>"},{"location":"pages/bertmap/","title":"BERTMap","text":"<p>Paper</p> <p>\\(\\textsf{BERTMap}\\) is proposed in the paper: BERTMap: A BERT-based Ontology Alignment System (AAAI-2022).</p> <p>This page gives the tutorial for \\(\\textsf{BERTMap}\\) family including the summary of the models and how to use them.</p>"},{"location":"pages/bertmap/#overview","title":"Overview","text":"<p> <p>Figure 1. Pipeline illustration of BERTMap.</p> </p> <p> The ontology matching (OM) pipeline of \\(\\textsf{BERTMap}\\) consists of following steps:</p> <ol> <li>Load the source and target ontologies and build annotation indices from them based on selected annotation properties.</li> <li>Construct the text semantics corpora including intra-ontology (from input ontologies), cross-ontology (optional, from input mappings), and auxiliary (optional, from auxiliary ontologies) sub-corpora. </li> <li>Split samples in the form of <code>(src_annotation, tgt_annotation, synonym_label)</code> into training and validation splits.</li> <li>Fine-tune a BERT synonym classifier on the samples and obtain the best checkpoint on the validation split.</li> <li> <p>Predict mappings for each class \\(c\\) of the source ontology \\(\\mathcal{O}\\) by:</p> <ul> <li>Selecting plausible candidates \\(c'\\)s in the target ontology \\(\\mathcal{O'}\\) based on idf scores w.r.t. the sub-word inverted index built from the target ontology annotation index. For \\(c\\) and a candidate \\(c'\\). first check if they can be string-matched (i.e., share a common annotation, or equivalently the maximum edit similarity score is \\(1.0\\)); if not, consider all combinations (cartesian product) of their respective class annotations, compute a synonym score for each combination, and take the average of synonym scores as the mapping score.</li> <li>\\(N\\) best scored mappings (no filtering) will be preserved as raw predictions which should have relatively higher recall and lower precision.</li> </ul> </li> <li> <p>Extend the raw predictions using an iterative algorithm based on the locality principle. To be specific, if \\(c\\) and \\(c'\\) are matched with a relatively high mapping score (\\(\\geq \\kappa\\)), then search for plausible mappings between the parents (resp. children) of \\(c\\) and the parents (resp. children) of \\(c'\\). This process is iterative because there would be new highly scored mappings at each round. Terminate mapping extension when there is no new mapping with score \\(\\geq \\kappa\\) found or exceed the maximum number of iterations. Note that \\(\\kappa\\) is set to \\(0.9\\) by default same as in the original paper.</p> </li> <li> <p>Truncate the extended mappings by preserving only those with scores \\(\\geq \\lambda\\). In the original paper, \\(\\lambda\\) is supposed to be tuned on validation mappings \u2013 which are often not available. Also, \\(\\lambda\\) is not a sensitive hyperparameter in practice. Therefore, we manually set \\(\\lambda\\) to \\(0.9995\\) as a default value which usually yields a higher F1 score. Note that both \\(\\kappa\\) and \\(\\lambda\\) are made available in the configuration file.</p> </li> <li> <p>Repair the rest of the mappings with the repair module built in LogMap (BERTMap does not focus on mapping repair). In short, a minimum set of inconsistent mappings will be removed (further improve precision).</p> </li> </ol> <p>Steps 5-8 are referred to as the global matching process which computes OM mappings from two input ontologies. \\(\\textsf{BERTMapLt}\\) is the light-weight version without BERT training and mapping refinement. The mapping filtering threshold for \\(\\textsf{BERTMapLt}\\) is \\(1.0\\) (i.e., string-matched). </p> <p>In addition to the traditional OM procedure, the scoring modules of \\(\\textsf{BERTMap}\\) and \\(\\textsf{BERTMapLt}\\) can be used to evaluate any class pair given their selected annotations. This is useful in ranking-based evaluation. </p> <p>Warning</p> <p>The \\(\\textsf{BERTMap}\\) family rely on sufficient class annotations for constructing training corpora of the BERT synonym classifier, especially under the unsupervised setting where the are no input mappings and/or external resources. It is very important to specify correct annotation properties in the configuration file.</p>"},{"location":"pages/bertmap/#usage","title":"Usage","text":"<p>To use \\(\\textsf{BERTMap}\\), import the configuration file and two input ontologies to be matched.</p> <pre><code>from deeponto.onto import Ontology\nfrom deeponto.align.bertmap import BERTMapPipeline\n\nconfig_file = \"path_to_config.yaml\"\nsrc_onto_file = \"path_to_the_source_ontology.owl\"  \ntgt_onto_file = \"path_to_the_target_ontology.owl\" \n\nconfig = BERTMapPipeline.load_bertmap_config(config_file)\nsrc_onto = Ontology(src_onto_file)\ntgt_onto = Ontology(tgt_onto_file)\n\nBERTMapPipeline(src_onto, tgt_onto, config)\n</code></pre> <p>The default configuration file can be loaded as:</p> <pre><code>from deeponto.align.bertmap import BERTMapPipeline, DEFAULT_CONFIG_FILE\n\nconfig = BERTMapPipeline.load_bertmap_config(DEFAULT_CONFIG_FILE)\n</code></pre> <p>The loaded configuration is a <code>CfgNode</code> object supporting attribute access of dictionary keys.  To customise the configuration, users can either copy the <code>DEFAULT_CONFIG_FILE</code>, save it locally using <code>BERTMapPipeline.save_bertmap_config</code> method, and modify it accordingly; or change it in the run time.</p> <pre><code>from deeponto.align.bertmap import BERTMapPipeline, DEFAULT_CONFIG_FILE\n\nconfig = BERTMapPipeline.load_bertmap_config(DEFAULT_CONFIG_FILE)\n\n# save the configuration file\nBERTMapPipeline.save_bertmap_config(config, \"path_to_saved_config.yaml\")\n\n# modify it in the run time\n# for example, add more annotation properties for synonyms\nconfig.annotation_property_iris.append(\"http://...\") \n</code></pre> <p>If using \\(\\textsf{BERTMap}\\) for scoring class pairs instead of global matching, disable automatic global matching and load class pairs to be scored.</p> <pre><code>from deeponto.onto import Ontology\nfrom deeponto.align.bertmap import BERTMapPipeline\n\nconfig_file = \"path_to_config.yaml\"\nsrc_onto_file = \"path_to_the_source_ontology.owl\"  \ntgt_onto_file = \"path_to_the_target_ontology.owl\" \n\nconfig = BERTMapPipeline.load_bertmap_config(config_file)\nconfig.global_match.enabled = False\nsrc_onto = Ontology(src_onto_file)\ntgt_onto = Ontology(tgt_onto_file)\n\nbertmap = BERTMapPipeline(src_onto, tgt_onto, config)\n\nclass_pairs_to_be_scored = [...]  # (src_class_iri, tgt_class_iri)\nfor src_class_iri, tgt_class_iri in class_pairs_to_be_scored:\n    # retrieve class annotations\n    src_class_annotations = bertmap.src_annotation_index[src_class_iri]\n    tgt_class_annotations = bertmap.tgt_annotation_index[tgt_class_iri]\n    # the bertmap score\n    bertmap_score = bertmap.mapping_predictor.bert_mapping_score(\n        src_class_annotations, tgt_class_annotations\n    )\n    # the bertmaplt score\n    bertmaplt_score = bertmap.mapping_predictor.edit_similarity_mapping_score(\n        src_class_annotations, tgt_class_annotations\n    )\n    ...\n</code></pre> <p>Tip</p> <p>The implemented \\(\\textsf{BERTMap}\\) by default searches for each source ontology class a set of possible matched target ontology classes. Because of this, it is recommended to set the source ontology as the one with a smaller number of classes for efficiency.</p> <p>Note that in the original paper, the model is expected to match for both directions <code>src2tgt</code> and <code>tgt2src</code>, and also consider the combination of both results. However, this does not usually bring better performance and significantly consumes more time. Therefore, this feature is discarded and the users can choose which direction to match.</p>"},{"location":"pages/bertmap/#configuration","title":"Configuration","text":"<p>The default configuration file looks like:</p> <pre><code>name: bertmap  # bertmap or bertmaplt\n\noutput_path: null  # if not provided, the current path \".\" is used\n\nannotation_property_iris:\n- http://www.w3.org/2000/01/rdf-schema#label  # rdfs:label\n- http://www.geneontology.org/formats/oboInOwl#hasSynonym\n- http://www.geneontology.org/formats/oboInOwl#hasExactSynonym\n- http://www.w3.org/2004/02/skos/core#exactMatch\n- http://www.ebi.ac.uk/efo/alternative_term\n- http://www.orpha.net/ORDO/Orphanet_#symbol\n- http://purl.org/sig/ont/fma/synonym\n- http://www.w3.org/2004/02/skos/core#prefLabel\n- http://www.w3.org/2004/02/skos/core#altLabel\n- http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#P108\n- http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#P90\n\n# additional corpora \nknown_mappings: null  # cross-ontology corpus\nauxiliary_ontos: [] # auxiliary corpus\n\n# bert config\nbert:  pretrained_path: emilyalsentzer/Bio_ClinicalBERT  max_length_for_input: 128 num_epochs_for_training: 3.0\nbatch_size_for_training: 32\nbatch_size_for_prediction: 128\nresume_training: null\n\n# global matching config\nglobal_matching:\nenabled: true\nnum_raw_candidates: 200 num_best_predictions: 10 mapping_extension_threshold: 0.9   mapping_filtered_threshold: 0.9995 </code></pre>"},{"location":"pages/bertmap/#bertmap-or-bertmaplt","title":"BERTMap or BERTMapLt","text":"<p>By changing the <code>config.model</code> parameter to <code>bertmap</code> or <code>bertmaplt</code>, users can switch between  \\(\\textsf{BERTMap}\\) and \\(\\textsf{BERTMapLt}\\). Note that \\(\\textsf{BERTMapLt}\\) does not use any training and mapping refinement parameters.</p>"},{"location":"pages/bertmap/#annotation-properties","title":"Annotation Properties","text":"<p>The IRIs stored in <code>config.annotation_property_iris</code> refer to annotation properties with literal values that define the synonyms of an ontology class. Many ontology matching systems rely on synonyms for good performance, including the \\(\\textsf{BERTMap}\\) family. The default <code>config.annotation_property_iris</code> are in line with the Bio-ML dataset, which will be constantly updated. Users can append or delete IRIs for specific input ontologies.</p> <p>Note that it is safe to specify all possible annotation properties regardless of input ontologies because the ones that are not used will be ignored.</p>"},{"location":"pages/bertmap/#additional-training-data","title":"Additional Training Data","text":"<p>The text semantics corpora by default (unsupervised setting) will consist of two intra-ontology sub-corpora built from two input ontologies (based on the specified annotation properties). To add more training data, users can opt to feed input mappings (cross-ontology sub-corpus) and/or a list of auxiliary ontologies (auxiliary sub-corpora). </p> <p>Specify the path to input mapping file in <code>config.known_mappings</code>; the input mapping file should be a <code>.tsv</code> or <code>.csv</code> file with three columns with headings: <code>[\"SrcEntity\", \"TgtEntity\", \"Score\"]</code>. Each row corresponds to a triple \\((c, c', s(c, c'))\\) where \\(c\\) is a source ontology class, \\(c'\\) is a target ontology class, and \\(s(c, c')\\) is the matching score. Note that in the BERTMap context, input mapppings are assumed to be gold standard (reference) mappings with scores equal to \\(1.0\\). Regardless of scores specified in the mapping file, the scores of the input mapppings will be adjusted to \\(1.0\\) automatically.</p> <p>Specify a list of paths to auxiliary ontology files in <code>config.auxiliary_ontos</code>. For each auxiliary ontology, a corresponding intra-ontology corpus will be created and thus produce more synonym and non-synonym samples.</p>"},{"location":"pages/bertmap/#bert-settings","title":"BERT Settings","text":"<p>\\(\\textsf{BERTMap}\\) uses the pre-trained Bio-Clincal BERT as specified in <code>config.bert.pretrained_path</code> because it was originally applied on biomedical ontologies. For general purpose ontology matching, users can use pre-trained variants such as <code>bert-base-uncased</code>.</p> <p>Adjust <code>config.bert.batch_size_for_training</code> and <code>config.bert.batch_size_for_prediction</code> if users found an inappropriate GPU memory fit. Set <code>config.bert.resume_training</code> to <code>true</code> if the BERT training process is somehow interrupted and users wish to continue training.</p>"},{"location":"pages/bertmap/#global-matching-settings","title":"Global Matching Settings","text":"<p>As mentioned in usage, users can disable automatic global matching by setting <code>config.global_matching.enabled</code> to <code>false</code> if they wish to use the mapping scoring module only. </p> <p><code>config.global_matching.num_raw_candidates</code> corresponds to the number of raw candidates selected in the mapping prediction phase. </p> <p><code>config.global_matching.num_best_predictions</code> corresponds to the number of best scored mappings preserved in the mapping prediction phase. The default value <code>10</code> is often more than enough.</p> <p><code>config.global_matching.mapping_extension_threshold</code> corresponds to the score threshold of mappings used in the iterative mapping extension process. Higher value shortens the time but reduces the recall. </p> <p><code>config.global_matching.mapping_filtered_threshold</code> corresponds to the score threshold of mappings preserved for final mapping refinement. </p>"},{"location":"pages/om_resources/","title":"OM Resources","text":""},{"location":"pages/om_resources/#ontology-matching-resources","title":"Ontology Matching Resources","text":"<p>paper</p> <p>The paper for the \\(\\textsf{Bio-ML}\\) ontology matching dataset: Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022).</p> <p>This page provides detailed instructions for using \\(\\textsf{Bio-ML}\\).</p> <p>It also gives the tutorial for how to apply the OM dataset construction approaches as proposed in the \\(\\textsf{Bio-ML}\\),  which can be applied to other ontologies.</p>"},{"location":"pages/om_resources/#bio-ml","title":"Bio-ML","text":"<p>The Bio-ML dataset provides five ontology pairs for both equivalence and subsumption ontology matching. These OM pairs are constructed using the approaches described in the OM construction section.</p> <ul> <li>Download link: https://doi.org/10.5281/zenodo.6510086 (CC BY 4.0 International).</li> <li>Resource Paper: https://arxiv.org/abs/2205.03447.</li> <li>OAEI Track: https://www.cs.ox.ac.uk/isg/projects/ConCur/oaei/. </li> </ul>"},{"location":"pages/om_resources/#data-statistics","title":"Data Statistics","text":""},{"location":"pages/om_resources/#equivalence-matching","title":"Equivalence Matching","text":"<p>Statistics for the equivalence matching set-ups. In the Category column, \"Disease\" indicates that the Mondo data are mainly about disease concepts, while \"Body\", \"Pharm\", and \"Neoplas\" denote semantic types of \"Body Part, Organ, or Organ Components\", \"Pharmacologic Substance\", and \"Neoplastic Process\"* in UMLS, respectively.</p> <p> Source Task Category #Classes #RefMaps (equiv) #Annot. AvgDepths Mondo OMIM-ORDO Disease 9,642-8838 3,721 34K-34K 1.44-1.63 Mondo NCIT-DOID Disease 6,835-8,848 4,684 80K-38K 2.04-6.85 UMLS SNOMED-FMA Body 24,182-64,726 7,256 39K-711K 1.86-9.32 UMLS SNOMED-NCIT Pharm 16,045-15,250 5,803 19K-220K 1.09-3.26 UMLS SNOMED-NCIT Neoplas 11,271-13,956 3,804 23K-182K 1.15-1.68 <p> </p>"},{"location":"pages/om_resources/#subsumption-matching","title":"Subsumption Matching","text":"<p>Statistics for the subsumption matching set-ups. Note that each subsumption matching task is constructed from an equivalence matching task subject to target side class deletion.</p> <p> Source Task Category #Classes #RefMaps (subs) Mondo OMIM-ORDO Disease 9,642-8,735 103 Mondo NCIT-DOID Disease 6,835-5,113 3,339 UMLS SNOMED-FMA Body 24,182-59,567 5,506 UMLS SNOMED-NCIT Pharm 16,045-12,462 4,225 UMLS SNOMED-NCIT Neoplas 11,271-13,790 213 <p> </p>"},{"location":"pages/om_resources/#file-structure","title":"File Structure","text":"<p>The downloaded datasets include <code>Mondo.zip</code> and <code>UMLS.zip</code> for resources constructed from Mondo and UMLS, respectively. Each <code>.zip</code> file has three folders: <code>raw_data</code>, <code>equiv_match</code>, and <code>subs_match</code>, corresponding to the raw source ontologies, data for equivalence matching, and data for subsumption matching, respectively. Detailed structure is presented in the following figure. </p> <p></p> <p> </p>"},{"location":"pages/om_resources/#evaluation-framework","title":"Evaluation Framework","text":"<p>There are two evaluation schemes (local ranking and global matching) and two data split settings (unsupervised and semi-supervised).</p>"},{"location":"pages/om_resources/#local-ranking","title":"Local Ranking","text":"<p>For local ranking, an OM model is required to rank candidate mappings (in <code>test.cands.tsv</code>) generated from test mappings and evalute using \\(Hits@K\\) and \\(MRR\\). </p> <p>Load a <code>test.cands.tsv</code> file using:</p> <pre><code>from deeponto.utils import FileUtils\ndf = FileUtils.read_table(\"test.cands.tsv\")  # with headings: \"SrcEntity\", \"TgtEntity\", \"TgtCandidates\"\ndf.head(3)\n&gt;&gt;&gt;     SrcEntity   TgtEntity   TgtCandidates\n0   http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...   http://purl.obolibrary.org/obo/DOID_0050806 ('http://purl.obolibrary.org/obo/DOID_0110279'...\n1   http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...   http://purl.obolibrary.org/obo/DOID_775 ('http://purl.obolibrary.org/obo/DOID_1670', '...\n2   http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...   http://purl.obolibrary.org/obo/DOID_4917    ('http://purl.obolibrary.org/obo/DOID_3704', '...\n</code></pre> <p>The <code>\"SrcEntity\"</code> and <code>\"TgtEntity\"</code> columns refer to the source class IRI and the target class IRI involved in a reference mapping. The <code>\"TgtCandidates\"</code> column stores a sequence of target candidate class IRIs (including the correct one) used for ranking, which can be accessed as:</p> <pre><code>eval(df.loc[0][\"TgtCandidates\"])\n&gt;&gt;&gt; ('http://purl.obolibrary.org/obo/DOID_0110279',\n 'http://purl.obolibrary.org/obo/DOID_3185',\n 'http://purl.obolibrary.org/obo/DOID_7008',\n ...)\n</code></pre> <p>An OM model is expected to re-rank the candidates in <code>\"TgtCandidates\"</code> according to the given reference source class in <code>\"SrcEntity\"</code>. The higher rank of <code>\"TgtEntity\"</code> is among the <code>\"TgtCandidates\"</code>, the higher the ranking score will be.</p> <p>The candidate mappings were separately generated w.r.t. the tesing data (<code>test.tsv</code>) in each data split.</p> <ul> <li>Unsupervised: <code>test.cands.tsv</code> in <code>refs/unsupervised</code> refers to candidate mappings generated from <code>refs/unsupervised/test.tsv</code> and <code>refs/unsupervised/val.tsv</code> is ensured to be excluded from candidates.</li> <li>Semi-supervised: <code>test.cands.tsv</code> in <code>refs/semi_supervised</code> referes to candidate mappings generated from <code>refs/semi_supervised/test.tsv</code> and `refs/semi_supervised/</li> </ul>"},{"location":"pages/om_resources/#global-matching","title":"Global Matching","text":"<p>For global matching, an OM model is required to output full mappings and compare them with the reference mappings using \\(Precision\\), \\(Recall\\), and \\(F1\\).</p> <p>For each OM pair, a <code>refs/full.tsv</code> file is provided for full reference mapping; the columns of this <code>.tsv</code> file are <code>[\"SrcEntity\", \"TgtEntity\", \"Score\"]</code> standing for the source reference class iri, target class iri, and the score (set to \\(1.0\\) for reference mappings). </p> <p>Load a mapping file using:</p> <pre><code>from deeponto.align.mapping import ReferenceMapping\nrefs = ReferenceMapping.read_table_mappings(\"refs/full.tsv\")\n</code></pre> <p>The full reference mappings in <code>full.tsv</code> are divided into different splits for training (semi-supervised), validation, and testing purposes.</p> <ul> <li>Unsupervised: <code>val.tsv</code> and <code>test.tsv</code> are provided in <code>refs/unsupervised</code> for validation (10%) and testing (90%), respectively.</li> <li>Semi-supervised: <code>train.tsv</code>, <code>val.tsv</code>, <code>train+val.tsv</code> and <code>test.tsv</code> are provided in <code>refs/semi_supervised</code> for training (20%), validation (10%), merged training and validation file for evaluation, and testing (70%), respectively.</li> </ul> <p>Tip</p> <p>When computing the scores (P, R, F1), mappings that are not in the testing set should be ignored by substraction from both system output and reference mappings.    - i.e., when evaluating on <code>refs/unsupervised/test.tsv</code>, <code>refs/unsupervised/val.tsv</code> should be ignored; when evaluating on <code>refs/semi_supervised/test.tsv</code>, <code>refs/semi_supervised/train+val.tsv</code> should be ignored.    - This feature is supported in the code here where the arguement <code>null_reference_mappings</code> is for inputting the reference mappings that should be ignored.</p> <p>Since the subsumption mappings are inherently incomplete, we suggest apply only local ranking for evaluating subsumption matching.</p>"},{"location":"pages/om_resources/#om-dataset-construction","title":"OM Dataset Construction","text":""},{"location":"pages/om_resources/#ontology-pruning","title":"Ontology Pruning","text":"<p>In order to obtain scalable OM pairs, the ontology pruning algorithm proposed in the \\(\\textsf{Bio-ML}\\) paper can be used to truncate a large-scale ontology according to certain criteria such as the semantic type. Once obtaining the list of class IRIs to be truncated, apply the ontology pruning following the code here.</p>"},{"location":"pages/om_resources/#subsumption-mapping-construction","title":"Subsumption Mapping Construction","text":"<p>It is often that OM datasets include equivalence matching but not subsumption matching. However, it is possible to create a subsumption matching task from each equivalence matching task. Given a list of equivalence reference mappings (see how to load from a local file) in the form of \\(\\{(c, c') | c \\equiv c' \\}\\), the reference subsumption mappings can be created by searching for the subsumers of \\(c'\\), then yielding \\(\\{(c, c'') | c \\equiv c', c' \\sqsubseteq c'' \\}\\). We have implemented a subsumption mapping generator for this purpose:</p> <pre><code>from deeponto.onto import Ontology\nfrom deeponto.align.mapping import SubsFromEquivMappingGenerator, ReferenceMapping\n\nncit = Ontology(\"ncit.owl\")  # load the NCIt ontology\ndoid = Ontology(\"doid.owl\")  # load the disease ontology\nncit2doid_equiv_mappings = ReferenceMapping.read_table_mappings(\"ncit2doid_equiv_mappings.tsv\")  # with headings [\"SrcEntity\", \"TgtEntity\", \"Score\"]\n\nsubs_generator = SubsFromEquivMappingGenerator(\n  ncit, doid, ncit2doid_equiv_mappings, \n  subs_generation_ratio=1, delete_used_equiv_tgt_class=True\n)\n&gt;&gt;&gt; 3299/4686 are used for creating at least one subsumption mapping.\n3305 subsumption mappings are created in the end.\n\nsubs_generator.subs_from_equivs\n&gt;&gt;&gt; [('http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C9311',\n  'http://purl.obolibrary.org/obo/DOID_120',\n  1.0),\n ('http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C8410',\n  'http://purl.obolibrary.org/obo/DOID_1612',\n  1.0), ...]\n</code></pre> <p>The <code>subs_generation_ratio</code> parameter determines at most how many subsumption mappings can be generated from an equivalence mapping. The <code>delete_used_equiv_tgt_class</code> determines whether or not to sabotage equivalence mappings used for creating at least one subsumption mappings. If setting to <code>True</code>, then the target side of an (used) equivalence mapping will be marked as deleted from the target ontology. Then, apply ontology pruning to the list of to-be-deleted target ontology classes which can be accessed as <code>subs_generator.used_equiv_tgt_class_iris</code>.</p>"},{"location":"pages/om_resources/#negative-candidate-mapping-generation","title":"Negative Candidate Mapping Generation","text":"<p>In order to examine an OM model's ability to distinguish the correct mapping among a set of challenging negative candidates, we can apply the negative canddiate mapping generation algorithm as proposed in the paper, which utilises the <code>idf_sample</code> to generate candidates ambiguious at the textual level (similar naming), and <code>neighbour_sample</code> to generate candidates ambiguious at the structural level (e.g., siblings). The algorithm makes sure that none of the reference mappings will be added as a negative candidate, and for the subsumption case, the algorithm also takes care of excluding the ancestors because they are correct subsumptions.</p> <pre><code>from deeponto.onto import Ontology\nfrom deeponto.align.mapping import NegativeCandidateMappingGenerator, ReferenceMapping\n\nncit = Ontology(\"ncit.owl\")  # load the NCIt ontology\ndoid = Ontology(\"doid.owl\")  # load the disease ontology\nncit2doid_equiv_mappings = ReferenceMapping.read_table_mappings(\"ncit2doid_equiv_mappings.tsv\")  # with headings [\"SrcEntity\", \"TgtEntity\", \"Score\"]\n\ncand_generator = NegativeCandidateMappingGenerator(\n  ncit, doid, ncit2doid_equiv_mappings, \n  annotation_property_iris = [...],  # used for idf sample\n  tokenizer=Tokenizer.from_pretrained(...),  # used for idf sample\n  max_hops=5, # used for neighbour sample\n  for_subsumptions=False,  # set to False because the input mappings in this example are equivalence mappings\n)\n</code></pre> <p>Sampling using the idf scores is originally proposed in the BERTMap paper. The parameter <code>annotation_property_iris</code> specifies the list of annotation properties used for extracting the names or aliases of an ontology class. The parameter <code>tokenizer</code> refers to a pre-trained sub-word level tokenizer used to build the inverted annotation index. They have been well-explained in the BERTMap tutorial.</p>"},{"location":"pages/ontology/","title":"Ontology Processing &amp; Reasoning","text":"<p>\\(\\textsf{DeepOnto}\\) extends from the OWLAPI and implements many useful methods for ontology processing and reasoning, integrated in the base class <code>Ontology</code>.</p> <p>This page gives typical examples of how to use <code>Ontology</code>. There are other more specific usages, please refer to the documentation by clicking <code>Ontology</code>.</p>"},{"location":"pages/ontology/#loading-ontology","title":"Loading Ontology","text":"<p><code>Ontology</code> can be easily loaded from a local ontology file by its path:</p> <pre><code>from deeponto.onto import Ontology\nonto = Ontology(\"path_to_ontology.owl\")\n</code></pre>"},{"location":"pages/ontology/#acessing-ontology-entities","title":"Acessing Ontology Entities","text":"<p>The most fundamental feature of <code>Ontology</code> is to access entities in the ontology such as classes (or concepts) and properties (object, data, and annotation properties). To get an entity by its IRI, use the following:</p> <pre><code>from deeponto.onto import Ontology\n# e.g., load the disease ontology\ndoid = Ontology(\"doid.owl\")\n# class or property IRI as input\ndoid.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_9969\")\n</code></pre> <p>To obtain the literal values (as <code>Set[str]</code>) of an annotation property (such as rdfs:label) for an entity:</p> <pre><code>...\n# note that annotations with no language tags are deemed as in English (\"en\")\ndoid.get_owl_object_annotations(\n    doid.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_9969\"),\n    annotation_property_iri='http://www.w3.org/2000/01/rdf-schema#label',\n    annotation_language_tag=None,\n    apply_lowercasing=False\n)\n&gt;&gt;&gt; {'carotenemia'}\n</code></pre> <p>To get the special entities related to \\(\\top\\) and \\(\\bot\\):</p> <pre><code>...\ndoid.OWLThing\n&gt;&gt;&gt; &lt;java object 'uk.ac.manchester.cs.owl.owlapi.OWLClassImpl'&gt;\ndoid.OWLBottomDataProperty\n&gt;&gt;&gt; &lt;java object 'uk.ac.manchester.cs.owl.owlapi.OWLDataPropertyImpl'&gt;\n</code></pre>"},{"location":"pages/ontology/#ontology-reasoning","title":"Ontology Reasoning","text":"<p><code>Ontology</code> has an important attribute <code>.reasoner</code> which refers to the HermitT reasoning tool.</p> <p>To get the super-classes (or super-properties) of an entity, use the following:</p> <pre><code>...\ndoid_class = doid.get_owl_object_from_iri(\"http://purl.obolibrary.org/obo/DOID_9969\")\ndoid.reasoner.super_entities_of(doid_class, direct=False) \n&gt;&gt;&gt; ['http://purl.obolibrary.org/obo/DOID_0014667',\n 'http://purl.obolibrary.org/obo/DOID_0060158',\n 'http://purl.obolibrary.org/obo/DOID_4']\n</code></pre> <p>The outputs are IRIs of the corresponding super-entities. <code>direct</code> is a boolean value indicating whether the returned entities are parents (<code>direct</code> is <code>True</code>) or ancestors (<code>direct</code> is <code>False</code>).</p> <p>To get the sub-entities (children or descendants), simply replace the method name with <code>.sub_entities_of</code>.</p> <p>To retrieve the entailed instances of a class:</p> <pre><code>...\ndoid.reasoner.instances_of(doid_class)\n</code></pre> <p>The implemented reasoner also supports several logical checks for subsumption, disjointness, and so on. For example:</p> <pre><code>...\ndoid.reasoner.check_subsumption(doid_potential_sub_entity, doid_potential_super_entity)\n</code></pre>"},{"location":"pages/ontology/#ontology-pruning","title":"Ontology Pruning","text":"<p>The pruning function aims to remove unwanted ontology classes while preserving the relevant hierarchy. Specifically, for each class \\(c\\) to be removed, subsumption axioms will be created between the parents of \\(c\\) and the children of \\(c'\\). Then, an <code>OWLEntityRemover</code> will be used to apply the pruning.</p> <p>paper</p> <p>The ontology pruning algorithm is introduced in the paper:  Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching (ISWC 2022).</p> <pre><code>from deeponto.onto import Ontology\n\ndoid = Ontology(\"doid.owl\")\nto_be_removed_class_iris = [\n    \"http://purl.obolibrary.org/obo/DOID_0060158\",\n    \"http://purl.obolibrary.org/obo/DOID_9969\"\n]\ndoid.apply_pruning(to_be_removed_class_iris)\ndoid.save_onto(\"doid.pruned.owl\")\n</code></pre>"},{"location":"pages/ontology/#ontology-verbalisation","title":"Ontology Verbalisation","text":"<p>to-be-added ...</p>"}]}